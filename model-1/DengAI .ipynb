{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom fbprophet import Prophet\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":91,"outputs":[{"output_type":"stream","text":"/kaggle/input/dengai-dataset/DengAI_Predicting_Disease_Spread_-_Test_Data_Features.csv\n/kaggle/input/dengai-dataset/DengAI_Predicting_Disease_Spread_-_Training_Data_Labels.csv\n/kaggle/input/dengai-dataset/DengAI_Predicting_Disease_Spread_-_Training_Data_Features.csv\n/kaggle/input/dengai-dataset/DengAI_Predicting_Disease_Spread_-_Submission_Format.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","execution_count":92,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"feature_data=pd.read_csv('../input/dengai-dataset/DengAI_Predicting_Disease_Spread_-_Training_Data_Features.csv')\nfeature_data.head()\nfeature_data.dtypes","execution_count":93,"outputs":[{"output_type":"execute_result","execution_count":93,"data":{"text/plain":"  city  year  weekofyear week_start_date   ndvi_ne   ndvi_nw   ndvi_se  \\\n0   sj  1990          18      1990-04-30  0.122600  0.103725  0.198483   \n1   sj  1990          19      1990-05-07  0.169900  0.142175  0.162357   \n2   sj  1990          20      1990-05-14  0.032250  0.172967  0.157200   \n3   sj  1990          21      1990-05-21  0.128633  0.245067  0.227557   \n4   sj  1990          22      1990-05-28  0.196200  0.262200  0.251200   \n\n    ndvi_sw  precipitation_amt_mm  reanalysis_air_temp_k  ...  \\\n0  0.177617                 12.42             297.572857  ...   \n1  0.155486                 22.82             298.211429  ...   \n2  0.170843                 34.54             298.781429  ...   \n3  0.235886                 15.36             298.987143  ...   \n4  0.247340                  7.52             299.518571  ...   \n\n   reanalysis_precip_amt_kg_per_m2  reanalysis_relative_humidity_percent  \\\n0                            32.00                             73.365714   \n1                            17.94                             77.368571   \n2                            26.10                             82.052857   \n3                            13.90                             80.337143   \n4                            12.20                             80.460000   \n\n   reanalysis_sat_precip_amt_mm  reanalysis_specific_humidity_g_per_kg  \\\n0                         12.42                              14.012857   \n1                         22.82                              15.372857   \n2                         34.54                              16.848571   \n3                         15.36                              16.672857   \n4                          7.52                              17.210000   \n\n   reanalysis_tdtr_k  station_avg_temp_c  station_diur_temp_rng_c  \\\n0           2.628571           25.442857                 6.900000   \n1           2.371429           26.714286                 6.371429   \n2           2.300000           26.714286                 6.485714   \n3           2.428571           27.471429                 6.771429   \n4           3.014286           28.942857                 9.371429   \n\n   station_max_temp_c  station_min_temp_c  station_precip_mm  \n0                29.4                20.0               16.0  \n1                31.7                22.2                8.6  \n2                32.2                22.8               41.4  \n3                33.3                23.3                4.0  \n4                35.0                23.9                5.8  \n\n[5 rows x 24 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>city</th>\n      <th>year</th>\n      <th>weekofyear</th>\n      <th>week_start_date</th>\n      <th>ndvi_ne</th>\n      <th>ndvi_nw</th>\n      <th>ndvi_se</th>\n      <th>ndvi_sw</th>\n      <th>precipitation_amt_mm</th>\n      <th>reanalysis_air_temp_k</th>\n      <th>...</th>\n      <th>reanalysis_precip_amt_kg_per_m2</th>\n      <th>reanalysis_relative_humidity_percent</th>\n      <th>reanalysis_sat_precip_amt_mm</th>\n      <th>reanalysis_specific_humidity_g_per_kg</th>\n      <th>reanalysis_tdtr_k</th>\n      <th>station_avg_temp_c</th>\n      <th>station_diur_temp_rng_c</th>\n      <th>station_max_temp_c</th>\n      <th>station_min_temp_c</th>\n      <th>station_precip_mm</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sj</td>\n      <td>1990</td>\n      <td>18</td>\n      <td>1990-04-30</td>\n      <td>0.122600</td>\n      <td>0.103725</td>\n      <td>0.198483</td>\n      <td>0.177617</td>\n      <td>12.42</td>\n      <td>297.572857</td>\n      <td>...</td>\n      <td>32.00</td>\n      <td>73.365714</td>\n      <td>12.42</td>\n      <td>14.012857</td>\n      <td>2.628571</td>\n      <td>25.442857</td>\n      <td>6.900000</td>\n      <td>29.4</td>\n      <td>20.0</td>\n      <td>16.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sj</td>\n      <td>1990</td>\n      <td>19</td>\n      <td>1990-05-07</td>\n      <td>0.169900</td>\n      <td>0.142175</td>\n      <td>0.162357</td>\n      <td>0.155486</td>\n      <td>22.82</td>\n      <td>298.211429</td>\n      <td>...</td>\n      <td>17.94</td>\n      <td>77.368571</td>\n      <td>22.82</td>\n      <td>15.372857</td>\n      <td>2.371429</td>\n      <td>26.714286</td>\n      <td>6.371429</td>\n      <td>31.7</td>\n      <td>22.2</td>\n      <td>8.6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sj</td>\n      <td>1990</td>\n      <td>20</td>\n      <td>1990-05-14</td>\n      <td>0.032250</td>\n      <td>0.172967</td>\n      <td>0.157200</td>\n      <td>0.170843</td>\n      <td>34.54</td>\n      <td>298.781429</td>\n      <td>...</td>\n      <td>26.10</td>\n      <td>82.052857</td>\n      <td>34.54</td>\n      <td>16.848571</td>\n      <td>2.300000</td>\n      <td>26.714286</td>\n      <td>6.485714</td>\n      <td>32.2</td>\n      <td>22.8</td>\n      <td>41.4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sj</td>\n      <td>1990</td>\n      <td>21</td>\n      <td>1990-05-21</td>\n      <td>0.128633</td>\n      <td>0.245067</td>\n      <td>0.227557</td>\n      <td>0.235886</td>\n      <td>15.36</td>\n      <td>298.987143</td>\n      <td>...</td>\n      <td>13.90</td>\n      <td>80.337143</td>\n      <td>15.36</td>\n      <td>16.672857</td>\n      <td>2.428571</td>\n      <td>27.471429</td>\n      <td>6.771429</td>\n      <td>33.3</td>\n      <td>23.3</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sj</td>\n      <td>1990</td>\n      <td>22</td>\n      <td>1990-05-28</td>\n      <td>0.196200</td>\n      <td>0.262200</td>\n      <td>0.251200</td>\n      <td>0.247340</td>\n      <td>7.52</td>\n      <td>299.518571</td>\n      <td>...</td>\n      <td>12.20</td>\n      <td>80.460000</td>\n      <td>7.52</td>\n      <td>17.210000</td>\n      <td>3.014286</td>\n      <td>28.942857</td>\n      <td>9.371429</td>\n      <td>35.0</td>\n      <td>23.9</td>\n      <td>5.8</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 24 columns</p>\n</div>"},"metadata":{}},{"output_type":"execute_result","execution_count":93,"data":{"text/plain":"city                                      object\nyear                                       int64\nweekofyear                                 int64\nweek_start_date                           object\nndvi_ne                                  float64\nndvi_nw                                  float64\nndvi_se                                  float64\nndvi_sw                                  float64\nprecipitation_amt_mm                     float64\nreanalysis_air_temp_k                    float64\nreanalysis_avg_temp_k                    float64\nreanalysis_dew_point_temp_k              float64\nreanalysis_max_air_temp_k                float64\nreanalysis_min_air_temp_k                float64\nreanalysis_precip_amt_kg_per_m2          float64\nreanalysis_relative_humidity_percent     float64\nreanalysis_sat_precip_amt_mm             float64\nreanalysis_specific_humidity_g_per_kg    float64\nreanalysis_tdtr_k                        float64\nstation_avg_temp_c                       float64\nstation_diur_temp_rng_c                  float64\nstation_max_temp_c                       float64\nstation_min_temp_c                       float64\nstation_precip_mm                        float64\ndtype: object"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"case_data=pd.read_csv('../input/dengai-dataset/DengAI_Predicting_Disease_Spread_-_Training_Data_Labels.csv')\ncase_data.head()\n","execution_count":94,"outputs":[{"output_type":"execute_result","execution_count":94,"data":{"text/plain":"  city  year  weekofyear  total_cases\n0   sj  1990          18            4\n1   sj  1990          19            5\n2   sj  1990          20            4\n3   sj  1990          21            3\n4   sj  1990          22            6","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>city</th>\n      <th>year</th>\n      <th>weekofyear</th>\n      <th>total_cases</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sj</td>\n      <td>1990</td>\n      <td>18</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sj</td>\n      <td>1990</td>\n      <td>19</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sj</td>\n      <td>1990</td>\n      <td>20</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sj</td>\n      <td>1990</td>\n      <td>21</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sj</td>\n      <td>1990</td>\n      <td>22</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"case_data.dtypes","execution_count":95,"outputs":[{"output_type":"execute_result","execution_count":95,"data":{"text/plain":"city           object\nyear            int64\nweekofyear      int64\ntotal_cases     int64\ndtype: object"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_data['week_start_date']=pd.to_datetime(feature_data['week_start_date'],format='%Y-%m-%d')\nfeature_data['yearweek']= feature_data['year'].astype(str) + feature_data['weekofyear'].astype(str)\ncase_data['yearweek']= case_data['year'].astype(str) + case_data['weekofyear'].astype(str)\n","execution_count":96,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(case_data)\ncase_data.dtypes\nfeature_data.dtypes\ntraining_data = pd.merge(feature_data,case_data[['city','total_cases','yearweek']], how='left',on=['city','yearweek'])\ntraining_data.head()","execution_count":97,"outputs":[{"output_type":"stream","text":"     city  year  weekofyear  total_cases yearweek\n0      sj  1990          18            4   199018\n1      sj  1990          19            5   199019\n2      sj  1990          20            4   199020\n3      sj  1990          21            3   199021\n4      sj  1990          22            6   199022\n...   ...   ...         ...          ...      ...\n1451   iq  2010          21            5   201021\n1452   iq  2010          22            8   201022\n1453   iq  2010          23            1   201023\n1454   iq  2010          24            1   201024\n1455   iq  2010          25            4   201025\n\n[1456 rows x 5 columns]\n","name":"stdout"},{"output_type":"execute_result","execution_count":97,"data":{"text/plain":"city           object\nyear            int64\nweekofyear      int64\ntotal_cases     int64\nyearweek       object\ndtype: object"},"metadata":{}},{"output_type":"execute_result","execution_count":97,"data":{"text/plain":"city                                             object\nyear                                              int64\nweekofyear                                        int64\nweek_start_date                          datetime64[ns]\nndvi_ne                                         float64\nndvi_nw                                         float64\nndvi_se                                         float64\nndvi_sw                                         float64\nprecipitation_amt_mm                            float64\nreanalysis_air_temp_k                           float64\nreanalysis_avg_temp_k                           float64\nreanalysis_dew_point_temp_k                     float64\nreanalysis_max_air_temp_k                       float64\nreanalysis_min_air_temp_k                       float64\nreanalysis_precip_amt_kg_per_m2                 float64\nreanalysis_relative_humidity_percent            float64\nreanalysis_sat_precip_amt_mm                    float64\nreanalysis_specific_humidity_g_per_kg           float64\nreanalysis_tdtr_k                               float64\nstation_avg_temp_c                              float64\nstation_diur_temp_rng_c                         float64\nstation_max_temp_c                              float64\nstation_min_temp_c                              float64\nstation_precip_mm                               float64\nyearweek                                         object\ndtype: object"},"metadata":{}},{"output_type":"execute_result","execution_count":97,"data":{"text/plain":"  city  year  weekofyear week_start_date   ndvi_ne   ndvi_nw   ndvi_se  \\\n0   sj  1990          18      1990-04-30  0.122600  0.103725  0.198483   \n1   sj  1990          19      1990-05-07  0.169900  0.142175  0.162357   \n2   sj  1990          20      1990-05-14  0.032250  0.172967  0.157200   \n3   sj  1990          21      1990-05-21  0.128633  0.245067  0.227557   \n4   sj  1990          22      1990-05-28  0.196200  0.262200  0.251200   \n\n    ndvi_sw  precipitation_amt_mm  reanalysis_air_temp_k  ...  \\\n0  0.177617                 12.42             297.572857  ...   \n1  0.155486                 22.82             298.211429  ...   \n2  0.170843                 34.54             298.781429  ...   \n3  0.235886                 15.36             298.987143  ...   \n4  0.247340                  7.52             299.518571  ...   \n\n   reanalysis_sat_precip_amt_mm  reanalysis_specific_humidity_g_per_kg  \\\n0                         12.42                              14.012857   \n1                         22.82                              15.372857   \n2                         34.54                              16.848571   \n3                         15.36                              16.672857   \n4                          7.52                              17.210000   \n\n   reanalysis_tdtr_k  station_avg_temp_c  station_diur_temp_rng_c  \\\n0           2.628571           25.442857                 6.900000   \n1           2.371429           26.714286                 6.371429   \n2           2.300000           26.714286                 6.485714   \n3           2.428571           27.471429                 6.771429   \n4           3.014286           28.942857                 9.371429   \n\n   station_max_temp_c  station_min_temp_c  station_precip_mm  yearweek  \\\n0                29.4                20.0               16.0    199018   \n1                31.7                22.2                8.6    199019   \n2                32.2                22.8               41.4    199020   \n3                33.3                23.3                4.0    199021   \n4                35.0                23.9                5.8    199022   \n\n   total_cases  \n0            4  \n1            5  \n2            4  \n3            3  \n4            6  \n\n[5 rows x 26 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>city</th>\n      <th>year</th>\n      <th>weekofyear</th>\n      <th>week_start_date</th>\n      <th>ndvi_ne</th>\n      <th>ndvi_nw</th>\n      <th>ndvi_se</th>\n      <th>ndvi_sw</th>\n      <th>precipitation_amt_mm</th>\n      <th>reanalysis_air_temp_k</th>\n      <th>...</th>\n      <th>reanalysis_sat_precip_amt_mm</th>\n      <th>reanalysis_specific_humidity_g_per_kg</th>\n      <th>reanalysis_tdtr_k</th>\n      <th>station_avg_temp_c</th>\n      <th>station_diur_temp_rng_c</th>\n      <th>station_max_temp_c</th>\n      <th>station_min_temp_c</th>\n      <th>station_precip_mm</th>\n      <th>yearweek</th>\n      <th>total_cases</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sj</td>\n      <td>1990</td>\n      <td>18</td>\n      <td>1990-04-30</td>\n      <td>0.122600</td>\n      <td>0.103725</td>\n      <td>0.198483</td>\n      <td>0.177617</td>\n      <td>12.42</td>\n      <td>297.572857</td>\n      <td>...</td>\n      <td>12.42</td>\n      <td>14.012857</td>\n      <td>2.628571</td>\n      <td>25.442857</td>\n      <td>6.900000</td>\n      <td>29.4</td>\n      <td>20.0</td>\n      <td>16.0</td>\n      <td>199018</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sj</td>\n      <td>1990</td>\n      <td>19</td>\n      <td>1990-05-07</td>\n      <td>0.169900</td>\n      <td>0.142175</td>\n      <td>0.162357</td>\n      <td>0.155486</td>\n      <td>22.82</td>\n      <td>298.211429</td>\n      <td>...</td>\n      <td>22.82</td>\n      <td>15.372857</td>\n      <td>2.371429</td>\n      <td>26.714286</td>\n      <td>6.371429</td>\n      <td>31.7</td>\n      <td>22.2</td>\n      <td>8.6</td>\n      <td>199019</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sj</td>\n      <td>1990</td>\n      <td>20</td>\n      <td>1990-05-14</td>\n      <td>0.032250</td>\n      <td>0.172967</td>\n      <td>0.157200</td>\n      <td>0.170843</td>\n      <td>34.54</td>\n      <td>298.781429</td>\n      <td>...</td>\n      <td>34.54</td>\n      <td>16.848571</td>\n      <td>2.300000</td>\n      <td>26.714286</td>\n      <td>6.485714</td>\n      <td>32.2</td>\n      <td>22.8</td>\n      <td>41.4</td>\n      <td>199020</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sj</td>\n      <td>1990</td>\n      <td>21</td>\n      <td>1990-05-21</td>\n      <td>0.128633</td>\n      <td>0.245067</td>\n      <td>0.227557</td>\n      <td>0.235886</td>\n      <td>15.36</td>\n      <td>298.987143</td>\n      <td>...</td>\n      <td>15.36</td>\n      <td>16.672857</td>\n      <td>2.428571</td>\n      <td>27.471429</td>\n      <td>6.771429</td>\n      <td>33.3</td>\n      <td>23.3</td>\n      <td>4.0</td>\n      <td>199021</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sj</td>\n      <td>1990</td>\n      <td>22</td>\n      <td>1990-05-28</td>\n      <td>0.196200</td>\n      <td>0.262200</td>\n      <td>0.251200</td>\n      <td>0.247340</td>\n      <td>7.52</td>\n      <td>299.518571</td>\n      <td>...</td>\n      <td>7.52</td>\n      <td>17.210000</td>\n      <td>3.014286</td>\n      <td>28.942857</td>\n      <td>9.371429</td>\n      <td>35.0</td>\n      <td>23.9</td>\n      <td>5.8</td>\n      <td>199022</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 26 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"is_sj = training_data.city == 'sj'\nsj_training_data=training_data[is_sj]\nsj_training_data.head()\nsj_training_data.describe()","execution_count":98,"outputs":[{"output_type":"execute_result","execution_count":98,"data":{"text/plain":"  city  year  weekofyear week_start_date   ndvi_ne   ndvi_nw   ndvi_se  \\\n0   sj  1990          18      1990-04-30  0.122600  0.103725  0.198483   \n1   sj  1990          19      1990-05-07  0.169900  0.142175  0.162357   \n2   sj  1990          20      1990-05-14  0.032250  0.172967  0.157200   \n3   sj  1990          21      1990-05-21  0.128633  0.245067  0.227557   \n4   sj  1990          22      1990-05-28  0.196200  0.262200  0.251200   \n\n    ndvi_sw  precipitation_amt_mm  reanalysis_air_temp_k  ...  \\\n0  0.177617                 12.42             297.572857  ...   \n1  0.155486                 22.82             298.211429  ...   \n2  0.170843                 34.54             298.781429  ...   \n3  0.235886                 15.36             298.987143  ...   \n4  0.247340                  7.52             299.518571  ...   \n\n   reanalysis_sat_precip_amt_mm  reanalysis_specific_humidity_g_per_kg  \\\n0                         12.42                              14.012857   \n1                         22.82                              15.372857   \n2                         34.54                              16.848571   \n3                         15.36                              16.672857   \n4                          7.52                              17.210000   \n\n   reanalysis_tdtr_k  station_avg_temp_c  station_diur_temp_rng_c  \\\n0           2.628571           25.442857                 6.900000   \n1           2.371429           26.714286                 6.371429   \n2           2.300000           26.714286                 6.485714   \n3           2.428571           27.471429                 6.771429   \n4           3.014286           28.942857                 9.371429   \n\n   station_max_temp_c  station_min_temp_c  station_precip_mm  yearweek  \\\n0                29.4                20.0               16.0    199018   \n1                31.7                22.2                8.6    199019   \n2                32.2                22.8               41.4    199020   \n3                33.3                23.3                4.0    199021   \n4                35.0                23.9                5.8    199022   \n\n   total_cases  \n0            4  \n1            5  \n2            4  \n3            3  \n4            6  \n\n[5 rows x 26 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>city</th>\n      <th>year</th>\n      <th>weekofyear</th>\n      <th>week_start_date</th>\n      <th>ndvi_ne</th>\n      <th>ndvi_nw</th>\n      <th>ndvi_se</th>\n      <th>ndvi_sw</th>\n      <th>precipitation_amt_mm</th>\n      <th>reanalysis_air_temp_k</th>\n      <th>...</th>\n      <th>reanalysis_sat_precip_amt_mm</th>\n      <th>reanalysis_specific_humidity_g_per_kg</th>\n      <th>reanalysis_tdtr_k</th>\n      <th>station_avg_temp_c</th>\n      <th>station_diur_temp_rng_c</th>\n      <th>station_max_temp_c</th>\n      <th>station_min_temp_c</th>\n      <th>station_precip_mm</th>\n      <th>yearweek</th>\n      <th>total_cases</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sj</td>\n      <td>1990</td>\n      <td>18</td>\n      <td>1990-04-30</td>\n      <td>0.122600</td>\n      <td>0.103725</td>\n      <td>0.198483</td>\n      <td>0.177617</td>\n      <td>12.42</td>\n      <td>297.572857</td>\n      <td>...</td>\n      <td>12.42</td>\n      <td>14.012857</td>\n      <td>2.628571</td>\n      <td>25.442857</td>\n      <td>6.900000</td>\n      <td>29.4</td>\n      <td>20.0</td>\n      <td>16.0</td>\n      <td>199018</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sj</td>\n      <td>1990</td>\n      <td>19</td>\n      <td>1990-05-07</td>\n      <td>0.169900</td>\n      <td>0.142175</td>\n      <td>0.162357</td>\n      <td>0.155486</td>\n      <td>22.82</td>\n      <td>298.211429</td>\n      <td>...</td>\n      <td>22.82</td>\n      <td>15.372857</td>\n      <td>2.371429</td>\n      <td>26.714286</td>\n      <td>6.371429</td>\n      <td>31.7</td>\n      <td>22.2</td>\n      <td>8.6</td>\n      <td>199019</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sj</td>\n      <td>1990</td>\n      <td>20</td>\n      <td>1990-05-14</td>\n      <td>0.032250</td>\n      <td>0.172967</td>\n      <td>0.157200</td>\n      <td>0.170843</td>\n      <td>34.54</td>\n      <td>298.781429</td>\n      <td>...</td>\n      <td>34.54</td>\n      <td>16.848571</td>\n      <td>2.300000</td>\n      <td>26.714286</td>\n      <td>6.485714</td>\n      <td>32.2</td>\n      <td>22.8</td>\n      <td>41.4</td>\n      <td>199020</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sj</td>\n      <td>1990</td>\n      <td>21</td>\n      <td>1990-05-21</td>\n      <td>0.128633</td>\n      <td>0.245067</td>\n      <td>0.227557</td>\n      <td>0.235886</td>\n      <td>15.36</td>\n      <td>298.987143</td>\n      <td>...</td>\n      <td>15.36</td>\n      <td>16.672857</td>\n      <td>2.428571</td>\n      <td>27.471429</td>\n      <td>6.771429</td>\n      <td>33.3</td>\n      <td>23.3</td>\n      <td>4.0</td>\n      <td>199021</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sj</td>\n      <td>1990</td>\n      <td>22</td>\n      <td>1990-05-28</td>\n      <td>0.196200</td>\n      <td>0.262200</td>\n      <td>0.251200</td>\n      <td>0.247340</td>\n      <td>7.52</td>\n      <td>299.518571</td>\n      <td>...</td>\n      <td>7.52</td>\n      <td>17.210000</td>\n      <td>3.014286</td>\n      <td>28.942857</td>\n      <td>9.371429</td>\n      <td>35.0</td>\n      <td>23.9</td>\n      <td>5.8</td>\n      <td>199022</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 26 columns</p>\n</div>"},"metadata":{}},{"output_type":"execute_result","execution_count":98,"data":{"text/plain":"              year  weekofyear     ndvi_ne     ndvi_nw     ndvi_se  \\\ncount   936.000000  936.000000  745.000000  887.000000  917.000000   \nmean   1998.826923   26.503205    0.057925    0.067469    0.177655   \nstd       5.212076   15.021909    0.107153    0.092479    0.057166   \nmin    1990.000000    1.000000   -0.406250   -0.456100   -0.015533   \n25%    1994.000000   13.750000    0.004500    0.016425    0.139283   \n50%    1999.000000   26.500000    0.057700    0.068075    0.177186   \n75%    2003.000000   39.250000    0.111100    0.115200    0.212557   \nmax    2008.000000   53.000000    0.493400    0.437100    0.393129   \n\n          ndvi_sw  precipitation_amt_mm  reanalysis_air_temp_k  \\\ncount  917.000000            927.000000             930.000000   \nmean     0.165956             35.470809             299.163653   \nstd      0.056073             44.606137               1.236429   \nmin     -0.063457              0.000000             295.938571   \n25%      0.129157              0.000000             298.195000   \n50%      0.165971             20.800000             299.254286   \n75%      0.202771             52.180000             300.132857   \nmax      0.381420            390.600000             302.200000   \n\n       reanalysis_avg_temp_k  reanalysis_dew_point_temp_k  ...  \\\ncount             930.000000                   930.000000  ...   \nmean              299.276920                   295.109519  ...   \nstd                 1.218637                     1.569943  ...   \nmin               296.114286                   289.642857  ...   \n25%               298.300000                   293.847857  ...   \n50%               299.378571                   295.464286  ...   \n75%               300.228571                   296.418929  ...   \nmax               302.164286                   297.795714  ...   \n\n       reanalysis_relative_humidity_percent  reanalysis_sat_precip_amt_mm  \\\ncount                            930.000000                    927.000000   \nmean                              78.568181                     35.470809   \nstd                                3.389488                     44.606137   \nmin                               66.735714                      0.000000   \n25%                               76.246071                      0.000000   \n50%                               78.667857                     20.800000   \n75%                               80.963214                     52.180000   \nmax                               87.575714                    390.600000   \n\n       reanalysis_specific_humidity_g_per_kg  reanalysis_tdtr_k  \\\ncount                             930.000000         930.000000   \nmean                               16.552409           2.516267   \nstd                                 1.560923           0.498892   \nmin                                11.715714           1.357143   \n25%                                15.236429           2.157143   \n50%                                16.845714           2.457143   \n75%                                17.858571           2.800000   \nmax                                19.440000           4.428571   \n\n       station_avg_temp_c  station_diur_temp_rng_c  station_max_temp_c  \\\ncount          930.000000               930.000000          930.000000   \nmean            27.006528                 6.757373           31.607957   \nstd              1.415473                 0.835993            1.717297   \nmin             22.842857                 4.528571           26.700000   \n25%             25.842857                 6.200000           30.600000   \n50%             27.228571                 6.757143           31.700000   \n75%             28.185714                 7.285714           32.800000   \nmax             30.071429                 9.914286           35.600000   \n\n       station_min_temp_c  station_precip_mm  total_cases  \ncount          930.000000         930.000000   936.000000  \nmean            22.600645          26.785484    34.180556  \nstd              1.506277          29.325811    51.381372  \nmin             17.800000           0.000000     0.000000  \n25%             21.700000           6.825000     9.000000  \n50%             22.800000          17.750000    19.000000  \n75%             23.900000          35.450000    37.000000  \nmax             25.600000         305.900000   461.000000  \n\n[8 rows x 23 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>weekofyear</th>\n      <th>ndvi_ne</th>\n      <th>ndvi_nw</th>\n      <th>ndvi_se</th>\n      <th>ndvi_sw</th>\n      <th>precipitation_amt_mm</th>\n      <th>reanalysis_air_temp_k</th>\n      <th>reanalysis_avg_temp_k</th>\n      <th>reanalysis_dew_point_temp_k</th>\n      <th>...</th>\n      <th>reanalysis_relative_humidity_percent</th>\n      <th>reanalysis_sat_precip_amt_mm</th>\n      <th>reanalysis_specific_humidity_g_per_kg</th>\n      <th>reanalysis_tdtr_k</th>\n      <th>station_avg_temp_c</th>\n      <th>station_diur_temp_rng_c</th>\n      <th>station_max_temp_c</th>\n      <th>station_min_temp_c</th>\n      <th>station_precip_mm</th>\n      <th>total_cases</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>936.000000</td>\n      <td>936.000000</td>\n      <td>745.000000</td>\n      <td>887.000000</td>\n      <td>917.000000</td>\n      <td>917.000000</td>\n      <td>927.000000</td>\n      <td>930.000000</td>\n      <td>930.000000</td>\n      <td>930.000000</td>\n      <td>...</td>\n      <td>930.000000</td>\n      <td>927.000000</td>\n      <td>930.000000</td>\n      <td>930.000000</td>\n      <td>930.000000</td>\n      <td>930.000000</td>\n      <td>930.000000</td>\n      <td>930.000000</td>\n      <td>930.000000</td>\n      <td>936.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1998.826923</td>\n      <td>26.503205</td>\n      <td>0.057925</td>\n      <td>0.067469</td>\n      <td>0.177655</td>\n      <td>0.165956</td>\n      <td>35.470809</td>\n      <td>299.163653</td>\n      <td>299.276920</td>\n      <td>295.109519</td>\n      <td>...</td>\n      <td>78.568181</td>\n      <td>35.470809</td>\n      <td>16.552409</td>\n      <td>2.516267</td>\n      <td>27.006528</td>\n      <td>6.757373</td>\n      <td>31.607957</td>\n      <td>22.600645</td>\n      <td>26.785484</td>\n      <td>34.180556</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>5.212076</td>\n      <td>15.021909</td>\n      <td>0.107153</td>\n      <td>0.092479</td>\n      <td>0.057166</td>\n      <td>0.056073</td>\n      <td>44.606137</td>\n      <td>1.236429</td>\n      <td>1.218637</td>\n      <td>1.569943</td>\n      <td>...</td>\n      <td>3.389488</td>\n      <td>44.606137</td>\n      <td>1.560923</td>\n      <td>0.498892</td>\n      <td>1.415473</td>\n      <td>0.835993</td>\n      <td>1.717297</td>\n      <td>1.506277</td>\n      <td>29.325811</td>\n      <td>51.381372</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1990.000000</td>\n      <td>1.000000</td>\n      <td>-0.406250</td>\n      <td>-0.456100</td>\n      <td>-0.015533</td>\n      <td>-0.063457</td>\n      <td>0.000000</td>\n      <td>295.938571</td>\n      <td>296.114286</td>\n      <td>289.642857</td>\n      <td>...</td>\n      <td>66.735714</td>\n      <td>0.000000</td>\n      <td>11.715714</td>\n      <td>1.357143</td>\n      <td>22.842857</td>\n      <td>4.528571</td>\n      <td>26.700000</td>\n      <td>17.800000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1994.000000</td>\n      <td>13.750000</td>\n      <td>0.004500</td>\n      <td>0.016425</td>\n      <td>0.139283</td>\n      <td>0.129157</td>\n      <td>0.000000</td>\n      <td>298.195000</td>\n      <td>298.300000</td>\n      <td>293.847857</td>\n      <td>...</td>\n      <td>76.246071</td>\n      <td>0.000000</td>\n      <td>15.236429</td>\n      <td>2.157143</td>\n      <td>25.842857</td>\n      <td>6.200000</td>\n      <td>30.600000</td>\n      <td>21.700000</td>\n      <td>6.825000</td>\n      <td>9.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1999.000000</td>\n      <td>26.500000</td>\n      <td>0.057700</td>\n      <td>0.068075</td>\n      <td>0.177186</td>\n      <td>0.165971</td>\n      <td>20.800000</td>\n      <td>299.254286</td>\n      <td>299.378571</td>\n      <td>295.464286</td>\n      <td>...</td>\n      <td>78.667857</td>\n      <td>20.800000</td>\n      <td>16.845714</td>\n      <td>2.457143</td>\n      <td>27.228571</td>\n      <td>6.757143</td>\n      <td>31.700000</td>\n      <td>22.800000</td>\n      <td>17.750000</td>\n      <td>19.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>2003.000000</td>\n      <td>39.250000</td>\n      <td>0.111100</td>\n      <td>0.115200</td>\n      <td>0.212557</td>\n      <td>0.202771</td>\n      <td>52.180000</td>\n      <td>300.132857</td>\n      <td>300.228571</td>\n      <td>296.418929</td>\n      <td>...</td>\n      <td>80.963214</td>\n      <td>52.180000</td>\n      <td>17.858571</td>\n      <td>2.800000</td>\n      <td>28.185714</td>\n      <td>7.285714</td>\n      <td>32.800000</td>\n      <td>23.900000</td>\n      <td>35.450000</td>\n      <td>37.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>2008.000000</td>\n      <td>53.000000</td>\n      <td>0.493400</td>\n      <td>0.437100</td>\n      <td>0.393129</td>\n      <td>0.381420</td>\n      <td>390.600000</td>\n      <td>302.200000</td>\n      <td>302.164286</td>\n      <td>297.795714</td>\n      <td>...</td>\n      <td>87.575714</td>\n      <td>390.600000</td>\n      <td>19.440000</td>\n      <td>4.428571</td>\n      <td>30.071429</td>\n      <td>9.914286</td>\n      <td>35.600000</td>\n      <td>25.600000</td>\n      <td>305.900000</td>\n      <td>461.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 23 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(sj_training_data['week_start_date'], sj_training_data['total_cases'])\nplt.show()","execution_count":99,"outputs":[{"output_type":"execute_result","execution_count":99,"data":{"text/plain":"<matplotlib.collections.PathCollection at 0x7f248e564a90>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sj_training_data.dtypes","execution_count":100,"outputs":[{"output_type":"execute_result","execution_count":100,"data":{"text/plain":"city                                             object\nyear                                              int64\nweekofyear                                        int64\nweek_start_date                          datetime64[ns]\nndvi_ne                                         float64\nndvi_nw                                         float64\nndvi_se                                         float64\nndvi_sw                                         float64\nprecipitation_amt_mm                            float64\nreanalysis_air_temp_k                           float64\nreanalysis_avg_temp_k                           float64\nreanalysis_dew_point_temp_k                     float64\nreanalysis_max_air_temp_k                       float64\nreanalysis_min_air_temp_k                       float64\nreanalysis_precip_amt_kg_per_m2                 float64\nreanalysis_relative_humidity_percent            float64\nreanalysis_sat_precip_amt_mm                    float64\nreanalysis_specific_humidity_g_per_kg           float64\nreanalysis_tdtr_k                               float64\nstation_avg_temp_c                              float64\nstation_diur_temp_rng_c                         float64\nstation_max_temp_c                              float64\nstation_min_temp_c                              float64\nstation_precip_mm                               float64\nyearweek                                         object\ntotal_cases                                       int64\ndtype: object"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#generating correlation matrix for sj data\nsj_corr=sj_training_data.corr()\nax = plt.subplots(figsize=(11,11))\nmask = np.triu(np.ones_like(sj_corr, dtype=np.bool))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n# Drawing the heatmap \nsns.heatmap(sj_corr, mask=mask, cmap=cmap, center=0,square=True, linewidths=.5, cbar_kws={\"shrink\": .5});\n#matfig=plt.figure(figsize=(12,12))\n#plt.matshow(sj_corr.corr(),aspect='auto',fignum=matfig.number)\n#plt.xticks(range(len(sj_corr.columns)),sj_corr.columns, rotation='vertical')\n#plt.yticks(range(len(sj_corr.columns)),sj_corr.columns)\n#plt.margins(0.2)\n#plt.colorbar()\n#plt.show();\n","execution_count":117,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 792x792 with 2 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating a subset from sj_training_data to be used in fbprohet model\nsj_model_data= sj_training_data.iloc[:,[3,25]]\nsj_model_data.columns=[\"ds\",\"y\"]\nsj_model_data.tail()\nsj_prophet=Prophet(yearly_seasonality=True, weekly_seasonality=True, seasonality_mode='multiplicative')\nsj_prophet.fit(sj_model_data)\n#help(Prophet.make_future_dataframe)\nsj_prop_future= sj_prophet.make_future_dataframe(freq='W',periods=260)\nsj_prop_future.tail()\nsj_prop_forecast= sj_prophet.predict(sj_prop_future)\nsj_prop_forecast.tail()\nplot1 = sj_prophet.plot(sj_prop_forecast)\nplot2 = sj_prophet.plot_components(sj_prop_forecast)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"help(pd.DataFrame)\nsj_test_forecast=pd.DataFrame(dict(city=\"sj\",ds=sj_prop_forecast['ds'], yhat=sj_prop_forecast['yhat']))\nsj_test_forecast.tail()\nsj_predicted=sj_test_forecast.iloc[936:1196,]\nsj_predicted.shape\nsj_predicted.columns=[\"city\",\"ds\",\"total_cases\"]\nsj_predicted.tail()","execution_count":119,"outputs":[{"output_type":"stream","text":"Help on class DataFrame in module pandas.core.frame:\n\nclass DataFrame(pandas.core.generic.NDFrame)\n |  Two-dimensional size-mutable, potentially heterogeneous tabular data\n |  structure with labeled axes (rows and columns). Arithmetic operations\n |  align on both row and column labels. Can be thought of as a dict-like\n |  container for Series objects. The primary pandas data structure.\n |  \n |  Parameters\n |  ----------\n |  data : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n |      Dict can contain Series, arrays, constants, or list-like objects\n |  \n |      .. versionchanged :: 0.23.0\n |         If data is a dict, column order follows insertion-order for\n |         Python 3.6 and later.\n |  \n |      .. versionchanged :: 0.25.0\n |         If data is a list of dicts, column order follows insertion-order\n |         for Python 3.6 and later.\n |  \n |  index : Index or array-like\n |      Index to use for resulting frame. Will default to RangeIndex if\n |      no indexing information part of input data and no index provided\n |  columns : Index or array-like\n |      Column labels to use for resulting frame. Will default to\n |      RangeIndex (0, 1, 2, ..., n) if no column labels are provided\n |  dtype : dtype, default None\n |      Data type to force. Only a single dtype is allowed. If None, infer\n |  copy : boolean, default False\n |      Copy data from inputs. Only affects DataFrame / 2d ndarray input\n |  \n |  See Also\n |  --------\n |  DataFrame.from_records : Constructor from tuples, also record arrays.\n |  DataFrame.from_dict : From dicts of Series, arrays, or dicts.\n |  DataFrame.from_items : From sequence of (key, value) pairs\n |      read_csv, pandas.read_table, pandas.read_clipboard.\n |  \n |  Examples\n |  --------\n |  Constructing DataFrame from a dictionary.\n |  \n |  >>> d = {'col1': [1, 2], 'col2': [3, 4]}\n |  >>> df = pd.DataFrame(data=d)\n |  >>> df\n |     col1  col2\n |  0     1     3\n |  1     2     4\n |  \n |  Notice that the inferred dtype is int64.\n |  \n |  >>> df.dtypes\n |  col1    int64\n |  col2    int64\n |  dtype: object\n |  \n |  To enforce a single dtype:\n |  \n |  >>> df = pd.DataFrame(data=d, dtype=np.int8)\n |  >>> df.dtypes\n |  col1    int8\n |  col2    int8\n |  dtype: object\n |  \n |  Constructing DataFrame from numpy ndarray:\n |  \n |  >>> df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n |  ...                    columns=['a', 'b', 'c'])\n |  >>> df2\n |     a  b  c\n |  0  1  2  3\n |  1  4  5  6\n |  2  7  8  9\n |  \n |  Method resolution order:\n |      DataFrame\n |      pandas.core.generic.NDFrame\n |      pandas.core.base.PandasObject\n |      pandas.core.accessor.DirNamesMixin\n |      pandas.core.base.SelectionMixin\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __add__(self, other, axis=None, level=None, fill_value=None)\n |      Binary operator __add__ with support to substitute a fill_value for missing data in\n |      one of the inputs\n |      \n |      Parameters\n |      ----------\n |      other : Series, DataFrame, or constant\n |      axis : {0, 1, 'index', 'columns'}\n |          For Series input, axis to match Series index on\n |      fill_value : None or float value, default None\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful DataFrame alignment, with this value before computation.\n |          If data in both corresponding DataFrame locations is missing\n |          the result will be missing\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level\n |      \n |      Returns\n |      -------\n |      result : DataFrame\n |      \n |      Notes\n |      -----\n |      Mismatched indices will be unioned together\n |  \n |  __and__(self, other, axis='columns', level=None, fill_value=None)\n |      Binary operator __and__ with support to substitute a fill_value for missing data in\n |      one of the inputs\n |      \n |      Parameters\n |      ----------\n |      other : Series, DataFrame, or constant\n |      axis : {0, 1, 'index', 'columns'}\n |          For Series input, axis to match Series index on\n |      fill_value : None or float value, default None\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful DataFrame alignment, with this value before computation.\n |          If data in both corresponding DataFrame locations is missing\n |          the result will be missing\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level\n |      \n |      Returns\n |      -------\n |      result : DataFrame\n |      \n |      Notes\n |      -----\n |      Mismatched indices will be unioned together\n |  \n |  __div__ = __truediv__(self, other, axis=None, level=None, fill_value=None)\n |  \n |  __eq__(self, other)\n |      Wrapper for comparison method __eq__\n |  \n |  __floordiv__(self, other, axis=None, level=None, fill_value=None)\n |      Binary operator __floordiv__ with support to substitute a fill_value for missing data in\n |      one of the inputs\n |      \n |      Parameters\n |      ----------\n |      other : Series, DataFrame, or constant\n |      axis : {0, 1, 'index', 'columns'}\n |          For Series input, axis to match Series index on\n |      fill_value : None or float value, default None\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful DataFrame alignment, with this value before computation.\n |          If data in both corresponding DataFrame locations is missing\n |          the result will be missing\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level\n |      \n |      Returns\n |      -------\n |      result : DataFrame\n |      \n |      Notes\n |      -----\n |      Mismatched indices will be unioned together\n |  \n |  __ge__(self, other)\n |      Wrapper for comparison method __ge__\n |  \n |  __getitem__(self, key)\n |  \n |  __gt__(self, other)\n |      Wrapper for comparison method __gt__\n |  \n |  __iadd__(self, other)\n |  \n |  __iand__(self, other)\n |  \n |  __ifloordiv__(self, other)\n |  \n |  __imod__(self, other)\n |  \n |  __imul__(self, other)\n |  \n |  __init__(self, data=None, index=None, columns=None, dtype=None, copy=False)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  __ior__(self, other)\n |  \n |  __ipow__(self, other)\n |  \n |  __isub__(self, other)\n |  \n |  __itruediv__(self, other)\n |  \n |  __ixor__(self, other)\n |  \n |  __le__(self, other)\n |      Wrapper for comparison method __le__\n |  \n |  __len__(self)\n |      Returns length of info axis, but here we use the index.\n |  \n |  __lt__(self, other)\n |      Wrapper for comparison method __lt__\n |  \n |  __matmul__(self, other)\n |      Matrix multiplication using binary `@` operator in Python>=3.5.\n |  \n |  __mod__(self, other, axis=None, level=None, fill_value=None)\n |      Binary operator __mod__ with support to substitute a fill_value for missing data in\n |      one of the inputs\n |      \n |      Parameters\n |      ----------\n |      other : Series, DataFrame, or constant\n |      axis : {0, 1, 'index', 'columns'}\n |          For Series input, axis to match Series index on\n |      fill_value : None or float value, default None\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful DataFrame alignment, with this value before computation.\n |          If data in both corresponding DataFrame locations is missing\n |          the result will be missing\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level\n |      \n |      Returns\n |      -------\n |      result : DataFrame\n |      \n |      Notes\n |      -----\n |      Mismatched indices will be unioned together\n |  \n |  __mul__(self, other, axis=None, level=None, fill_value=None)\n |      Binary operator __mul__ with support to substitute a fill_value for missing data in\n |      one of the inputs\n |      \n |      Parameters\n |      ----------\n |      other : Series, DataFrame, or constant\n |      axis : {0, 1, 'index', 'columns'}\n |          For Series input, axis to match Series index on\n |      fill_value : None or float value, default None\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful DataFrame alignment, with this value before computation.\n |          If data in both corresponding DataFrame locations is missing\n |          the result will be missing\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level\n |      \n |      Returns\n |      -------\n |      result : DataFrame\n |      \n |      Notes\n |      -----\n |      Mismatched indices will be unioned together\n |  \n |  __ne__(self, other)\n |      Wrapper for comparison method __ne__\n |  \n |  __or__(self, other, axis='columns', level=None, fill_value=None)\n |      Binary operator __or__ with support to substitute a fill_value for missing data in\n |      one of the inputs\n |      \n |      Parameters\n |      ----------\n |      other : Series, DataFrame, or constant\n |      axis : {0, 1, 'index', 'columns'}\n |          For Series input, axis to match Series index on\n |      fill_value : None or float value, default None\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful DataFrame alignment, with this value before computation.\n |          If data in both corresponding DataFrame locations is missing\n |          the result will be missing\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level\n |      \n |      Returns\n |      -------\n |      result : DataFrame\n |      \n |      Notes\n |      -----\n |      Mismatched indices will be unioned together\n |  \n |  __pow__(self, other, axis=None, level=None, fill_value=None)\n |      Binary operator __pow__ with support to substitute a fill_value for missing data in\n |      one of the inputs\n |      \n |      Parameters\n |      ----------\n |      other : Series, DataFrame, or constant\n |      axis : {0, 1, 'index', 'columns'}\n |          For Series input, axis to match Series index on\n |      fill_value : None or float value, default None\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful DataFrame alignment, with this value before computation.\n |          If data in both corresponding DataFrame locations is missing\n |          the result will be missing\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level\n |      \n |      Returns\n |      -------\n |      result : DataFrame\n |      \n |      Notes\n |      -----\n |      Mismatched indices will be unioned together\n |  \n |  __radd__(self, other, axis=None, level=None, fill_value=None)\n |      Binary operator __radd__ with support to substitute a fill_value for missing data in\n |      one of the inputs\n |      \n |      Parameters\n |      ----------\n |      other : Series, DataFrame, or constant\n |      axis : {0, 1, 'index', 'columns'}\n |          For Series input, axis to match Series index on\n |      fill_value : None or float value, default None\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful DataFrame alignment, with this value before computation.\n |          If data in both corresponding DataFrame locations is missing\n |          the result will be missing\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level\n |      \n |      Returns\n |      -------\n |      result : DataFrame\n |      \n |      Notes\n |      -----\n |      Mismatched indices will be unioned together\n |  \n |  __rand__(self, other, axis='columns', level=None, fill_value=None)\n |      Binary operator __rand__ with support to substitute a fill_value for missing data in\n |      one of the inputs\n |      \n |      Parameters\n |      ----------\n |      other : Series, DataFrame, or constant\n |      axis : {0, 1, 'index', 'columns'}\n |          For Series input, axis to match Series index on\n |      fill_value : None or float value, default None\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful DataFrame alignment, with this value before computation.\n |          If data in both corresponding DataFrame locations is missing\n |          the result will be missing\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level\n |      \n |      Returns\n |      -------\n |      result : DataFrame\n |      \n |      Notes\n |      -----\n |      Mismatched indices will be unioned together\n |  \n |  __rdiv__ = __rtruediv__(self, other, axis=None, level=None, fill_value=None)\n |  \n |  __repr__(self)\n |      Return a string representation for a particular DataFrame.\n |  \n |  __rfloordiv__(self, other, axis=None, level=None, fill_value=None)\n |      Binary operator __rfloordiv__ with support to substitute a fill_value for missing data in\n |      one of the inputs\n |      \n |      Parameters\n |      ----------\n |      other : Series, DataFrame, or constant\n |      axis : {0, 1, 'index', 'columns'}\n |          For Series input, axis to match Series index on\n |      fill_value : None or float value, default None\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful DataFrame alignment, with this value before computation.\n |          If data in both corresponding DataFrame locations is missing\n |          the result will be missing\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level\n |      \n |      Returns\n |      -------\n |      result : DataFrame\n |      \n |      Notes\n |      -----\n |      Mismatched indices will be unioned together\n |  \n |  __rmatmul__(self, other)\n |      Matrix multiplication using binary `@` operator in Python>=3.5.\n |  \n |  __rmod__(self, other, axis=None, level=None, fill_value=None)\n |      Binary operator __rmod__ with support to substitute a fill_value for missing data in\n |      one of the inputs\n |      \n |      Parameters\n |      ----------\n |      other : Series, DataFrame, or constant\n |      axis : {0, 1, 'index', 'columns'}\n |          For Series input, axis to match Series index on\n |      fill_value : None or float value, default None\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful DataFrame alignment, with this value before computation.\n |          If data in both corresponding DataFrame locations is missing\n |          the result will be missing\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level\n |      \n |      Returns\n |      -------\n |      result : DataFrame\n |      \n |      Notes\n |      -----\n |      Mismatched indices will be unioned together\n |  \n |  __rmul__(self, other, axis=None, level=None, fill_value=None)\n |      Binary operator __rmul__ with support to substitute a fill_value for missing data in\n |      one of the inputs\n |      \n |      Parameters\n |      ----------\n |      other : Series, DataFrame, or constant\n |      axis : {0, 1, 'index', 'columns'}\n |          For Series input, axis to match Series index on\n |      fill_value : None or float value, default None\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful DataFrame alignment, with this value before computation.\n |          If data in both corresponding DataFrame locations is missing\n |          the result will be missing\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level\n |      \n |      Returns\n |      -------\n |      result : DataFrame\n |      \n |      Notes\n |      -----\n |      Mismatched indices will be unioned together\n |  \n |  __ror__(self, other, axis='columns', level=None, fill_value=None)\n |      Binary operator __ror__ with support to substitute a fill_value for missing data in\n |      one of the inputs\n |      \n |      Parameters\n |      ----------\n |      other : Series, DataFrame, or constant\n |      axis : {0, 1, 'index', 'columns'}\n |          For Series input, axis to match Series index on\n |      fill_value : None or float value, default None\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful DataFrame alignment, with this value before computation.\n |          If data in both corresponding DataFrame locations is missing\n |          the result will be missing\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level\n |      \n |      Returns\n |      -------\n |      result : DataFrame\n |      \n |      Notes\n |      -----\n |      Mismatched indices will be unioned together\n |  \n |  __rpow__(self, other, axis=None, level=None, fill_value=None)\n |      Binary operator __rpow__ with support to substitute a fill_value for missing data in\n |      one of the inputs\n |      \n |      Parameters\n |      ----------\n |      other : Series, DataFrame, or constant\n |      axis : {0, 1, 'index', 'columns'}\n |          For Series input, axis to match Series index on\n |      fill_value : None or float value, default None\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful DataFrame alignment, with this value before computation.\n |          If data in both corresponding DataFrame locations is missing\n |          the result will be missing\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level\n |      \n |      Returns\n |      -------\n |      result : DataFrame\n |      \n |      Notes\n |      -----\n |      Mismatched indices will be unioned together\n |  \n |  __rsub__(self, other, axis=None, level=None, fill_value=None)\n |      Binary operator __rsub__ with support to substitute a fill_value for missing data in\n |      one of the inputs\n |      \n |      Parameters\n |      ----------\n |      other : Series, DataFrame, or constant\n |      axis : {0, 1, 'index', 'columns'}\n |          For Series input, axis to match Series index on\n |      fill_value : None or float value, default None\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful DataFrame alignment, with this value before computation.\n |          If data in both corresponding DataFrame locations is missing\n |          the result will be missing\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level\n |      \n |      Returns\n |      -------\n |      result : DataFrame\n |      \n |      Notes\n |      -----\n |      Mismatched indices will be unioned together\n |  \n |  __rtruediv__(self, other, axis=None, level=None, fill_value=None)\n |      Binary operator __rtruediv__ with support to substitute a fill_value for missing data in\n |      one of the inputs\n |      \n |      Parameters\n |      ----------\n |      other : Series, DataFrame, or constant\n |      axis : {0, 1, 'index', 'columns'}\n |          For Series input, axis to match Series index on\n |      fill_value : None or float value, default None\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful DataFrame alignment, with this value before computation.\n |          If data in both corresponding DataFrame locations is missing\n |          the result will be missing\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level\n |      \n |      Returns\n |      -------\n |      result : DataFrame\n |      \n |      Notes\n |      -----\n |      Mismatched indices will be unioned together\n |  \n |  __rxor__(self, other, axis='columns', level=None, fill_value=None)\n |      Binary operator __rxor__ with support to substitute a fill_value for missing data in\n |      one of the inputs\n |      \n |      Parameters\n |      ----------\n |      other : Series, DataFrame, or constant\n |      axis : {0, 1, 'index', 'columns'}\n |          For Series input, axis to match Series index on\n |      fill_value : None or float value, default None\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful DataFrame alignment, with this value before computation.\n |          If data in both corresponding DataFrame locations is missing\n |          the result will be missing\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level\n |      \n |      Returns\n |      -------\n |      result : DataFrame\n |      \n |      Notes\n |      -----\n |      Mismatched indices will be unioned together\n |  \n |  __setitem__(self, key, value)\n |  \n |  __sub__(self, other, axis=None, level=None, fill_value=None)\n |      Binary operator __sub__ with support to substitute a fill_value for missing data in\n |      one of the inputs\n |      \n |      Parameters\n |      ----------\n |      other : Series, DataFrame, or constant\n |      axis : {0, 1, 'index', 'columns'}\n |          For Series input, axis to match Series index on\n |      fill_value : None or float value, default None\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful DataFrame alignment, with this value before computation.\n |          If data in both corresponding DataFrame locations is missing\n |          the result will be missing\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level\n |      \n |      Returns\n |      -------\n |      result : DataFrame\n |      \n |      Notes\n |      -----\n |      Mismatched indices will be unioned together\n |  \n |  __truediv__(self, other, axis=None, level=None, fill_value=None)\n |      Binary operator __truediv__ with support to substitute a fill_value for missing data in\n |      one of the inputs\n |      \n |      Parameters\n |      ----------\n |      other : Series, DataFrame, or constant\n |      axis : {0, 1, 'index', 'columns'}\n |          For Series input, axis to match Series index on\n |      fill_value : None or float value, default None\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful DataFrame alignment, with this value before computation.\n |          If data in both corresponding DataFrame locations is missing\n |          the result will be missing\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level\n |      \n |      Returns\n |      -------\n |      result : DataFrame\n |      \n |      Notes\n |      -----\n |      Mismatched indices will be unioned together\n |  \n |  __xor__(self, other, axis='columns', level=None, fill_value=None)\n |      Binary operator __xor__ with support to substitute a fill_value for missing data in\n |      one of the inputs\n |      \n |      Parameters\n |      ----------\n |      other : Series, DataFrame, or constant\n |      axis : {0, 1, 'index', 'columns'}\n |          For Series input, axis to match Series index on\n |      fill_value : None or float value, default None\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful DataFrame alignment, with this value before computation.\n |          If data in both corresponding DataFrame locations is missing\n |          the result will be missing\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level\n |      \n |      Returns\n |      -------\n |      result : DataFrame\n |      \n |      Notes\n |      -----\n |      Mismatched indices will be unioned together\n |  \n |  add(self, other, axis='columns', level=None, fill_value=None)\n |      Get Addition of dataframe and other, element-wise (binary operator `add`).\n |      \n |      Equivalent to ``dataframe + other``, but with support to substitute a fill_value\n |      for missing data in one of the inputs. With reverse version, `radd`.\n |      \n |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `mod`, `pow`) to\n |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n |      \n |      Parameters\n |      ----------\n |      other : scalar, sequence, Series, or DataFrame\n |          Any single or multiple element data structure, or list-like object.\n |      axis :  {0 or 'index', 1 or 'columns'}\n |          Whether to compare by the index (0 or 'index') or columns\n |          (1 or 'columns'). For Series input, axis to match Series index on.\n |      level : int or label\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level.\n |      fill_value : float or None, default None\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful DataFrame alignment, with this value before computation.\n |          If data in both corresponding DataFrame locations is missing\n |          the result will be missing.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          Result of the arithmetic operation.\n |      \n |      See Also\n |      --------\n |      DataFrame.add : Add DataFrames.\n |      DataFrame.sub : Subtract DataFrames.\n |      DataFrame.mul : Multiply DataFrames.\n |      DataFrame.div : Divide DataFrames (float division).\n |      DataFrame.truediv : Divide DataFrames (float division).\n |      DataFrame.floordiv : Divide DataFrames (integer division).\n |      DataFrame.mod : Calculate modulo (remainder after division).\n |      DataFrame.pow : Calculate exponential power.\n |      \n |      Notes\n |      -----\n |      Mismatched indices will be unioned together.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'angles': [0, 3, 4],\n |      ...                    'degrees': [360, 180, 360]},\n |      ...                   index=['circle', 'triangle', 'rectangle'])\n |      >>> df\n |                 angles  degrees\n |      circle          0      360\n |      triangle        3      180\n |      rectangle       4      360\n |      \n |      Add a scalar with operator version which return the same\n |      results.\n |      \n |      >>> df + 1\n |                 angles  degrees\n |      circle          1      361\n |      triangle        4      181\n |      rectangle       5      361\n |      \n |      >>> df.add(1)\n |                 angles  degrees\n |      circle          1      361\n |      triangle        4      181\n |      rectangle       5      361\n |      \n |      Divide by constant with reverse version.\n |      \n |      >>> df.div(10)\n |                 angles  degrees\n |      circle        0.0     36.0\n |      triangle      0.3     18.0\n |      rectangle     0.4     36.0\n |      \n |      >>> df.rdiv(10)\n |                   angles   degrees\n |      circle          inf  0.027778\n |      triangle   3.333333  0.055556\n |      rectangle  2.500000  0.027778\n |      \n |      Subtract a list and Series by axis with operator version.\n |      \n |      >>> df - [1, 2]\n |                 angles  degrees\n |      circle         -1      358\n |      triangle        2      178\n |      rectangle       3      358\n |      \n |      >>> df.sub([1, 2], axis='columns')\n |                 angles  degrees\n |      circle         -1      358\n |      triangle        2      178\n |      rectangle       3      358\n |      \n |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n |      ...        axis='index')\n |                 angles  degrees\n |      circle         -1      359\n |      triangle        2      179\n |      rectangle       3      359\n |      \n |      Multiply a DataFrame of different shape with operator version.\n |      \n |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},\n |      ...                      index=['circle', 'triangle', 'rectangle'])\n |      >>> other\n |                 angles\n |      circle          0\n |      triangle        3\n |      rectangle       4\n |      \n |      >>> df * other\n |                 angles  degrees\n |      circle          0      NaN\n |      triangle        9      NaN\n |      rectangle      16      NaN\n |      \n |      >>> df.mul(other, fill_value=0)\n |                 angles  degrees\n |      circle          0      0.0\n |      triangle        9      0.0\n |      rectangle      16      0.0\n |      \n |      Divide by a MultiIndex by level.\n |      \n |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n |      ...                                    ['circle', 'triangle', 'rectangle',\n |      ...                                     'square', 'pentagon', 'hexagon']])\n |      >>> df_multindex\n |                   angles  degrees\n |      A circle          0      360\n |        triangle        3      180\n |        rectangle       4      360\n |      B square          4      360\n |        pentagon        5      540\n |        hexagon         6      720\n |      \n |      >>> df.div(df_multindex, level=1, fill_value=0)\n |                   angles  degrees\n |      A circle        NaN      1.0\n |        triangle      1.0      1.0\n |        rectangle     1.0      1.0\n |      B square        0.0      0.0\n |        pentagon      0.0      0.0\n |        hexagon       0.0      0.0\n |  \n |  agg = aggregate(self, func, axis=0, *args, **kwargs)\n |  \n |  aggregate(self, func, axis=0, *args, **kwargs)\n |      Aggregate using one or more operations over the specified axis.\n |      \n |      .. versionadded:: 0.20.0\n |      \n |      Parameters\n |      ----------\n |      func : function, str, list or dict\n |          Function to use for aggregating the data. If a function, must either\n |          work when passed a DataFrame or when passed to DataFrame.apply.\n |      \n |          Accepted combinations are:\n |      \n |          - function\n |          - string function name\n |          - list of functions and/or function names, e.g. ``[np.sum, 'mean']``\n |          - dict of axis labels -> functions, function names or list of such.\n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |              If 0 or 'index': apply function to each column.\n |              If 1 or 'columns': apply function to each row.\n |      *args\n |          Positional arguments to pass to `func`.\n |      **kwargs\n |          Keyword arguments to pass to `func`.\n |      \n |      Returns\n |      -------\n |      scalar, Series or DataFrame\n |      \n |          The return can be:\n |      \n |          * scalar : when Series.agg is called with single function\n |          * Series : when DataFrame.agg is called with a single function\n |          * DataFrame : when DataFrame.agg is called with several functions\n |      \n |          Return scalar, Series or DataFrame.\n |      \n |      The aggregation operations are always performed over an axis, either the\n |      index (default) or the column axis. This behavior is different from\n |      `numpy` aggregation functions (`mean`, `median`, `prod`, `sum`, `std`,\n |      `var`), where the default is to compute the aggregation of the flattened\n |      array, e.g., ``numpy.mean(arr_2d)`` as opposed to\n |      ``numpy.mean(arr_2d, axis=0)``.\n |      \n |      `agg` is an alias for `aggregate`. Use the alias.\n |      \n |      See Also\n |      --------\n |      DataFrame.apply : Perform any type of operations.\n |      DataFrame.transform : Perform transformation type operations.\n |      core.groupby.GroupBy : Perform operations over groups.\n |      core.resample.Resampler : Perform operations over resampled bins.\n |      core.window.Rolling : Perform operations over rolling window.\n |      core.window.Expanding : Perform operations over expanding window.\n |      core.window.EWM : Perform operation over exponential weighted\n |          window.\n |      \n |      Notes\n |      -----\n |      `agg` is an alias for `aggregate`. Use the alias.\n |      \n |      A passed user-defined-function will be passed a Series for evaluation.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame([[1, 2, 3],\n |      ...                    [4, 5, 6],\n |      ...                    [7, 8, 9],\n |      ...                    [np.nan, np.nan, np.nan]],\n |      ...                   columns=['A', 'B', 'C'])\n |      \n |      Aggregate these functions over the rows.\n |      \n |      >>> df.agg(['sum', 'min'])\n |              A     B     C\n |      sum  12.0  15.0  18.0\n |      min   1.0   2.0   3.0\n |      \n |      Different aggregations per column.\n |      \n |      >>> df.agg({'A' : ['sum', 'min'], 'B' : ['min', 'max']})\n |              A    B\n |      max   NaN  8.0\n |      min   1.0  2.0\n |      sum  12.0  NaN\n |      \n |      Aggregate over the columns.\n |      \n |      >>> df.agg(\"mean\", axis=\"columns\")\n |      0    2.0\n |      1    5.0\n |      2    8.0\n |      3    NaN\n |      dtype: float64\n |  \n |  align(self, other, join='outer', axis=None, level=None, copy=True, fill_value=None, method=None, limit=None, fill_axis=0, broadcast_axis=None)\n |      Align two objects on their axes with the\n |      specified join method for each axis Index.\n |      \n |      Parameters\n |      ----------\n |      other : DataFrame or Series\n |      join : {'outer', 'inner', 'left', 'right'}, default 'outer'\n |      axis : allowed axis of the other object, default None\n |          Align on index (0), columns (1), or both (None)\n |      level : int or level name, default None\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level\n |      copy : boolean, default True\n |          Always returns new objects. If copy=False and no reindexing is\n |          required then original objects are returned.\n |      fill_value : scalar, default np.NaN\n |          Value to use for missing values. Defaults to NaN, but can be any\n |          \"compatible\" value\n |      method : {'backfill', 'bfill', 'pad', 'ffill', None}, default None\n |          Method to use for filling holes in reindexed Series\n |          pad / ffill: propagate last valid observation forward to next valid\n |          backfill / bfill: use NEXT valid observation to fill gap\n |      limit : int, default None\n |          If method is specified, this is the maximum number of consecutive\n |          NaN values to forward/backward fill. In other words, if there is\n |          a gap with more than this number of consecutive NaNs, it will only\n |          be partially filled. If method is not specified, this is the\n |          maximum number of entries along the entire axis where NaNs will be\n |          filled. Must be greater than 0 if not None.\n |      fill_axis : {0 or 'index', 1 or 'columns'}, default 0\n |          Filling axis, method and limit\n |      broadcast_axis : {0 or 'index', 1 or 'columns'}, default None\n |          Broadcast values along this axis, if aligning two objects of\n |          different dimensions\n |      \n |      Returns\n |      -------\n |      (left, right) : (DataFrame, type of other)\n |          Aligned objects.\n |  \n |  all(self, axis=0, bool_only=None, skipna=True, level=None, **kwargs)\n |      Return whether all elements are True, potentially over an axis.\n |      \n |      Returns True unless there at least one element within a series or\n |      along a Dataframe axis that is False or equivalent (e.g. zero or\n |      empty).\n |      \n |      Parameters\n |      ----------\n |      axis : {0 or 'index', 1 or 'columns', None}, default 0\n |          Indicate which axis or axes should be reduced.\n |      \n |          * 0 / 'index' : reduce the index, return a Series whose index is the\n |            original column labels.\n |          * 1 / 'columns' : reduce the columns, return a Series whose index is the\n |            original index.\n |          * None : reduce all axes, return a scalar.\n |      \n |      bool_only : bool, default None\n |          Include only boolean columns. If None, will attempt to use everything,\n |          then use only boolean data. Not implemented for Series.\n |      skipna : bool, default True\n |          Exclude NA/null values. If the entire row/column is NA and skipna is\n |          True, then the result will be True, as for an empty row/column.\n |          If skipna is False, then NA are treated as True, because these are not\n |          equal to zero.\n |      level : int or level name, default None\n |          If the axis is a MultiIndex (hierarchical), count along a\n |          particular level, collapsing into a Series.\n |      **kwargs : any, default None\n |          Additional keywords have no effect but might be accepted for\n |          compatibility with NumPy.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame\n |          If level is specified, then, DataFrame is returned; otherwise, Series\n |          is returned.\n |      \n |      See Also\n |      --------\n |      Series.all : Return True if all elements are True.\n |      DataFrame.any : Return True if one (or more) elements are True.\n |      \n |      Examples\n |      --------\n |      **Series**\n |      \n |      >>> pd.Series([True, True]).all()\n |      True\n |      >>> pd.Series([True, False]).all()\n |      False\n |      >>> pd.Series([]).all()\n |      True\n |      >>> pd.Series([np.nan]).all()\n |      True\n |      >>> pd.Series([np.nan]).all(skipna=False)\n |      True\n |      \n |      **DataFrames**\n |      \n |      Create a dataframe from a dictionary.\n |      \n |      >>> df = pd.DataFrame({'col1': [True, True], 'col2': [True, False]})\n |      >>> df\n |         col1   col2\n |      0  True   True\n |      1  True  False\n |      \n |      Default behaviour checks if column-wise values all return True.\n |      \n |      >>> df.all()\n |      col1     True\n |      col2    False\n |      dtype: bool\n |      \n |      Specify ``axis='columns'`` to check if row-wise values all return True.\n |      \n |      >>> df.all(axis='columns')\n |      0     True\n |      1    False\n |      dtype: bool\n |      \n |      Or ``axis=None`` for whether every value is True.\n |      \n |      >>> df.all(axis=None)\n |      False\n |  \n |  any(self, axis=0, bool_only=None, skipna=True, level=None, **kwargs)\n |      Return whether any element is True, potentially over an axis.\n |      \n |      Returns False unless there at least one element within a series or\n |      along a Dataframe axis that is True or equivalent (e.g. non-zero or\n |      non-empty).\n |      \n |      Parameters\n |      ----------\n |      axis : {0 or 'index', 1 or 'columns', None}, default 0\n |          Indicate which axis or axes should be reduced.\n |      \n |          * 0 / 'index' : reduce the index, return a Series whose index is the\n |            original column labels.\n |          * 1 / 'columns' : reduce the columns, return a Series whose index is the\n |            original index.\n |          * None : reduce all axes, return a scalar.\n |      \n |      bool_only : bool, default None\n |          Include only boolean columns. If None, will attempt to use everything,\n |          then use only boolean data. Not implemented for Series.\n |      skipna : bool, default True\n |          Exclude NA/null values. If the entire row/column is NA and skipna is\n |          True, then the result will be False, as for an empty row/column.\n |          If skipna is False, then NA are treated as True, because these are not\n |          equal to zero.\n |      level : int or level name, default None\n |          If the axis is a MultiIndex (hierarchical), count along a\n |          particular level, collapsing into a Series.\n |      **kwargs : any, default None\n |          Additional keywords have no effect but might be accepted for\n |          compatibility with NumPy.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame\n |          If level is specified, then, DataFrame is returned; otherwise, Series\n |          is returned.\n |      \n |      See Also\n |      --------\n |      numpy.any : Numpy version of this method.\n |      Series.any : Return whether any element is True.\n |      Series.all : Return whether all elements are True.\n |      DataFrame.any : Return whether any element is True over requested axis.\n |      DataFrame.all : Return whether all elements are True over requested axis.\n |      \n |      Examples\n |      --------\n |      **Series**\n |      \n |      For Series input, the output is a scalar indicating whether any element\n |      is True.\n |      \n |      >>> pd.Series([False, False]).any()\n |      False\n |      >>> pd.Series([True, False]).any()\n |      True\n |      >>> pd.Series([]).any()\n |      False\n |      >>> pd.Series([np.nan]).any()\n |      False\n |      >>> pd.Series([np.nan]).any(skipna=False)\n |      True\n |      \n |      **DataFrame**\n |      \n |      Whether each column contains at least one True element (the default).\n |      \n |      >>> df = pd.DataFrame({\"A\": [1, 2], \"B\": [0, 2], \"C\": [0, 0]})\n |      >>> df\n |         A  B  C\n |      0  1  0  0\n |      1  2  2  0\n |      \n |      >>> df.any()\n |      A     True\n |      B     True\n |      C    False\n |      dtype: bool\n |      \n |      Aggregating over the columns.\n |      \n |      >>> df = pd.DataFrame({\"A\": [True, False], \"B\": [1, 2]})\n |      >>> df\n |             A  B\n |      0   True  1\n |      1  False  2\n |      \n |      >>> df.any(axis='columns')\n |      0    True\n |      1    True\n |      dtype: bool\n |      \n |      >>> df = pd.DataFrame({\"A\": [True, False], \"B\": [1, 0]})\n |      >>> df\n |             A  B\n |      0   True  1\n |      1  False  0\n |      \n |      >>> df.any(axis='columns')\n |      0    True\n |      1    False\n |      dtype: bool\n |      \n |      Aggregating over the entire DataFrame with ``axis=None``.\n |      \n |      >>> df.any(axis=None)\n |      True\n |      \n |      `any` for an empty DataFrame is an empty Series.\n |      \n |      >>> pd.DataFrame([]).any()\n |      Series([], dtype: bool)\n |  \n |  append(self, other, ignore_index=False, verify_integrity=False, sort=None)\n |      Append rows of `other` to the end of caller, returning a new object.\n |      \n |      Columns in `other` that are not in the caller are added as new columns.\n |      \n |      Parameters\n |      ----------\n |      other : DataFrame or Series/dict-like object, or list of these\n |          The data to append.\n |      ignore_index : boolean, default False\n |          If True, do not use the index labels.\n |      verify_integrity : boolean, default False\n |          If True, raise ValueError on creating index with duplicates.\n |      sort : boolean, default None\n |          Sort columns if the columns of `self` and `other` are not aligned.\n |          The default sorting is deprecated and will change to not-sorting\n |          in a future version of pandas. Explicitly pass ``sort=True`` to\n |          silence the warning and sort. Explicitly pass ``sort=False`` to\n |          silence the warning and not sort.\n |      \n |          .. versionadded:: 0.23.0\n |      \n |      Returns\n |      -------\n |      DataFrame\n |      \n |      See Also\n |      --------\n |      concat : General function to concatenate DataFrame or Series objects.\n |      \n |      Notes\n |      -----\n |      If a list of dict/series is passed and the keys are all contained in\n |      the DataFrame's index, the order of the columns in the resulting\n |      DataFrame will be unchanged.\n |      \n |      Iteratively appending rows to a DataFrame can be more computationally\n |      intensive than a single concatenate. A better solution is to append\n |      those rows to a list and then concatenate the list with the original\n |      DataFrame all at once.\n |      \n |      Examples\n |      --------\n |      \n |      >>> df = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB'))\n |      >>> df\n |         A  B\n |      0  1  2\n |      1  3  4\n |      >>> df2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB'))\n |      >>> df.append(df2)\n |         A  B\n |      0  1  2\n |      1  3  4\n |      0  5  6\n |      1  7  8\n |      \n |      With `ignore_index` set to True:\n |      \n |      >>> df.append(df2, ignore_index=True)\n |         A  B\n |      0  1  2\n |      1  3  4\n |      2  5  6\n |      3  7  8\n |      \n |      The following, while not recommended methods for generating DataFrames,\n |      show two ways to generate a DataFrame from multiple data sources.\n |      \n |      Less efficient:\n |      \n |      >>> df = pd.DataFrame(columns=['A'])\n |      >>> for i in range(5):\n |      ...     df = df.append({'A': i}, ignore_index=True)\n |      >>> df\n |         A\n |      0  0\n |      1  1\n |      2  2\n |      3  3\n |      4  4\n |      \n |      More efficient:\n |      \n |      >>> pd.concat([pd.DataFrame([i], columns=['A']) for i in range(5)],\n |      ...           ignore_index=True)\n |         A\n |      0  0\n |      1  1\n |      2  2\n |      3  3\n |      4  4\n |  \n |  apply(self, func, axis=0, broadcast=None, raw=False, reduce=None, result_type=None, args=(), **kwds)\n |      Apply a function along an axis of the DataFrame.\n |      \n |      Objects passed to the function are Series objects whose index is\n |      either the DataFrame's index (``axis=0``) or the DataFrame's columns\n |      (``axis=1``). By default (``result_type=None``), the final return type\n |      is inferred from the return type of the applied function. Otherwise,\n |      it depends on the `result_type` argument.\n |      \n |      Parameters\n |      ----------\n |      func : function\n |          Function to apply to each column or row.\n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          Axis along which the function is applied:\n |      \n |          * 0 or 'index': apply function to each column.\n |          * 1 or 'columns': apply function to each row.\n |      broadcast : bool, optional\n |          Only relevant for aggregation functions:\n |      \n |          * ``False`` or ``None`` : returns a Series whose length is the\n |            length of the index or the number of columns (based on the\n |            `axis` parameter)\n |          * ``True`` : results will be broadcast to the original shape\n |            of the frame, the original index and columns will be retained.\n |      \n |          .. deprecated:: 0.23.0\n |             This argument will be removed in a future version, replaced\n |             by result_type='broadcast'.\n |      \n |      raw : bool, default False\n |          * ``False`` : passes each row or column as a Series to the\n |            function.\n |          * ``True`` : the passed function will receive ndarray objects\n |            instead.\n |            If you are just applying a NumPy reduction function this will\n |            achieve much better performance.\n |      reduce : bool or None, default None\n |          Try to apply reduction procedures. If the DataFrame is empty,\n |          `apply` will use `reduce` to determine whether the result\n |          should be a Series or a DataFrame. If ``reduce=None`` (the\n |          default), `apply`'s return value will be guessed by calling\n |          `func` on an empty Series\n |          (note: while guessing, exceptions raised by `func` will be\n |          ignored).\n |          If ``reduce=True`` a Series will always be returned, and if\n |          ``reduce=False`` a DataFrame will always be returned.\n |      \n |          .. deprecated:: 0.23.0\n |             This argument will be removed in a future version, replaced\n |             by ``result_type='reduce'``.\n |      \n |      result_type : {'expand', 'reduce', 'broadcast', None}, default None\n |          These only act when ``axis=1`` (columns):\n |      \n |          * 'expand' : list-like results will be turned into columns.\n |          * 'reduce' : returns a Series if possible rather than expanding\n |            list-like results. This is the opposite of 'expand'.\n |          * 'broadcast' : results will be broadcast to the original shape\n |            of the DataFrame, the original index and columns will be\n |            retained.\n |      \n |          The default behaviour (None) depends on the return value of the\n |          applied function: list-like results will be returned as a Series\n |          of those. However if the apply function returns a Series these\n |          are expanded to columns.\n |      \n |          .. versionadded:: 0.23.0\n |      \n |      args : tuple\n |          Positional arguments to pass to `func` in addition to the\n |          array/series.\n |      **kwds\n |          Additional keyword arguments to pass as keywords arguments to\n |          `func`.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame\n |          Result of applying ``func`` along the given axis of the\n |          DataFrame.\n |      \n |      See Also\n |      --------\n |      DataFrame.applymap: For elementwise operations.\n |      DataFrame.aggregate: Only perform aggregating type operations.\n |      DataFrame.transform: Only perform transforming type operations.\n |      \n |      Notes\n |      -----\n |      In the current implementation apply calls `func` twice on the\n |      first column/row to decide whether it can take a fast or slow\n |      code path. This can lead to unexpected behavior if `func` has\n |      side-effects, as they will take effect twice for the first\n |      column/row.\n |      \n |      Examples\n |      --------\n |      \n |      >>> df = pd.DataFrame([[4, 9]] * 3, columns=['A', 'B'])\n |      >>> df\n |         A  B\n |      0  4  9\n |      1  4  9\n |      2  4  9\n |      \n |      Using a numpy universal function (in this case the same as\n |      ``np.sqrt(df)``):\n |      \n |      >>> df.apply(np.sqrt)\n |           A    B\n |      0  2.0  3.0\n |      1  2.0  3.0\n |      2  2.0  3.0\n |      \n |      Using a reducing function on either axis\n |      \n |      >>> df.apply(np.sum, axis=0)\n |      A    12\n |      B    27\n |      dtype: int64\n |      \n |      >>> df.apply(np.sum, axis=1)\n |      0    13\n |      1    13\n |      2    13\n |      dtype: int64\n |      \n |      Returning a list-like will result in a Series\n |      \n |      >>> df.apply(lambda x: [1, 2], axis=1)\n |      0    [1, 2]\n |      1    [1, 2]\n |      2    [1, 2]\n |      dtype: object\n |      \n |      Passing result_type='expand' will expand list-like results\n |      to columns of a Dataframe\n |      \n |      >>> df.apply(lambda x: [1, 2], axis=1, result_type='expand')\n |         0  1\n |      0  1  2\n |      1  1  2\n |      2  1  2\n |      \n |      Returning a Series inside the function is similar to passing\n |      ``result_type='expand'``. The resulting column names\n |      will be the Series index.\n |      \n |      >>> df.apply(lambda x: pd.Series([1, 2], index=['foo', 'bar']), axis=1)\n |         foo  bar\n |      0    1    2\n |      1    1    2\n |      2    1    2\n |      \n |      Passing ``result_type='broadcast'`` will ensure the same shape\n |      result, whether list-like or scalar is returned by the function,\n |      and broadcast it along the axis. The resulting column names will\n |      be the originals.\n |      \n |      >>> df.apply(lambda x: [1, 2], axis=1, result_type='broadcast')\n |         A  B\n |      0  1  2\n |      1  1  2\n |      2  1  2\n |  \n |  applymap(self, func)\n |      Apply a function to a Dataframe elementwise.\n |      \n |      This method applies a function that accepts and returns a scalar\n |      to every element of a DataFrame.\n |      \n |      Parameters\n |      ----------\n |      func : callable\n |          Python function, returns a single value from a single value.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          Transformed DataFrame.\n |      \n |      See Also\n |      --------\n |      DataFrame.apply : Apply a function along input axis of DataFrame.\n |      \n |      Notes\n |      -----\n |      In the current implementation applymap calls `func` twice on the\n |      first column/row to decide whether it can take a fast or slow\n |      code path. This can lead to unexpected behavior if `func` has\n |      side-effects, as they will take effect twice for the first\n |      column/row.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame([[1, 2.12], [3.356, 4.567]])\n |      >>> df\n |             0      1\n |      0  1.000  2.120\n |      1  3.356  4.567\n |      \n |      >>> df.applymap(lambda x: len(str(x)))\n |         0  1\n |      0  3  4\n |      1  5  5\n |      \n |      Note that a vectorized version of `func` often exists, which will\n |      be much faster. You could square each number elementwise.\n |      \n |      >>> df.applymap(lambda x: x**2)\n |                 0          1\n |      0   1.000000   4.494400\n |      1  11.262736  20.857489\n |      \n |      But it's better to avoid applymap in that case.\n |      \n |      >>> df ** 2\n |                 0          1\n |      0   1.000000   4.494400\n |      1  11.262736  20.857489\n |  \n |  assign(self, **kwargs)\n |      Assign new columns to a DataFrame.\n |      \n |      Returns a new object with all original columns in addition to new ones.\n |      Existing columns that are re-assigned will be overwritten.\n |      \n |      Parameters\n |      ----------\n |      **kwargs : dict of {str: callable or Series}\n |          The column names are keywords. If the values are\n |          callable, they are computed on the DataFrame and\n |          assigned to the new columns. The callable must not\n |          change input DataFrame (though pandas doesn't check it).\n |          If the values are not callable, (e.g. a Series, scalar, or array),\n |          they are simply assigned.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          A new DataFrame with the new columns in addition to\n |          all the existing columns.\n |      \n |      Notes\n |      -----\n |      Assigning multiple columns within the same ``assign`` is possible.\n |      For Python 3.6 and above, later items in '\\*\\*kwargs' may refer to\n |      newly created or modified columns in 'df'; items are computed and\n |      assigned into 'df' in order.  For Python 3.5 and below, the order of\n |      keyword arguments is not specified, you cannot refer to newly created\n |      or modified columns. All items are computed first, and then assigned\n |      in alphabetical order.\n |      \n |      .. versionchanged :: 0.23.0\n |      \n |         Keyword argument order is maintained for Python 3.6 and later.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'temp_c': [17.0, 25.0]},\n |      ...                   index=['Portland', 'Berkeley'])\n |      >>> df\n |                temp_c\n |      Portland    17.0\n |      Berkeley    25.0\n |      \n |      Where the value is a callable, evaluated on `df`:\n |      \n |      >>> df.assign(temp_f=lambda x: x.temp_c * 9 / 5 + 32)\n |                temp_c  temp_f\n |      Portland    17.0    62.6\n |      Berkeley    25.0    77.0\n |      \n |      Alternatively, the same behavior can be achieved by directly\n |      referencing an existing Series or sequence:\n |      \n |      >>> df.assign(temp_f=df['temp_c'] * 9 / 5 + 32)\n |                temp_c  temp_f\n |      Portland    17.0    62.6\n |      Berkeley    25.0    77.0\n |      \n |      In Python 3.6+, you can create multiple columns within the same assign\n |      where one of the columns depends on another one defined within the same\n |      assign:\n |      \n |      >>> df.assign(temp_f=lambda x: x['temp_c'] * 9 / 5 + 32,\n |      ...           temp_k=lambda x: (x['temp_f'] +  459.67) * 5 / 9)\n |                temp_c  temp_f  temp_k\n |      Portland    17.0    62.6  290.15\n |      Berkeley    25.0    77.0  298.15\n |  \n |  boxplot = boxplot_frame(self, column=None, by=None, ax=None, fontsize=None, rot=0, grid=True, figsize=None, layout=None, return_type=None, **kwds)\n |      Make a box plot from DataFrame columns.\n |      \n |      Make a box-and-whisker plot from DataFrame columns, optionally grouped\n |      by some other columns. A box plot is a method for graphically depicting\n |      groups of numerical data through their quartiles.\n |      The box extends from the Q1 to Q3 quartile values of the data,\n |      with a line at the median (Q2). The whiskers extend from the edges\n |      of box to show the range of the data. The position of the whiskers\n |      is set by default to `1.5 * IQR (IQR = Q3 - Q1)` from the edges of the box.\n |      Outlier points are those past the end of the whiskers.\n |      \n |      For further details see\n |      Wikipedia's entry for `boxplot <https://en.wikipedia.org/wiki/Box_plot>`_.\n |      \n |      Parameters\n |      ----------\n |      column : str or list of str, optional\n |          Column name or list of names, or vector.\n |          Can be any valid input to :meth:`pandas.DataFrame.groupby`.\n |      by : str or array-like, optional\n |          Column in the DataFrame to :meth:`pandas.DataFrame.groupby`.\n |          One box-plot will be done per value of columns in `by`.\n |      ax : object of class matplotlib.axes.Axes, optional\n |          The matplotlib axes to be used by boxplot.\n |      fontsize : float or str\n |          Tick label font size in points or as a string (e.g., `large`).\n |      rot : int or float, default 0\n |          The rotation angle of labels (in degrees)\n |          with respect to the screen coordinate system.\n |      grid : bool, default True\n |          Setting this to True will show the grid.\n |      figsize : A tuple (width, height) in inches\n |          The size of the figure to create in matplotlib.\n |      layout : tuple (rows, columns), optional\n |          For example, (3, 5) will display the subplots\n |          using 3 columns and 5 rows, starting from the top-left.\n |      return_type : {'axes', 'dict', 'both'} or None, default 'axes'\n |          The kind of object to return. The default is ``axes``.\n |      \n |          * 'axes' returns the matplotlib axes the boxplot is drawn on.\n |          * 'dict' returns a dictionary whose values are the matplotlib\n |            Lines of the boxplot.\n |          * 'both' returns a namedtuple with the axes and dict.\n |          * when grouping with ``by``, a Series mapping columns to\n |            ``return_type`` is returned.\n |      \n |            If ``return_type`` is `None`, a NumPy array\n |            of axes with the same shape as ``layout`` is returned.\n |      **kwds\n |          All other plotting keyword arguments to be passed to\n |          :func:`matplotlib.pyplot.boxplot`.\n |      \n |      Returns\n |      -------\n |      result\n |          See Notes.\n |      \n |      See Also\n |      --------\n |      Series.plot.hist: Make a histogram.\n |      matplotlib.pyplot.boxplot : Matplotlib equivalent plot.\n |      \n |      Notes\n |      -----\n |      The return type depends on the `return_type` parameter:\n |      \n |      * 'axes' : object of class matplotlib.axes.Axes\n |      * 'dict' : dict of matplotlib.lines.Line2D objects\n |      * 'both' : a namedtuple with structure (ax, lines)\n |      \n |      For data grouped with ``by``, return a Series of the above or a numpy\n |      array:\n |      \n |      * :class:`~pandas.Series`\n |      * :class:`~numpy.array` (for ``return_type = None``)\n |      \n |      Use ``return_type='dict'`` when you want to tweak the appearance\n |      of the lines after plotting. In this case a dict containing the Lines\n |      making up the boxes, caps, fliers, medians, and whiskers is returned.\n |      \n |      Examples\n |      --------\n |      \n |      Boxplots can be created for every column in the dataframe\n |      by ``df.boxplot()`` or indicating the columns to be used:\n |      \n |      .. plot::\n |          :context: close-figs\n |      \n |          >>> np.random.seed(1234)\n |          >>> df = pd.DataFrame(np.random.randn(10,4),\n |          ...                   columns=['Col1', 'Col2', 'Col3', 'Col4'])\n |          >>> boxplot = df.boxplot(column=['Col1', 'Col2', 'Col3'])\n |      \n |      Boxplots of variables distributions grouped by the values of a third\n |      variable can be created using the option ``by``. For instance:\n |      \n |      .. plot::\n |          :context: close-figs\n |      \n |          >>> df = pd.DataFrame(np.random.randn(10, 2),\n |          ...                   columns=['Col1', 'Col2'])\n |          >>> df['X'] = pd.Series(['A', 'A', 'A', 'A', 'A',\n |          ...                      'B', 'B', 'B', 'B', 'B'])\n |          >>> boxplot = df.boxplot(by='X')\n |      \n |      A list of strings (i.e. ``['X', 'Y']``) can be passed to boxplot\n |      in order to group the data by combination of the variables in the x-axis:\n |      \n |      .. plot::\n |          :context: close-figs\n |      \n |          >>> df = pd.DataFrame(np.random.randn(10,3),\n |          ...                   columns=['Col1', 'Col2', 'Col3'])\n |          >>> df['X'] = pd.Series(['A', 'A', 'A', 'A', 'A',\n |          ...                      'B', 'B', 'B', 'B', 'B'])\n |          >>> df['Y'] = pd.Series(['A', 'B', 'A', 'B', 'A',\n |          ...                      'B', 'A', 'B', 'A', 'B'])\n |          >>> boxplot = df.boxplot(column=['Col1', 'Col2'], by=['X', 'Y'])\n |      \n |      The layout of boxplot can be adjusted giving a tuple to ``layout``:\n |      \n |      .. plot::\n |          :context: close-figs\n |      \n |          >>> boxplot = df.boxplot(column=['Col1', 'Col2'], by='X',\n |          ...                      layout=(2, 1))\n |      \n |      Additional formatting can be done to the boxplot, like suppressing the grid\n |      (``grid=False``), rotating the labels in the x-axis (i.e. ``rot=45``)\n |      or changing the fontsize (i.e. ``fontsize=15``):\n |      \n |      .. plot::\n |          :context: close-figs\n |      \n |          >>> boxplot = df.boxplot(grid=False, rot=45, fontsize=15)\n |      \n |      The parameter ``return_type`` can be used to select the type of element\n |      returned by `boxplot`.  When ``return_type='axes'`` is selected,\n |      the matplotlib axes on which the boxplot is drawn are returned:\n |      \n |          >>> boxplot = df.boxplot(column=['Col1','Col2'], return_type='axes')\n |          >>> type(boxplot)\n |          <class 'matplotlib.axes._subplots.AxesSubplot'>\n |      \n |      When grouping with ``by``, a Series mapping columns to ``return_type``\n |      is returned:\n |      \n |          >>> boxplot = df.boxplot(column=['Col1', 'Col2'], by='X',\n |          ...                      return_type='axes')\n |          >>> type(boxplot)\n |          <class 'pandas.core.series.Series'>\n |      \n |      If ``return_type`` is `None`, a NumPy array of axes with the same shape\n |      as ``layout`` is returned:\n |      \n |          >>> boxplot =  df.boxplot(column=['Col1', 'Col2'], by='X',\n |          ...                       return_type=None)\n |          >>> type(boxplot)\n |          <class 'numpy.ndarray'>\n |  \n |  combine(self, other, func, fill_value=None, overwrite=True)\n |      Perform column-wise combine with another DataFrame.\n |      \n |      Combines a DataFrame with `other` DataFrame using `func`\n |      to element-wise combine columns. The row and column indexes of the\n |      resulting DataFrame will be the union of the two.\n |      \n |      Parameters\n |      ----------\n |      other : DataFrame\n |          The DataFrame to merge column-wise.\n |      func : function\n |          Function that takes two series as inputs and return a Series or a\n |          scalar. Used to merge the two dataframes column by columns.\n |      fill_value : scalar value, default None\n |          The value to fill NaNs with prior to passing any column to the\n |          merge func.\n |      overwrite : bool, default True\n |          If True, columns in `self` that do not exist in `other` will be\n |          overwritten with NaNs.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          Combination of the provided DataFrames.\n |      \n |      See Also\n |      --------\n |      DataFrame.combine_first : Combine two DataFrame objects and default to\n |          non-null values in frame calling the method.\n |      \n |      Examples\n |      --------\n |      Combine using a simple function that chooses the smaller column.\n |      \n |      >>> df1 = pd.DataFrame({'A': [0, 0], 'B': [4, 4]})\n |      >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})\n |      >>> take_smaller = lambda s1, s2: s1 if s1.sum() < s2.sum() else s2\n |      >>> df1.combine(df2, take_smaller)\n |         A  B\n |      0  0  3\n |      1  0  3\n |      \n |      Example using a true element-wise combine function.\n |      \n |      >>> df1 = pd.DataFrame({'A': [5, 0], 'B': [2, 4]})\n |      >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})\n |      >>> df1.combine(df2, np.minimum)\n |         A  B\n |      0  1  2\n |      1  0  3\n |      \n |      Using `fill_value` fills Nones prior to passing the column to the\n |      merge function.\n |      \n |      >>> df1 = pd.DataFrame({'A': [0, 0], 'B': [None, 4]})\n |      >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})\n |      >>> df1.combine(df2, take_smaller, fill_value=-5)\n |         A    B\n |      0  0 -5.0\n |      1  0  4.0\n |      \n |      However, if the same element in both dataframes is None, that None\n |      is preserved\n |      \n |      >>> df1 = pd.DataFrame({'A': [0, 0], 'B': [None, 4]})\n |      >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [None, 3]})\n |      >>> df1.combine(df2, take_smaller, fill_value=-5)\n |          A    B\n |      0  0 -5.0\n |      1  0  3.0\n |      \n |      Example that demonstrates the use of `overwrite` and behavior when\n |      the axis differ between the dataframes.\n |      \n |      >>> df1 = pd.DataFrame({'A': [0, 0], 'B': [4, 4]})\n |      >>> df2 = pd.DataFrame({'B': [3, 3], 'C': [-10, 1], }, index=[1, 2])\n |      >>> df1.combine(df2, take_smaller)\n |           A    B     C\n |      0  NaN  NaN   NaN\n |      1  NaN  3.0 -10.0\n |      2  NaN  3.0   1.0\n |      \n |      >>> df1.combine(df2, take_smaller, overwrite=False)\n |           A    B     C\n |      0  0.0  NaN   NaN\n |      1  0.0  3.0 -10.0\n |      2  NaN  3.0   1.0\n |      \n |      Demonstrating the preference of the passed in dataframe.\n |      \n |      >>> df2 = pd.DataFrame({'B': [3, 3], 'C': [1, 1], }, index=[1, 2])\n |      >>> df2.combine(df1, take_smaller)\n |         A    B   C\n |      0  0.0  NaN NaN\n |      1  0.0  3.0 NaN\n |      2  NaN  3.0 NaN\n |      \n |      >>> df2.combine(df1, take_smaller, overwrite=False)\n |           A    B   C\n |      0  0.0  NaN NaN\n |      1  0.0  3.0 1.0\n |      2  NaN  3.0 1.0\n |  \n |  combine_first(self, other)\n |      Update null elements with value in the same location in `other`.\n |      \n |      Combine two DataFrame objects by filling null values in one DataFrame\n |      with non-null values from other DataFrame. The row and column indexes\n |      of the resulting DataFrame will be the union of the two.\n |      \n |      Parameters\n |      ----------\n |      other : DataFrame\n |          Provided DataFrame to use to fill null values.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |      \n |      See Also\n |      --------\n |      DataFrame.combine : Perform series-wise operation on two DataFrames\n |          using a given function.\n |      \n |      Examples\n |      --------\n |      \n |      >>> df1 = pd.DataFrame({'A': [None, 0], 'B': [None, 4]})\n |      >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})\n |      >>> df1.combine_first(df2)\n |           A    B\n |      0  1.0  3.0\n |      1  0.0  4.0\n |      \n |      Null values still persist if the location of that null value\n |      does not exist in `other`\n |      \n |      >>> df1 = pd.DataFrame({'A': [None, 0], 'B': [4, None]})\n |      >>> df2 = pd.DataFrame({'B': [3, 3], 'C': [1, 1]}, index=[1, 2])\n |      >>> df1.combine_first(df2)\n |           A    B    C\n |      0  NaN  4.0  NaN\n |      1  0.0  3.0  1.0\n |      2  NaN  3.0  1.0\n |  \n |  compound(self, axis=None, skipna=None, level=None)\n |      Return the compound percentage of the values for the requested axis.\n |      \n |      .. deprecated:: 0.25.0\n |      \n |      Parameters\n |      ----------\n |      axis : {index (0), columns (1)}\n |          Axis for the function to be applied on.\n |      skipna : bool, default True\n |          Exclude NA/null values when computing the result.\n |      level : int or level name, default None\n |          If the axis is a MultiIndex (hierarchical), count along a\n |          particular level, collapsing into a Series.\n |      numeric_only : bool, default None\n |          Include only float, int, boolean columns. If None, will attempt to use\n |          everything, then use only numeric data. Not implemented for Series.\n |      **kwargs\n |          Additional keyword arguments to be passed to the function.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame (if level specified)\n |  \n |  corr(self, method='pearson', min_periods=1)\n |      Compute pairwise correlation of columns, excluding NA/null values.\n |      \n |      Parameters\n |      ----------\n |      method : {'pearson', 'kendall', 'spearman'} or callable\n |          * pearson : standard correlation coefficient\n |          * kendall : Kendall Tau correlation coefficient\n |          * spearman : Spearman rank correlation\n |          * callable: callable with input two 1d ndarrays\n |              and returning a float. Note that the returned matrix from corr\n |              will have 1 along the diagonals and will be symmetric\n |              regardless of the callable's behavior\n |              .. versionadded:: 0.24.0\n |      \n |      min_periods : int, optional\n |          Minimum number of observations required per pair of columns\n |          to have a valid result. Currently only available for Pearson\n |          and Spearman correlation.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          Correlation matrix.\n |      \n |      See Also\n |      --------\n |      DataFrame.corrwith\n |      Series.corr\n |      \n |      Examples\n |      --------\n |      >>> def histogram_intersection(a, b):\n |      ...     v = np.minimum(a, b).sum().round(decimals=1)\n |      ...     return v\n |      >>> df = pd.DataFrame([(.2, .3), (.0, .6), (.6, .0), (.2, .1)],\n |      ...                   columns=['dogs', 'cats'])\n |      >>> df.corr(method=histogram_intersection)\n |            dogs  cats\n |      dogs   1.0   0.3\n |      cats   0.3   1.0\n |  \n |  corrwith(self, other, axis=0, drop=False, method='pearson')\n |      Compute pairwise correlation between rows or columns of DataFrame\n |      with rows or columns of Series or DataFrame.  DataFrames are first\n |      aligned along both axes before computing the correlations.\n |      \n |      Parameters\n |      ----------\n |      other : DataFrame, Series\n |          Object with which to compute correlations.\n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          0 or 'index' to compute column-wise, 1 or 'columns' for row-wise.\n |      drop : bool, default False\n |          Drop missing indices from result.\n |      method : {'pearson', 'kendall', 'spearman'} or callable\n |          * pearson : standard correlation coefficient\n |          * kendall : Kendall Tau correlation coefficient\n |          * spearman : Spearman rank correlation\n |          * callable: callable with input two 1d ndarrays\n |              and returning a float\n |      \n |          .. versionadded:: 0.24.0\n |      \n |      Returns\n |      -------\n |      Series\n |          Pairwise correlations.\n |      \n |      See Also\n |      --------\n |      DataFrame.corr\n |  \n |  count(self, axis=0, level=None, numeric_only=False)\n |      Count non-NA cells for each column or row.\n |      \n |      The values `None`, `NaN`, `NaT`, and optionally `numpy.inf` (depending\n |      on `pandas.options.mode.use_inf_as_na`) are considered NA.\n |      \n |      Parameters\n |      ----------\n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          If 0 or 'index' counts are generated for each column.\n |          If 1 or 'columns' counts are generated for each **row**.\n |      level : int or str, optional\n |          If the axis is a `MultiIndex` (hierarchical), count along a\n |          particular `level`, collapsing into a `DataFrame`.\n |          A `str` specifies the level name.\n |      numeric_only : bool, default False\n |          Include only `float`, `int` or `boolean` data.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame\n |          For each column/row the number of non-NA/null entries.\n |          If `level` is specified returns a `DataFrame`.\n |      \n |      See Also\n |      --------\n |      Series.count: Number of non-NA elements in a Series.\n |      DataFrame.shape: Number of DataFrame rows and columns (including NA\n |          elements).\n |      DataFrame.isna: Boolean same-sized DataFrame showing places of NA\n |          elements.\n |      \n |      Examples\n |      --------\n |      Constructing DataFrame from a dictionary:\n |      \n |      >>> df = pd.DataFrame({\"Person\":\n |      ...                    [\"John\", \"Myla\", \"Lewis\", \"John\", \"Myla\"],\n |      ...                    \"Age\": [24., np.nan, 21., 33, 26],\n |      ...                    \"Single\": [False, True, True, True, False]})\n |      >>> df\n |         Person   Age  Single\n |      0    John  24.0   False\n |      1    Myla   NaN    True\n |      2   Lewis  21.0    True\n |      3    John  33.0    True\n |      4    Myla  26.0   False\n |      \n |      Notice the uncounted NA values:\n |      \n |      >>> df.count()\n |      Person    5\n |      Age       4\n |      Single    5\n |      dtype: int64\n |      \n |      Counts for each **row**:\n |      \n |      >>> df.count(axis='columns')\n |      0    3\n |      1    2\n |      2    3\n |      3    3\n |      4    3\n |      dtype: int64\n |      \n |      Counts for one level of a `MultiIndex`:\n |      \n |      >>> df.set_index([\"Person\", \"Single\"]).count(level=\"Person\")\n |              Age\n |      Person\n |      John      2\n |      Lewis     1\n |      Myla      1\n |  \n |  cov(self, min_periods=None)\n |      Compute pairwise covariance of columns, excluding NA/null values.\n |      \n |      Compute the pairwise covariance among the series of a DataFrame.\n |      The returned data frame is the `covariance matrix\n |      <https://en.wikipedia.org/wiki/Covariance_matrix>`__ of the columns\n |      of the DataFrame.\n |      \n |      Both NA and null values are automatically excluded from the\n |      calculation. (See the note below about bias from missing values.)\n |      A threshold can be set for the minimum number of\n |      observations for each value created. Comparisons with observations\n |      below this threshold will be returned as ``NaN``.\n |      \n |      This method is generally used for the analysis of time series data to\n |      understand the relationship between different measures\n |      across time.\n |      \n |      Parameters\n |      ----------\n |      min_periods : int, optional\n |          Minimum number of observations required per pair of columns\n |          to have a valid result.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          The covariance matrix of the series of the DataFrame.\n |      \n |      See Also\n |      --------\n |      Series.cov : Compute covariance with another Series.\n |      core.window.EWM.cov: Exponential weighted sample covariance.\n |      core.window.Expanding.cov : Expanding sample covariance.\n |      core.window.Rolling.cov : Rolling sample covariance.\n |      \n |      Notes\n |      -----\n |      Returns the covariance matrix of the DataFrame's time series.\n |      The covariance is normalized by N-1.\n |      \n |      For DataFrames that have Series that are missing data (assuming that\n |      data is `missing at random\n |      <https://en.wikipedia.org/wiki/Missing_data#Missing_at_random>`__)\n |      the returned covariance matrix will be an unbiased estimate\n |      of the variance and covariance between the member Series.\n |      \n |      However, for many applications this estimate may not be acceptable\n |      because the estimate covariance matrix is not guaranteed to be positive\n |      semi-definite. This could lead to estimate correlations having\n |      absolute values which are greater than one, and/or a non-invertible\n |      covariance matrix. See `Estimation of covariance matrices\n |      <http://en.wikipedia.org/w/index.php?title=Estimation_of_covariance_\n |      matrices>`__ for more details.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame([(1, 2), (0, 3), (2, 0), (1, 1)],\n |      ...                   columns=['dogs', 'cats'])\n |      >>> df.cov()\n |                dogs      cats\n |      dogs  0.666667 -1.000000\n |      cats -1.000000  1.666667\n |      \n |      >>> np.random.seed(42)\n |      >>> df = pd.DataFrame(np.random.randn(1000, 5),\n |      ...                   columns=['a', 'b', 'c', 'd', 'e'])\n |      >>> df.cov()\n |                a         b         c         d         e\n |      a  0.998438 -0.020161  0.059277 -0.008943  0.014144\n |      b -0.020161  1.059352 -0.008543 -0.024738  0.009826\n |      c  0.059277 -0.008543  1.010670 -0.001486 -0.000271\n |      d -0.008943 -0.024738 -0.001486  0.921297 -0.013692\n |      e  0.014144  0.009826 -0.000271 -0.013692  0.977795\n |      \n |      **Minimum number of periods**\n |      \n |      This method also supports an optional ``min_periods`` keyword\n |      that specifies the required minimum number of non-NA observations for\n |      each column pair in order to have a valid result:\n |      \n |      >>> np.random.seed(42)\n |      >>> df = pd.DataFrame(np.random.randn(20, 3),\n |      ...                   columns=['a', 'b', 'c'])\n |      >>> df.loc[df.index[:5], 'a'] = np.nan\n |      >>> df.loc[df.index[5:10], 'b'] = np.nan\n |      >>> df.cov(min_periods=12)\n |                a         b         c\n |      a  0.316741       NaN -0.150812\n |      b       NaN  1.248003  0.191417\n |      c -0.150812  0.191417  0.895202\n |  \n |  cummax(self, axis=None, skipna=True, *args, **kwargs)\n |      Return cumulative maximum over a DataFrame or Series axis.\n |      \n |      Returns a DataFrame or Series of the same size containing the cumulative\n |      maximum.\n |      \n |      Parameters\n |      ----------\n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          The index or the name of the axis. 0 is equivalent to None or 'index'.\n |      skipna : boolean, default True\n |          Exclude NA/null values. If an entire row/column is NA, the result\n |          will be NA.\n |      *args, **kwargs :\n |          Additional keywords have no effect but might be accepted for\n |          compatibility with NumPy.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame\n |      \n |      See Also\n |      --------\n |      core.window.Expanding.max : Similar functionality\n |          but ignores ``NaN`` values.\n |      DataFrame.max : Return the maximum over\n |          DataFrame axis.\n |      DataFrame.cummax : Return cumulative maximum over DataFrame axis.\n |      DataFrame.cummin : Return cumulative minimum over DataFrame axis.\n |      DataFrame.cumsum : Return cumulative sum over DataFrame axis.\n |      DataFrame.cumprod : Return cumulative product over DataFrame axis.\n |      \n |      Examples\n |      --------\n |      **Series**\n |      \n |      >>> s = pd.Series([2, np.nan, 5, -1, 0])\n |      >>> s\n |      0    2.0\n |      1    NaN\n |      2    5.0\n |      3   -1.0\n |      4    0.0\n |      dtype: float64\n |      \n |      By default, NA values are ignored.\n |      \n |      >>> s.cummax()\n |      0    2.0\n |      1    NaN\n |      2    5.0\n |      3    5.0\n |      4    5.0\n |      dtype: float64\n |      \n |      To include NA values in the operation, use ``skipna=False``\n |      \n |      >>> s.cummax(skipna=False)\n |      0    2.0\n |      1    NaN\n |      2    NaN\n |      3    NaN\n |      4    NaN\n |      dtype: float64\n |      \n |      **DataFrame**\n |      \n |      >>> df = pd.DataFrame([[2.0, 1.0],\n |      ...                    [3.0, np.nan],\n |      ...                    [1.0, 0.0]],\n |      ...                    columns=list('AB'))\n |      >>> df\n |           A    B\n |      0  2.0  1.0\n |      1  3.0  NaN\n |      2  1.0  0.0\n |      \n |      By default, iterates over rows and finds the maximum\n |      in each column. This is equivalent to ``axis=None`` or ``axis='index'``.\n |      \n |      >>> df.cummax()\n |           A    B\n |      0  2.0  1.0\n |      1  3.0  NaN\n |      2  3.0  1.0\n |      \n |      To iterate over columns and find the maximum in each row,\n |      use ``axis=1``\n |      \n |      >>> df.cummax(axis=1)\n |           A    B\n |      0  2.0  2.0\n |      1  3.0  NaN\n |      2  1.0  1.0\n |  \n |  cummin(self, axis=None, skipna=True, *args, **kwargs)\n |      Return cumulative minimum over a DataFrame or Series axis.\n |      \n |      Returns a DataFrame or Series of the same size containing the cumulative\n |      minimum.\n |      \n |      Parameters\n |      ----------\n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          The index or the name of the axis. 0 is equivalent to None or 'index'.\n |      skipna : boolean, default True\n |          Exclude NA/null values. If an entire row/column is NA, the result\n |          will be NA.\n |      *args, **kwargs :\n |          Additional keywords have no effect but might be accepted for\n |          compatibility with NumPy.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame\n |      \n |      See Also\n |      --------\n |      core.window.Expanding.min : Similar functionality\n |          but ignores ``NaN`` values.\n |      DataFrame.min : Return the minimum over\n |          DataFrame axis.\n |      DataFrame.cummax : Return cumulative maximum over DataFrame axis.\n |      DataFrame.cummin : Return cumulative minimum over DataFrame axis.\n |      DataFrame.cumsum : Return cumulative sum over DataFrame axis.\n |      DataFrame.cumprod : Return cumulative product over DataFrame axis.\n |      \n |      Examples\n |      --------\n |      **Series**\n |      \n |      >>> s = pd.Series([2, np.nan, 5, -1, 0])\n |      >>> s\n |      0    2.0\n |      1    NaN\n |      2    5.0\n |      3   -1.0\n |      4    0.0\n |      dtype: float64\n |      \n |      By default, NA values are ignored.\n |      \n |      >>> s.cummin()\n |      0    2.0\n |      1    NaN\n |      2    2.0\n |      3   -1.0\n |      4   -1.0\n |      dtype: float64\n |      \n |      To include NA values in the operation, use ``skipna=False``\n |      \n |      >>> s.cummin(skipna=False)\n |      0    2.0\n |      1    NaN\n |      2    NaN\n |      3    NaN\n |      4    NaN\n |      dtype: float64\n |      \n |      **DataFrame**\n |      \n |      >>> df = pd.DataFrame([[2.0, 1.0],\n |      ...                    [3.0, np.nan],\n |      ...                    [1.0, 0.0]],\n |      ...                    columns=list('AB'))\n |      >>> df\n |           A    B\n |      0  2.0  1.0\n |      1  3.0  NaN\n |      2  1.0  0.0\n |      \n |      By default, iterates over rows and finds the minimum\n |      in each column. This is equivalent to ``axis=None`` or ``axis='index'``.\n |      \n |      >>> df.cummin()\n |           A    B\n |      0  2.0  1.0\n |      1  2.0  NaN\n |      2  1.0  0.0\n |      \n |      To iterate over columns and find the minimum in each row,\n |      use ``axis=1``\n |      \n |      >>> df.cummin(axis=1)\n |           A    B\n |      0  2.0  1.0\n |      1  3.0  NaN\n |      2  1.0  0.0\n |  \n |  cumprod(self, axis=None, skipna=True, *args, **kwargs)\n |      Return cumulative product over a DataFrame or Series axis.\n |      \n |      Returns a DataFrame or Series of the same size containing the cumulative\n |      product.\n |      \n |      Parameters\n |      ----------\n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          The index or the name of the axis. 0 is equivalent to None or 'index'.\n |      skipna : boolean, default True\n |          Exclude NA/null values. If an entire row/column is NA, the result\n |          will be NA.\n |      *args, **kwargs :\n |          Additional keywords have no effect but might be accepted for\n |          compatibility with NumPy.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame\n |      \n |      See Also\n |      --------\n |      core.window.Expanding.prod : Similar functionality\n |          but ignores ``NaN`` values.\n |      DataFrame.prod : Return the product over\n |          DataFrame axis.\n |      DataFrame.cummax : Return cumulative maximum over DataFrame axis.\n |      DataFrame.cummin : Return cumulative minimum over DataFrame axis.\n |      DataFrame.cumsum : Return cumulative sum over DataFrame axis.\n |      DataFrame.cumprod : Return cumulative product over DataFrame axis.\n |      \n |      Examples\n |      --------\n |      **Series**\n |      \n |      >>> s = pd.Series([2, np.nan, 5, -1, 0])\n |      >>> s\n |      0    2.0\n |      1    NaN\n |      2    5.0\n |      3   -1.0\n |      4    0.0\n |      dtype: float64\n |      \n |      By default, NA values are ignored.\n |      \n |      >>> s.cumprod()\n |      0     2.0\n |      1     NaN\n |      2    10.0\n |      3   -10.0\n |      4    -0.0\n |      dtype: float64\n |      \n |      To include NA values in the operation, use ``skipna=False``\n |      \n |      >>> s.cumprod(skipna=False)\n |      0    2.0\n |      1    NaN\n |      2    NaN\n |      3    NaN\n |      4    NaN\n |      dtype: float64\n |      \n |      **DataFrame**\n |      \n |      >>> df = pd.DataFrame([[2.0, 1.0],\n |      ...                    [3.0, np.nan],\n |      ...                    [1.0, 0.0]],\n |      ...                    columns=list('AB'))\n |      >>> df\n |           A    B\n |      0  2.0  1.0\n |      1  3.0  NaN\n |      2  1.0  0.0\n |      \n |      By default, iterates over rows and finds the product\n |      in each column. This is equivalent to ``axis=None`` or ``axis='index'``.\n |      \n |      >>> df.cumprod()\n |           A    B\n |      0  2.0  1.0\n |      1  6.0  NaN\n |      2  6.0  0.0\n |      \n |      To iterate over columns and find the product in each row,\n |      use ``axis=1``\n |      \n |      >>> df.cumprod(axis=1)\n |           A    B\n |      0  2.0  2.0\n |      1  3.0  NaN\n |      2  1.0  0.0\n |  \n |  cumsum(self, axis=None, skipna=True, *args, **kwargs)\n |      Return cumulative sum over a DataFrame or Series axis.\n |      \n |      Returns a DataFrame or Series of the same size containing the cumulative\n |      sum.\n |      \n |      Parameters\n |      ----------\n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          The index or the name of the axis. 0 is equivalent to None or 'index'.\n |      skipna : boolean, default True\n |          Exclude NA/null values. If an entire row/column is NA, the result\n |          will be NA.\n |      *args, **kwargs :\n |          Additional keywords have no effect but might be accepted for\n |          compatibility with NumPy.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame\n |      \n |      See Also\n |      --------\n |      core.window.Expanding.sum : Similar functionality\n |          but ignores ``NaN`` values.\n |      DataFrame.sum : Return the sum over\n |          DataFrame axis.\n |      DataFrame.cummax : Return cumulative maximum over DataFrame axis.\n |      DataFrame.cummin : Return cumulative minimum over DataFrame axis.\n |      DataFrame.cumsum : Return cumulative sum over DataFrame axis.\n |      DataFrame.cumprod : Return cumulative product over DataFrame axis.\n |      \n |      Examples\n |      --------\n |      **Series**\n |      \n |      >>> s = pd.Series([2, np.nan, 5, -1, 0])\n |      >>> s\n |      0    2.0\n |      1    NaN\n |      2    5.0\n |      3   -1.0\n |      4    0.0\n |      dtype: float64\n |      \n |      By default, NA values are ignored.\n |      \n |      >>> s.cumsum()\n |      0    2.0\n |      1    NaN\n |      2    7.0\n |      3    6.0\n |      4    6.0\n |      dtype: float64\n |      \n |      To include NA values in the operation, use ``skipna=False``\n |      \n |      >>> s.cumsum(skipna=False)\n |      0    2.0\n |      1    NaN\n |      2    NaN\n |      3    NaN\n |      4    NaN\n |      dtype: float64\n |      \n |      **DataFrame**\n |      \n |      >>> df = pd.DataFrame([[2.0, 1.0],\n |      ...                    [3.0, np.nan],\n |      ...                    [1.0, 0.0]],\n |      ...                    columns=list('AB'))\n |      >>> df\n |           A    B\n |      0  2.0  1.0\n |      1  3.0  NaN\n |      2  1.0  0.0\n |      \n |      By default, iterates over rows and finds the sum\n |      in each column. This is equivalent to ``axis=None`` or ``axis='index'``.\n |      \n |      >>> df.cumsum()\n |           A    B\n |      0  2.0  1.0\n |      1  5.0  NaN\n |      2  6.0  1.0\n |      \n |      To iterate over columns and find the sum in each row,\n |      use ``axis=1``\n |      \n |      >>> df.cumsum(axis=1)\n |           A    B\n |      0  2.0  3.0\n |      1  3.0  NaN\n |      2  1.0  1.0\n |  \n |  diff(self, periods=1, axis=0)\n |      First discrete difference of element.\n |      \n |      Calculates the difference of a DataFrame element compared with another\n |      element in the DataFrame (default is the element in the same column\n |      of the previous row).\n |      \n |      Parameters\n |      ----------\n |      periods : int, default 1\n |          Periods to shift for calculating difference, accepts negative\n |          values.\n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          Take difference over rows (0) or columns (1).\n |      \n |          .. versionadded:: 0.16.1.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |      \n |      See Also\n |      --------\n |      Series.diff: First discrete difference for a Series.\n |      DataFrame.pct_change: Percent change over given number of periods.\n |      DataFrame.shift: Shift index by desired number of periods with an\n |          optional time freq.\n |      \n |      Examples\n |      --------\n |      Difference with previous row\n |      \n |      >>> df = pd.DataFrame({'a': [1, 2, 3, 4, 5, 6],\n |      ...                    'b': [1, 1, 2, 3, 5, 8],\n |      ...                    'c': [1, 4, 9, 16, 25, 36]})\n |      >>> df\n |         a  b   c\n |      0  1  1   1\n |      1  2  1   4\n |      2  3  2   9\n |      3  4  3  16\n |      4  5  5  25\n |      5  6  8  36\n |      \n |      >>> df.diff()\n |           a    b     c\n |      0  NaN  NaN   NaN\n |      1  1.0  0.0   3.0\n |      2  1.0  1.0   5.0\n |      3  1.0  1.0   7.0\n |      4  1.0  2.0   9.0\n |      5  1.0  3.0  11.0\n |      \n |      Difference with previous column\n |      \n |      >>> df.diff(axis=1)\n |          a    b     c\n |      0 NaN  0.0   0.0\n |      1 NaN -1.0   3.0\n |      2 NaN -1.0   7.0\n |      3 NaN -1.0  13.0\n |      4 NaN  0.0  20.0\n |      5 NaN  2.0  28.0\n |      \n |      Difference with 3rd previous row\n |      \n |      >>> df.diff(periods=3)\n |           a    b     c\n |      0  NaN  NaN   NaN\n |      1  NaN  NaN   NaN\n |      2  NaN  NaN   NaN\n |      3  3.0  2.0  15.0\n |      4  3.0  4.0  21.0\n |      5  3.0  6.0  27.0\n |      \n |      Difference with following row\n |      \n |      >>> df.diff(periods=-1)\n |           a    b     c\n |      0 -1.0  0.0  -3.0\n |      1 -1.0 -1.0  -5.0\n |      2 -1.0 -1.0  -7.0\n |      3 -1.0 -2.0  -9.0\n |      4 -1.0 -3.0 -11.0\n |      5  NaN  NaN   NaN\n |  \n |  div = truediv(self, other, axis='columns', level=None, fill_value=None)\n |  \n |  divide = truediv(self, other, axis='columns', level=None, fill_value=None)\n |  \n |  dot(self, other)\n |      Compute the matrix multiplication between the DataFrame and other.\n |      \n |      This method computes the matrix product between the DataFrame and the\n |      values of an other Series, DataFrame or a numpy array.\n |      \n |      It can also be called using ``self @ other`` in Python >= 3.5.\n |      \n |      Parameters\n |      ----------\n |      other : Series, DataFrame or array-like\n |          The other object to compute the matrix product with.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame\n |          If other is a Series, return the matrix product between self and\n |          other as a Serie. If other is a DataFrame or a numpy.array, return\n |          the matrix product of self and other in a DataFrame of a np.array.\n |      \n |      See Also\n |      --------\n |      Series.dot: Similar method for Series.\n |      \n |      Notes\n |      -----\n |      The dimensions of DataFrame and other must be compatible in order to\n |      compute the matrix multiplication. In addition, the column names of\n |      DataFrame and the index of other must contain the same values, as they\n |      will be aligned prior to the multiplication.\n |      \n |      The dot method for Series computes the inner product, instead of the\n |      matrix product here.\n |      \n |      Examples\n |      --------\n |      Here we multiply a DataFrame with a Series.\n |      \n |      >>> df = pd.DataFrame([[0, 1, -2, -1], [1, 1, 1, 1]])\n |      >>> s = pd.Series([1, 1, 2, 1])\n |      >>> df.dot(s)\n |      0    -4\n |      1     5\n |      dtype: int64\n |      \n |      Here we multiply a DataFrame with another DataFrame.\n |      \n |      >>> other = pd.DataFrame([[0, 1], [1, 2], [-1, -1], [2, 0]])\n |      >>> df.dot(other)\n |          0   1\n |      0   1   4\n |      1   2   2\n |      \n |      Note that the dot method give the same result as @\n |      \n |      >>> df @ other\n |          0   1\n |      0   1   4\n |      1   2   2\n |      \n |      The dot method works also if other is an np.array.\n |      \n |      >>> arr = np.array([[0, 1], [1, 2], [-1, -1], [2, 0]])\n |      >>> df.dot(arr)\n |          0   1\n |      0   1   4\n |      1   2   2\n |      \n |      Note how shuffling of the objects does not change the result.\n |      \n |      >>> s2 = s.reindex([1, 0, 2, 3])\n |      >>> df.dot(s2)\n |      0    -4\n |      1     5\n |      dtype: int64\n |  \n |  drop(self, labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise')\n |      Drop specified labels from rows or columns.\n |      \n |      Remove rows or columns by specifying label names and corresponding\n |      axis, or by specifying directly index or column names. When using a\n |      multi-index, labels on different levels can be removed by specifying\n |      the level.\n |      \n |      Parameters\n |      ----------\n |      labels : single label or list-like\n |          Index or column labels to drop.\n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          Whether to drop labels from the index (0 or 'index') or\n |          columns (1 or 'columns').\n |      index : single label or list-like\n |          Alternative to specifying axis (``labels, axis=0``\n |          is equivalent to ``index=labels``).\n |      \n |          .. versionadded:: 0.21.0\n |      columns : single label or list-like\n |          Alternative to specifying axis (``labels, axis=1``\n |          is equivalent to ``columns=labels``).\n |      \n |          .. versionadded:: 0.21.0\n |      level : int or level name, optional\n |          For MultiIndex, level from which the labels will be removed.\n |      inplace : bool, default False\n |          If True, do operation inplace and return None.\n |      errors : {'ignore', 'raise'}, default 'raise'\n |          If 'ignore', suppress error and only existing labels are\n |          dropped.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          DataFrame without the removed index or column labels.\n |      \n |      Raises\n |      ------\n |      KeyError\n |          If any of the labels is not found in the selected axis.\n |      \n |      See Also\n |      --------\n |      DataFrame.loc : Label-location based indexer for selection by label.\n |      DataFrame.dropna : Return DataFrame with labels on given axis omitted\n |          where (all or any) data are missing.\n |      DataFrame.drop_duplicates : Return DataFrame with duplicate rows\n |          removed, optionally only considering certain columns.\n |      Series.drop : Return Series with specified index labels removed.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame(np.arange(12).reshape(3, 4),\n |      ...                   columns=['A', 'B', 'C', 'D'])\n |      >>> df\n |         A  B   C   D\n |      0  0  1   2   3\n |      1  4  5   6   7\n |      2  8  9  10  11\n |      \n |      Drop columns\n |      \n |      >>> df.drop(['B', 'C'], axis=1)\n |         A   D\n |      0  0   3\n |      1  4   7\n |      2  8  11\n |      \n |      >>> df.drop(columns=['B', 'C'])\n |         A   D\n |      0  0   3\n |      1  4   7\n |      2  8  11\n |      \n |      Drop a row by index\n |      \n |      >>> df.drop([0, 1])\n |         A  B   C   D\n |      2  8  9  10  11\n |      \n |      Drop columns and/or rows of MultiIndex DataFrame\n |      \n |      >>> midx = pd.MultiIndex(levels=[['lama', 'cow', 'falcon'],\n |      ...                              ['speed', 'weight', 'length']],\n |      ...                      codes=[[0, 0, 0, 1, 1, 1, 2, 2, 2],\n |      ...                             [0, 1, 2, 0, 1, 2, 0, 1, 2]])\n |      >>> df = pd.DataFrame(index=midx, columns=['big', 'small'],\n |      ...                   data=[[45, 30], [200, 100], [1.5, 1], [30, 20],\n |      ...                         [250, 150], [1.5, 0.8], [320, 250],\n |      ...                         [1, 0.8], [0.3, 0.2]])\n |      >>> df\n |                      big     small\n |      lama    speed   45.0    30.0\n |              weight  200.0   100.0\n |              length  1.5     1.0\n |      cow     speed   30.0    20.0\n |              weight  250.0   150.0\n |              length  1.5     0.8\n |      falcon  speed   320.0   250.0\n |              weight  1.0     0.8\n |              length  0.3     0.2\n |      \n |      >>> df.drop(index='cow', columns='small')\n |                      big\n |      lama    speed   45.0\n |              weight  200.0\n |              length  1.5\n |      falcon  speed   320.0\n |              weight  1.0\n |              length  0.3\n |      \n |      >>> df.drop(index='length', level=1)\n |                      big     small\n |      lama    speed   45.0    30.0\n |              weight  200.0   100.0\n |      cow     speed   30.0    20.0\n |              weight  250.0   150.0\n |      falcon  speed   320.0   250.0\n |              weight  1.0     0.8\n |  \n |  drop_duplicates(self, subset=None, keep='first', inplace=False)\n |      Return DataFrame with duplicate rows removed, optionally only\n |      considering certain columns. Indexes, including time indexes\n |      are ignored.\n |      \n |      Parameters\n |      ----------\n |      subset : column label or sequence of labels, optional\n |          Only consider certain columns for identifying duplicates, by\n |          default use all of the columns\n |      keep : {'first', 'last', False}, default 'first'\n |          - ``first`` : Drop duplicates except for the first occurrence.\n |          - ``last`` : Drop duplicates except for the last occurrence.\n |          - False : Drop all duplicates.\n |      inplace : boolean, default False\n |          Whether to drop duplicates in place or to return a copy\n |      \n |      Returns\n |      -------\n |      DataFrame\n |  \n |  dropna(self, axis=0, how='any', thresh=None, subset=None, inplace=False)\n |      Remove missing values.\n |      \n |      See the :ref:`User Guide <missing_data>` for more on which values are\n |      considered missing, and how to work with missing data.\n |      \n |      Parameters\n |      ----------\n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          Determine if rows or columns which contain missing values are\n |          removed.\n |      \n |          * 0, or 'index' : Drop rows which contain missing values.\n |          * 1, or 'columns' : Drop columns which contain missing value.\n |      \n |          .. deprecated:: 0.23.0\n |      \n |             Pass tuple or list to drop on multiple axes.\n |             Only a single axis is allowed.\n |      \n |      how : {'any', 'all'}, default 'any'\n |          Determine if row or column is removed from DataFrame, when we have\n |          at least one NA or all NA.\n |      \n |          * 'any' : If any NA values are present, drop that row or column.\n |          * 'all' : If all values are NA, drop that row or column.\n |      \n |      thresh : int, optional\n |          Require that many non-NA values.\n |      subset : array-like, optional\n |          Labels along other axis to consider, e.g. if you are dropping rows\n |          these would be a list of columns to include.\n |      inplace : bool, default False\n |          If True, do operation inplace and return None.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          DataFrame with NA entries dropped from it.\n |      \n |      See Also\n |      --------\n |      DataFrame.isna: Indicate missing values.\n |      DataFrame.notna : Indicate existing (non-missing) values.\n |      DataFrame.fillna : Replace missing values.\n |      Series.dropna : Drop missing values.\n |      Index.dropna : Drop missing indices.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({\"name\": ['Alfred', 'Batman', 'Catwoman'],\n |      ...                    \"toy\": [np.nan, 'Batmobile', 'Bullwhip'],\n |      ...                    \"born\": [pd.NaT, pd.Timestamp(\"1940-04-25\"),\n |      ...                             pd.NaT]})\n |      >>> df\n |             name        toy       born\n |      0    Alfred        NaN        NaT\n |      1    Batman  Batmobile 1940-04-25\n |      2  Catwoman   Bullwhip        NaT\n |      \n |      Drop the rows where at least one element is missing.\n |      \n |      >>> df.dropna()\n |           name        toy       born\n |      1  Batman  Batmobile 1940-04-25\n |      \n |      Drop the columns where at least one element is missing.\n |      \n |      >>> df.dropna(axis='columns')\n |             name\n |      0    Alfred\n |      1    Batman\n |      2  Catwoman\n |      \n |      Drop the rows where all elements are missing.\n |      \n |      >>> df.dropna(how='all')\n |             name        toy       born\n |      0    Alfred        NaN        NaT\n |      1    Batman  Batmobile 1940-04-25\n |      2  Catwoman   Bullwhip        NaT\n |      \n |      Keep only the rows with at least 2 non-NA values.\n |      \n |      >>> df.dropna(thresh=2)\n |             name        toy       born\n |      1    Batman  Batmobile 1940-04-25\n |      2  Catwoman   Bullwhip        NaT\n |      \n |      Define in which columns to look for missing values.\n |      \n |      >>> df.dropna(subset=['name', 'born'])\n |             name        toy       born\n |      1    Batman  Batmobile 1940-04-25\n |      \n |      Keep the DataFrame with valid entries in the same variable.\n |      \n |      >>> df.dropna(inplace=True)\n |      >>> df\n |           name        toy       born\n |      1  Batman  Batmobile 1940-04-25\n |  \n |  duplicated(self, subset=None, keep='first')\n |      Return boolean Series denoting duplicate rows, optionally only\n |      considering certain columns.\n |      \n |      Parameters\n |      ----------\n |      subset : column label or sequence of labels, optional\n |          Only consider certain columns for identifying duplicates, by\n |          default use all of the columns\n |      keep : {'first', 'last', False}, default 'first'\n |          - ``first`` : Mark duplicates as ``True`` except for the\n |            first occurrence.\n |          - ``last`` : Mark duplicates as ``True`` except for the\n |            last occurrence.\n |          - False : Mark all duplicates as ``True``.\n |      \n |      Returns\n |      -------\n |      Series\n |  \n |  eq(self, other, axis='columns', level=None)\n |      Get Equal to of dataframe and other, element-wise (binary operator `eq`).\n |      \n |      Among flexible wrappers (`eq`, `ne`, `le`, `lt`, `ge`, `gt`) to comparison\n |      operators.\n |      \n |      Equivalent to `==`, `=!`, `<=`, `<`, `>=`, `>` with support to choose axis\n |      (rows or columns) and level for comparison.\n |      \n |      Parameters\n |      ----------\n |      other : scalar, sequence, Series, or DataFrame\n |          Any single or multiple element data structure, or list-like object.\n |      axis :  {0 or 'index', 1 or 'columns'}, default 'columns'\n |          Whether to compare by the index (0 or 'index') or columns\n |          (1 or 'columns').\n |      level : int or label\n |          Broadcast across a level, matching Index values on the passed\n |          MultiIndex level.\n |      \n |      Returns\n |      -------\n |      DataFrame of bool\n |          Result of the comparison.\n |      \n |      See Also\n |      --------\n |      DataFrame.eq : Compare DataFrames for equality elementwise.\n |      DataFrame.ne : Compare DataFrames for inequality elementwise.\n |      DataFrame.le : Compare DataFrames for less than inequality\n |          or equality elementwise.\n |      DataFrame.lt : Compare DataFrames for strictly less than\n |          inequality elementwise.\n |      DataFrame.ge : Compare DataFrames for greater than inequality\n |          or equality elementwise.\n |      DataFrame.gt : Compare DataFrames for strictly greater than\n |          inequality elementwise.\n |      \n |      Notes\n |      -----\n |      Mismatched indices will be unioned together.\n |      `NaN` values are considered different (i.e. `NaN` != `NaN`).\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'cost': [250, 150, 100],\n |      ...                    'revenue': [100, 250, 300]},\n |      ...                   index=['A', 'B', 'C'])\n |      >>> df\n |         cost  revenue\n |      A   250      100\n |      B   150      250\n |      C   100      300\n |      \n |      Comparison with a scalar, using either the operator or method:\n |      \n |      >>> df == 100\n |          cost  revenue\n |      A  False     True\n |      B  False    False\n |      C   True    False\n |      \n |      >>> df.eq(100)\n |          cost  revenue\n |      A  False     True\n |      B  False    False\n |      C   True    False\n |      \n |      When `other` is a :class:`Series`, the columns of a DataFrame are aligned\n |      with the index of `other` and broadcast:\n |      \n |      >>> df != pd.Series([100, 250], index=[\"cost\", \"revenue\"])\n |          cost  revenue\n |      A   True     True\n |      B   True    False\n |      C  False     True\n |      \n |      Use the method to control the broadcast axis:\n |      \n |      >>> df.ne(pd.Series([100, 300], index=[\"A\", \"D\"]), axis='index')\n |         cost  revenue\n |      A  True    False\n |      B  True     True\n |      C  True     True\n |      D  True     True\n |      \n |      When comparing to an arbitrary sequence, the number of columns must\n |      match the number elements in `other`:\n |      \n |      >>> df == [250, 100]\n |          cost  revenue\n |      A   True     True\n |      B  False    False\n |      C  False    False\n |      \n |      Use the method to control the axis:\n |      \n |      >>> df.eq([250, 250, 100], axis='index')\n |          cost  revenue\n |      A   True    False\n |      B  False     True\n |      C   True    False\n |      \n |      Compare to a DataFrame of different shape.\n |      \n |      >>> other = pd.DataFrame({'revenue': [300, 250, 100, 150]},\n |      ...                      index=['A', 'B', 'C', 'D'])\n |      >>> other\n |         revenue\n |      A      300\n |      B      250\n |      C      100\n |      D      150\n |      \n |      >>> df.gt(other)\n |          cost  revenue\n |      A  False    False\n |      B  False    False\n |      C  False     True\n |      D  False    False\n |      \n |      Compare to a MultiIndex by level.\n |      \n |      >>> df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220],\n |      ...                              'revenue': [100, 250, 300, 200, 175, 225]},\n |      ...                             index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'],\n |      ...                                    ['A', 'B', 'C', 'A', 'B', 'C']])\n |      >>> df_multindex\n |            cost  revenue\n |      Q1 A   250      100\n |         B   150      250\n |         C   100      300\n |      Q2 A   150      200\n |         B   300      175\n |         C   220      225\n |      \n |      >>> df.le(df_multindex, level=1)\n |             cost  revenue\n |      Q1 A   True     True\n |         B   True     True\n |         C   True     True\n |      Q2 A  False     True\n |         B   True    False\n |         C   True    False\n |  \n |  eval(self, expr, inplace=False, **kwargs)\n |      Evaluate a string describing operations on DataFrame columns.\n |      \n |      Operates on columns only, not specific rows or elements.  This allows\n |      `eval` to run arbitrary code, which can make you vulnerable to code\n |      injection if you pass user input to this function.\n |      \n |      Parameters\n |      ----------\n |      expr : str\n |          The expression string to evaluate.\n |      inplace : bool, default False\n |          If the expression contains an assignment, whether to perform the\n |          operation inplace and mutate the existing DataFrame. Otherwise,\n |          a new DataFrame is returned.\n |      \n |          .. versionadded:: 0.18.0.\n |      kwargs : dict\n |          See the documentation for :func:`eval` for complete details\n |          on the keyword arguments accepted by\n |          :meth:`~pandas.DataFrame.query`.\n |      \n |      Returns\n |      -------\n |      ndarray, scalar, or pandas object\n |          The result of the evaluation.\n |      \n |      See Also\n |      --------\n |      DataFrame.query : Evaluates a boolean expression to query the columns\n |          of a frame.\n |      DataFrame.assign : Can evaluate an expression or function to create new\n |          values for a column.\n |      eval : Evaluate a Python expression as a string using various\n |          backends.\n |      \n |      Notes\n |      -----\n |      For more details see the API documentation for :func:`~eval`.\n |      For detailed examples see :ref:`enhancing performance with eval\n |      <enhancingperf.eval>`.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'A': range(1, 6), 'B': range(10, 0, -2)})\n |      >>> df\n |         A   B\n |      0  1  10\n |      1  2   8\n |      2  3   6\n |      3  4   4\n |      4  5   2\n |      >>> df.eval('A + B')\n |      0    11\n |      1    10\n |      2     9\n |      3     8\n |      4     7\n |      dtype: int64\n |      \n |      Assignment is allowed though by default the original DataFrame is not\n |      modified.\n |      \n |      >>> df.eval('C = A + B')\n |         A   B   C\n |      0  1  10  11\n |      1  2   8  10\n |      2  3   6   9\n |      3  4   4   8\n |      4  5   2   7\n |      >>> df\n |         A   B\n |      0  1  10\n |      1  2   8\n |      2  3   6\n |      3  4   4\n |      4  5   2\n |      \n |      Use ``inplace=True`` to modify the original DataFrame.\n |      \n |      >>> df.eval('C = A + B', inplace=True)\n |      >>> df\n |         A   B   C\n |      0  1  10  11\n |      1  2   8  10\n |      2  3   6   9\n |      3  4   4   8\n |      4  5   2   7\n |  \n |  ewm(self, com=None, span=None, halflife=None, alpha=None, min_periods=0, adjust=True, ignore_na=False, axis=0)\n |      Provide exponential weighted functions.\n |      \n |      .. versionadded:: 0.18.0\n |      \n |      Parameters\n |      ----------\n |      com : float, optional\n |          Specify decay in terms of center of mass,\n |          :math:`\\alpha = 1 / (1 + com),\\text{ for } com \\geq 0`.\n |      span : float, optional\n |          Specify decay in terms of span,\n |          :math:`\\alpha = 2 / (span + 1),\\text{ for } span \\geq 1`.\n |      halflife : float, optional\n |          Specify decay in terms of half-life,\n |          :math:`\\alpha = 1 - exp(log(0.5) / halflife),\\text{for} halflife > 0`.\n |      alpha : float, optional\n |          Specify smoothing factor :math:`\\alpha` directly,\n |          :math:`0 < \\alpha \\leq 1`.\n |      \n |          .. versionadded:: 0.18.0\n |      \n |      min_periods : int, default 0\n |          Minimum number of observations in window required to have a value\n |          (otherwise result is NA).\n |      adjust : bool, default True\n |          Divide by decaying adjustment factor in beginning periods to account\n |          for imbalance in relative weightings\n |          (viewing EWMA as a moving average).\n |      ignore_na : bool, default False\n |          Ignore missing values when calculating weights;\n |          specify True to reproduce pre-0.15.0 behavior.\n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          The axis to use. The value 0 identifies the rows, and 1\n |          identifies the columns.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          A Window sub-classed for the particular operation.\n |      \n |      See Also\n |      --------\n |      rolling : Provides rolling window calculations.\n |      expanding : Provides expanding transformations.\n |      \n |      Notes\n |      -----\n |      Exactly one of center of mass, span, half-life, and alpha must be provided.\n |      Allowed values and relationship between the parameters are specified in the\n |      parameter descriptions above; see the link at the end of this section for\n |      a detailed explanation.\n |      \n |      When adjust is True (default), weighted averages are calculated using\n |      weights (1-alpha)**(n-1), (1-alpha)**(n-2), ..., 1-alpha, 1.\n |      \n |      When adjust is False, weighted averages are calculated recursively as:\n |         weighted_average[0] = arg[0];\n |         weighted_average[i] = (1-alpha)*weighted_average[i-1] + alpha*arg[i].\n |      \n |      When ignore_na is False (default), weights are based on absolute positions.\n |      For example, the weights of x and y used in calculating the final weighted\n |      average of [x, None, y] are (1-alpha)**2 and 1 (if adjust is True), and\n |      (1-alpha)**2 and alpha (if adjust is False).\n |      \n |      When ignore_na is True (reproducing pre-0.15.0 behavior), weights are based\n |      on relative positions. For example, the weights of x and y used in\n |      calculating the final weighted average of [x, None, y] are 1-alpha and 1\n |      (if adjust is True), and 1-alpha and alpha (if adjust is False).\n |      \n |      More details can be found at\n |      http://pandas.pydata.org/pandas-docs/stable/user_guide/computation.html#exponentially-weighted-windows\n |      \n |      Examples\n |      --------\n |      \n |      >>> df = pd.DataFrame({'B': [0, 1, 2, np.nan, 4]})\n |      >>> df\n |           B\n |      0  0.0\n |      1  1.0\n |      2  2.0\n |      3  NaN\n |      4  4.0\n |      \n |      >>> df.ewm(com=0.5).mean()\n |                B\n |      0  0.000000\n |      1  0.750000\n |      2  1.615385\n |      3  1.615385\n |      4  3.670213\n |  \n |  expanding(self, min_periods=1, center=False, axis=0)\n |      Provide expanding transformations.\n |      \n |      .. versionadded:: 0.18.0\n |      \n |      Parameters\n |      ----------\n |      min_periods : int, default 1\n |          Minimum number of observations in window required to have a value\n |          (otherwise result is NA).\n |      center : bool, default False\n |          Set the labels at the center of the window.\n |      axis : int or str, default 0\n |      \n |      Returns\n |      -------\n |      a Window sub-classed for the particular operation\n |      \n |      See Also\n |      --------\n |      rolling : Provides rolling window calculations.\n |      ewm : Provides exponential weighted functions.\n |      \n |      Notes\n |      -----\n |      By default, the result is set to the right edge of the window. This can be\n |      changed to the center of the window by setting ``center=True``.\n |      \n |      Examples\n |      --------\n |      \n |      >>> df = pd.DataFrame({'B': [0, 1, 2, np.nan, 4]})\n |           B\n |      0  0.0\n |      1  1.0\n |      2  2.0\n |      3  NaN\n |      4  4.0\n |      \n |      >>> df.expanding(2).sum()\n |           B\n |      0  NaN\n |      1  1.0\n |      2  3.0\n |      3  3.0\n |      4  7.0\n |  \n |  explode(self, column:Union[str, Tuple]) -> 'DataFrame'\n |      Transform each element of a list-like to a row, replicating the\n |      index values.\n |      \n |      .. versionadded:: 0.25.0\n |      \n |      Parameters\n |      ----------\n |      column : str or tuple\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          Exploded lists to rows of the subset columns;\n |          index will be duplicated for these rows.\n |      \n |      Raises\n |      ------\n |      ValueError :\n |          if columns of the frame are not unique.\n |      \n |      See Also\n |      --------\n |      DataFrame.unstack : Pivot a level of the (necessarily hierarchical)\n |          index labels\n |      DataFrame.melt : Unpivot a DataFrame from wide format to long format\n |      Series.explode : Explode a DataFrame from list-like columns to long format.\n |      \n |      Notes\n |      -----\n |      This routine will explode list-likes including lists, tuples,\n |      Series, and np.ndarray. The result dtype of the subset rows will\n |      be object. Scalars will be returned unchanged. Empty list-likes will\n |      result in a np.nan for that row.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'A': [[1, 2, 3], 'foo', [], [3, 4]], 'B': 1})\n |      >>> df\n |                 A  B\n |      0  [1, 2, 3]  1\n |      1        foo  1\n |      2         []  1\n |      3     [3, 4]  1\n |      \n |      >>> df.explode('A')\n |           A  B\n |      0    1  1\n |      0    2  1\n |      0    3  1\n |      1  foo  1\n |      2  NaN  1\n |      3    3  1\n |      3    4  1\n |  \n |  fillna(self, value=None, method=None, axis=None, inplace=False, limit=None, downcast=None, **kwargs)\n |      Fill NA/NaN values using the specified method.\n |      \n |      Parameters\n |      ----------\n |      value : scalar, dict, Series, or DataFrame\n |          Value to use to fill holes (e.g. 0), alternately a\n |          dict/Series/DataFrame of values specifying which value to use for\n |          each index (for a Series) or column (for a DataFrame).  Values not\n |          in the dict/Series/DataFrame will not be filled. This value cannot\n |          be a list.\n |      method : {'backfill', 'bfill', 'pad', 'ffill', None}, default None\n |          Method to use for filling holes in reindexed Series\n |          pad / ffill: propagate last valid observation forward to next valid\n |          backfill / bfill: use next valid observation to fill gap.\n |      axis : {0 or 'index', 1 or 'columns'}\n |          Axis along which to fill missing values.\n |      inplace : bool, default False\n |          If True, fill in-place. Note: this will modify any\n |          other views on this object (e.g., a no-copy slice for a column in a\n |          DataFrame).\n |      limit : int, default None\n |          If method is specified, this is the maximum number of consecutive\n |          NaN values to forward/backward fill. In other words, if there is\n |          a gap with more than this number of consecutive NaNs, it will only\n |          be partially filled. If method is not specified, this is the\n |          maximum number of entries along the entire axis where NaNs will be\n |          filled. Must be greater than 0 if not None.\n |      downcast : dict, default is None\n |          A dict of item->dtype of what to downcast if possible,\n |          or the string 'infer' which will try to downcast to an appropriate\n |          equal type (e.g. float64 to int64 if possible).\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          Object with missing values filled.\n |      \n |      See Also\n |      --------\n |      interpolate : Fill NaN values using interpolation.\n |      reindex : Conform object to new index.\n |      asfreq : Convert TimeSeries to specified frequency.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame([[np.nan, 2, np.nan, 0],\n |      ...                    [3, 4, np.nan, 1],\n |      ...                    [np.nan, np.nan, np.nan, 5],\n |      ...                    [np.nan, 3, np.nan, 4]],\n |      ...                   columns=list('ABCD'))\n |      >>> df\n |           A    B   C  D\n |      0  NaN  2.0 NaN  0\n |      1  3.0  4.0 NaN  1\n |      2  NaN  NaN NaN  5\n |      3  NaN  3.0 NaN  4\n |      \n |      Replace all NaN elements with 0s.\n |      \n |      >>> df.fillna(0)\n |          A   B   C   D\n |      0   0.0 2.0 0.0 0\n |      1   3.0 4.0 0.0 1\n |      2   0.0 0.0 0.0 5\n |      3   0.0 3.0 0.0 4\n |      \n |      We can also propagate non-null values forward or backward.\n |      \n |      >>> df.fillna(method='ffill')\n |          A   B   C   D\n |      0   NaN 2.0 NaN 0\n |      1   3.0 4.0 NaN 1\n |      2   3.0 4.0 NaN 5\n |      3   3.0 3.0 NaN 4\n |      \n |      Replace all NaN elements in column 'A', 'B', 'C', and 'D', with 0, 1,\n |      2, and 3 respectively.\n |      \n |      >>> values = {'A': 0, 'B': 1, 'C': 2, 'D': 3}\n |      >>> df.fillna(value=values)\n |          A   B   C   D\n |      0   0.0 2.0 2.0 0\n |      1   3.0 4.0 2.0 1\n |      2   0.0 1.0 2.0 5\n |      3   0.0 3.0 2.0 4\n |      \n |      Only replace the first NaN element.\n |      \n |      >>> df.fillna(value=values, limit=1)\n |          A   B   C   D\n |      0   0.0 2.0 2.0 0\n |      1   3.0 4.0 NaN 1\n |      2   NaN 1.0 NaN 5\n |      3   NaN 3.0 NaN 4\n |  \n |  floordiv(self, other, axis='columns', level=None, fill_value=None)\n |      Get Integer division of dataframe and other, element-wise (binary operator `floordiv`).\n |      \n |      Equivalent to ``dataframe // other``, but with support to substitute a fill_value\n |      for missing data in one of the inputs. With reverse version, `rfloordiv`.\n |      \n |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `mod`, `pow`) to\n |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n |      \n |      Parameters\n |      ----------\n |      other : scalar, sequence, Series, or DataFrame\n |          Any single or multiple element data structure, or list-like object.\n |      axis :  {0 or 'index', 1 or 'columns'}\n |          Whether to compare by the index (0 or 'index') or columns\n |          (1 or 'columns'). For Series input, axis to match Series index on.\n |      level : int or label\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level.\n |      fill_value : float or None, default None\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful DataFrame alignment, with this value before computation.\n |          If data in both corresponding DataFrame locations is missing\n |          the result will be missing.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          Result of the arithmetic operation.\n |      \n |      See Also\n |      --------\n |      DataFrame.add : Add DataFrames.\n |      DataFrame.sub : Subtract DataFrames.\n |      DataFrame.mul : Multiply DataFrames.\n |      DataFrame.div : Divide DataFrames (float division).\n |      DataFrame.truediv : Divide DataFrames (float division).\n |      DataFrame.floordiv : Divide DataFrames (integer division).\n |      DataFrame.mod : Calculate modulo (remainder after division).\n |      DataFrame.pow : Calculate exponential power.\n |      \n |      Notes\n |      -----\n |      Mismatched indices will be unioned together.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'angles': [0, 3, 4],\n |      ...                    'degrees': [360, 180, 360]},\n |      ...                   index=['circle', 'triangle', 'rectangle'])\n |      >>> df\n |                 angles  degrees\n |      circle          0      360\n |      triangle        3      180\n |      rectangle       4      360\n |      \n |      Add a scalar with operator version which return the same\n |      results.\n |      \n |      >>> df + 1\n |                 angles  degrees\n |      circle          1      361\n |      triangle        4      181\n |      rectangle       5      361\n |      \n |      >>> df.add(1)\n |                 angles  degrees\n |      circle          1      361\n |      triangle        4      181\n |      rectangle       5      361\n |      \n |      Divide by constant with reverse version.\n |      \n |      >>> df.div(10)\n |                 angles  degrees\n |      circle        0.0     36.0\n |      triangle      0.3     18.0\n |      rectangle     0.4     36.0\n |      \n |      >>> df.rdiv(10)\n |                   angles   degrees\n |      circle          inf  0.027778\n |      triangle   3.333333  0.055556\n |      rectangle  2.500000  0.027778\n |      \n |      Subtract a list and Series by axis with operator version.\n |      \n |      >>> df - [1, 2]\n |                 angles  degrees\n |      circle         -1      358\n |      triangle        2      178\n |      rectangle       3      358\n |      \n |      >>> df.sub([1, 2], axis='columns')\n |                 angles  degrees\n |      circle         -1      358\n |      triangle        2      178\n |      rectangle       3      358\n |      \n |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n |      ...        axis='index')\n |                 angles  degrees\n |      circle         -1      359\n |      triangle        2      179\n |      rectangle       3      359\n |      \n |      Multiply a DataFrame of different shape with operator version.\n |      \n |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},\n |      ...                      index=['circle', 'triangle', 'rectangle'])\n |      >>> other\n |                 angles\n |      circle          0\n |      triangle        3\n |      rectangle       4\n |      \n |      >>> df * other\n |                 angles  degrees\n |      circle          0      NaN\n |      triangle        9      NaN\n |      rectangle      16      NaN\n |      \n |      >>> df.mul(other, fill_value=0)\n |                 angles  degrees\n |      circle          0      0.0\n |      triangle        9      0.0\n |      rectangle      16      0.0\n |      \n |      Divide by a MultiIndex by level.\n |      \n |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n |      ...                                    ['circle', 'triangle', 'rectangle',\n |      ...                                     'square', 'pentagon', 'hexagon']])\n |      >>> df_multindex\n |                   angles  degrees\n |      A circle          0      360\n |        triangle        3      180\n |        rectangle       4      360\n |      B square          4      360\n |        pentagon        5      540\n |        hexagon         6      720\n |      \n |      >>> df.div(df_multindex, level=1, fill_value=0)\n |                   angles  degrees\n |      A circle        NaN      1.0\n |        triangle      1.0      1.0\n |        rectangle     1.0      1.0\n |      B square        0.0      0.0\n |        pentagon      0.0      0.0\n |        hexagon       0.0      0.0\n |  \n |  ge(self, other, axis='columns', level=None)\n |      Get Greater than or equal to of dataframe and other, element-wise (binary operator `ge`).\n |      \n |      Among flexible wrappers (`eq`, `ne`, `le`, `lt`, `ge`, `gt`) to comparison\n |      operators.\n |      \n |      Equivalent to `==`, `=!`, `<=`, `<`, `>=`, `>` with support to choose axis\n |      (rows or columns) and level for comparison.\n |      \n |      Parameters\n |      ----------\n |      other : scalar, sequence, Series, or DataFrame\n |          Any single or multiple element data structure, or list-like object.\n |      axis :  {0 or 'index', 1 or 'columns'}, default 'columns'\n |          Whether to compare by the index (0 or 'index') or columns\n |          (1 or 'columns').\n |      level : int or label\n |          Broadcast across a level, matching Index values on the passed\n |          MultiIndex level.\n |      \n |      Returns\n |      -------\n |      DataFrame of bool\n |          Result of the comparison.\n |      \n |      See Also\n |      --------\n |      DataFrame.eq : Compare DataFrames for equality elementwise.\n |      DataFrame.ne : Compare DataFrames for inequality elementwise.\n |      DataFrame.le : Compare DataFrames for less than inequality\n |          or equality elementwise.\n |      DataFrame.lt : Compare DataFrames for strictly less than\n |          inequality elementwise.\n |      DataFrame.ge : Compare DataFrames for greater than inequality\n |          or equality elementwise.\n |      DataFrame.gt : Compare DataFrames for strictly greater than\n |          inequality elementwise.\n |      \n |      Notes\n |      -----\n |      Mismatched indices will be unioned together.\n |      `NaN` values are considered different (i.e. `NaN` != `NaN`).\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'cost': [250, 150, 100],\n |      ...                    'revenue': [100, 250, 300]},\n |      ...                   index=['A', 'B', 'C'])\n |      >>> df\n |         cost  revenue\n |      A   250      100\n |      B   150      250\n |      C   100      300\n |      \n |      Comparison with a scalar, using either the operator or method:\n |      \n |      >>> df == 100\n |          cost  revenue\n |      A  False     True\n |      B  False    False\n |      C   True    False\n |      \n |      >>> df.eq(100)\n |          cost  revenue\n |      A  False     True\n |      B  False    False\n |      C   True    False\n |      \n |      When `other` is a :class:`Series`, the columns of a DataFrame are aligned\n |      with the index of `other` and broadcast:\n |      \n |      >>> df != pd.Series([100, 250], index=[\"cost\", \"revenue\"])\n |          cost  revenue\n |      A   True     True\n |      B   True    False\n |      C  False     True\n |      \n |      Use the method to control the broadcast axis:\n |      \n |      >>> df.ne(pd.Series([100, 300], index=[\"A\", \"D\"]), axis='index')\n |         cost  revenue\n |      A  True    False\n |      B  True     True\n |      C  True     True\n |      D  True     True\n |      \n |      When comparing to an arbitrary sequence, the number of columns must\n |      match the number elements in `other`:\n |      \n |      >>> df == [250, 100]\n |          cost  revenue\n |      A   True     True\n |      B  False    False\n |      C  False    False\n |      \n |      Use the method to control the axis:\n |      \n |      >>> df.eq([250, 250, 100], axis='index')\n |          cost  revenue\n |      A   True    False\n |      B  False     True\n |      C   True    False\n |      \n |      Compare to a DataFrame of different shape.\n |      \n |      >>> other = pd.DataFrame({'revenue': [300, 250, 100, 150]},\n |      ...                      index=['A', 'B', 'C', 'D'])\n |      >>> other\n |         revenue\n |      A      300\n |      B      250\n |      C      100\n |      D      150\n |      \n |      >>> df.gt(other)\n |          cost  revenue\n |      A  False    False\n |      B  False    False\n |      C  False     True\n |      D  False    False\n |      \n |      Compare to a MultiIndex by level.\n |      \n |      >>> df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220],\n |      ...                              'revenue': [100, 250, 300, 200, 175, 225]},\n |      ...                             index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'],\n |      ...                                    ['A', 'B', 'C', 'A', 'B', 'C']])\n |      >>> df_multindex\n |            cost  revenue\n |      Q1 A   250      100\n |         B   150      250\n |         C   100      300\n |      Q2 A   150      200\n |         B   300      175\n |         C   220      225\n |      \n |      >>> df.le(df_multindex, level=1)\n |             cost  revenue\n |      Q1 A   True     True\n |         B   True     True\n |         C   True     True\n |      Q2 A  False     True\n |         B   True    False\n |         C   True    False\n |  \n |  get_value(self, index, col, takeable=False)\n |      Quickly retrieve single value at passed column and index.\n |      \n |      .. deprecated:: 0.21.0\n |          Use .at[] or .iat[] accessors instead.\n |      \n |      Parameters\n |      ----------\n |      index : row label\n |      col : column label\n |      takeable : interpret the index/col as indexers, default False\n |      \n |      Returns\n |      -------\n |      scalar\n |  \n |  gt(self, other, axis='columns', level=None)\n |      Get Greater than of dataframe and other, element-wise (binary operator `gt`).\n |      \n |      Among flexible wrappers (`eq`, `ne`, `le`, `lt`, `ge`, `gt`) to comparison\n |      operators.\n |      \n |      Equivalent to `==`, `=!`, `<=`, `<`, `>=`, `>` with support to choose axis\n |      (rows or columns) and level for comparison.\n |      \n |      Parameters\n |      ----------\n |      other : scalar, sequence, Series, or DataFrame\n |          Any single or multiple element data structure, or list-like object.\n |      axis :  {0 or 'index', 1 or 'columns'}, default 'columns'\n |          Whether to compare by the index (0 or 'index') or columns\n |          (1 or 'columns').\n |      level : int or label\n |          Broadcast across a level, matching Index values on the passed\n |          MultiIndex level.\n |      \n |      Returns\n |      -------\n |      DataFrame of bool\n |          Result of the comparison.\n |      \n |      See Also\n |      --------\n |      DataFrame.eq : Compare DataFrames for equality elementwise.\n |      DataFrame.ne : Compare DataFrames for inequality elementwise.\n |      DataFrame.le : Compare DataFrames for less than inequality\n |          or equality elementwise.\n |      DataFrame.lt : Compare DataFrames for strictly less than\n |          inequality elementwise.\n |      DataFrame.ge : Compare DataFrames for greater than inequality\n |          or equality elementwise.\n |      DataFrame.gt : Compare DataFrames for strictly greater than\n |          inequality elementwise.\n |      \n |      Notes\n |      -----\n |      Mismatched indices will be unioned together.\n |      `NaN` values are considered different (i.e. `NaN` != `NaN`).\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'cost': [250, 150, 100],\n |      ...                    'revenue': [100, 250, 300]},\n |      ...                   index=['A', 'B', 'C'])\n |      >>> df\n |         cost  revenue\n |      A   250      100\n |      B   150      250\n |      C   100      300\n |      \n |      Comparison with a scalar, using either the operator or method:\n |      \n |      >>> df == 100\n |          cost  revenue\n |      A  False     True\n |      B  False    False\n |      C   True    False\n |      \n |      >>> df.eq(100)\n |          cost  revenue\n |      A  False     True\n |      B  False    False\n |      C   True    False\n |      \n |      When `other` is a :class:`Series`, the columns of a DataFrame are aligned\n |      with the index of `other` and broadcast:\n |      \n |      >>> df != pd.Series([100, 250], index=[\"cost\", \"revenue\"])\n |          cost  revenue\n |      A   True     True\n |      B   True    False\n |      C  False     True\n |      \n |      Use the method to control the broadcast axis:\n |      \n |      >>> df.ne(pd.Series([100, 300], index=[\"A\", \"D\"]), axis='index')\n |         cost  revenue\n |      A  True    False\n |      B  True     True\n |      C  True     True\n |      D  True     True\n |      \n |      When comparing to an arbitrary sequence, the number of columns must\n |      match the number elements in `other`:\n |      \n |      >>> df == [250, 100]\n |          cost  revenue\n |      A   True     True\n |      B  False    False\n |      C  False    False\n |      \n |      Use the method to control the axis:\n |      \n |      >>> df.eq([250, 250, 100], axis='index')\n |          cost  revenue\n |      A   True    False\n |      B  False     True\n |      C   True    False\n |      \n |      Compare to a DataFrame of different shape.\n |      \n |      >>> other = pd.DataFrame({'revenue': [300, 250, 100, 150]},\n |      ...                      index=['A', 'B', 'C', 'D'])\n |      >>> other\n |         revenue\n |      A      300\n |      B      250\n |      C      100\n |      D      150\n |      \n |      >>> df.gt(other)\n |          cost  revenue\n |      A  False    False\n |      B  False    False\n |      C  False     True\n |      D  False    False\n |      \n |      Compare to a MultiIndex by level.\n |      \n |      >>> df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220],\n |      ...                              'revenue': [100, 250, 300, 200, 175, 225]},\n |      ...                             index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'],\n |      ...                                    ['A', 'B', 'C', 'A', 'B', 'C']])\n |      >>> df_multindex\n |            cost  revenue\n |      Q1 A   250      100\n |         B   150      250\n |         C   100      300\n |      Q2 A   150      200\n |         B   300      175\n |         C   220      225\n |      \n |      >>> df.le(df_multindex, level=1)\n |             cost  revenue\n |      Q1 A   True     True\n |         B   True     True\n |         C   True     True\n |      Q2 A  False     True\n |         B   True    False\n |         C   True    False\n |  \n |  hist = hist_frame(data, column=None, by=None, grid=True, xlabelsize=None, xrot=None, ylabelsize=None, yrot=None, ax=None, sharex=False, sharey=False, figsize=None, layout=None, bins=10, **kwds)\n |      Make a histogram of the DataFrame's.\n |      \n |      A `histogram`_ is a representation of the distribution of data.\n |      This function calls :meth:`matplotlib.pyplot.hist`, on each series in\n |      the DataFrame, resulting in one histogram per column.\n |      \n |      .. _histogram: https://en.wikipedia.org/wiki/Histogram\n |      \n |      Parameters\n |      ----------\n |      data : DataFrame\n |          The pandas object holding the data.\n |      column : string or sequence\n |          If passed, will be used to limit data to a subset of columns.\n |      by : object, optional\n |          If passed, then used to form histograms for separate groups.\n |      grid : bool, default True\n |          Whether to show axis grid lines.\n |      xlabelsize : int, default None\n |          If specified changes the x-axis label size.\n |      xrot : float, default None\n |          Rotation of x axis labels. For example, a value of 90 displays the\n |          x labels rotated 90 degrees clockwise.\n |      ylabelsize : int, default None\n |          If specified changes the y-axis label size.\n |      yrot : float, default None\n |          Rotation of y axis labels. For example, a value of 90 displays the\n |          y labels rotated 90 degrees clockwise.\n |      ax : Matplotlib axes object, default None\n |          The axes to plot the histogram on.\n |      sharex : bool, default True if ax is None else False\n |          In case subplots=True, share x axis and set some x axis labels to\n |          invisible; defaults to True if ax is None otherwise False if an ax\n |          is passed in.\n |          Note that passing in both an ax and sharex=True will alter all x axis\n |          labels for all subplots in a figure.\n |      sharey : bool, default False\n |          In case subplots=True, share y axis and set some y axis labels to\n |          invisible.\n |      figsize : tuple\n |          The size in inches of the figure to create. Uses the value in\n |          `matplotlib.rcParams` by default.\n |      layout : tuple, optional\n |          Tuple of (rows, columns) for the layout of the histograms.\n |      bins : integer or sequence, default 10\n |          Number of histogram bins to be used. If an integer is given, bins + 1\n |          bin edges are calculated and returned. If bins is a sequence, gives\n |          bin edges, including left edge of first bin and right edge of last\n |          bin. In this case, bins is returned unmodified.\n |      **kwds\n |          All other plotting keyword arguments to be passed to\n |          :meth:`matplotlib.pyplot.hist`.\n |      \n |      Returns\n |      -------\n |      matplotlib.AxesSubplot or numpy.ndarray of them\n |      \n |      See Also\n |      --------\n |      matplotlib.pyplot.hist : Plot a histogram using matplotlib.\n |      \n |      Examples\n |      --------\n |      \n |      .. plot::\n |          :context: close-figs\n |      \n |          This example draws a histogram based on the length and width of\n |          some animals, displayed in three bins\n |      \n |          >>> df = pd.DataFrame({\n |          ...     'length': [1.5, 0.5, 1.2, 0.9, 3],\n |          ...     'width': [0.7, 0.2, 0.15, 0.2, 1.1]\n |          ...     }, index= ['pig', 'rabbit', 'duck', 'chicken', 'horse'])\n |          >>> hist = df.hist(bins=3)\n |  \n |  idxmax(self, axis=0, skipna=True)\n |      Return index of first occurrence of maximum over requested axis.\n |      NA/null values are excluded.\n |      \n |      Parameters\n |      ----------\n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          0 or 'index' for row-wise, 1 or 'columns' for column-wise\n |      skipna : boolean, default True\n |          Exclude NA/null values. If an entire row/column is NA, the result\n |          will be NA.\n |      \n |      Returns\n |      -------\n |      Series\n |          Indexes of maxima along the specified axis.\n |      \n |      Raises\n |      ------\n |      ValueError\n |          * If the row/column is empty\n |      \n |      See Also\n |      --------\n |      Series.idxmax\n |      \n |      Notes\n |      -----\n |      This method is the DataFrame version of ``ndarray.argmax``.\n |  \n |  idxmin(self, axis=0, skipna=True)\n |      Return index of first occurrence of minimum over requested axis.\n |      NA/null values are excluded.\n |      \n |      Parameters\n |      ----------\n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          0 or 'index' for row-wise, 1 or 'columns' for column-wise\n |      skipna : boolean, default True\n |          Exclude NA/null values. If an entire row/column is NA, the result\n |          will be NA.\n |      \n |      Returns\n |      -------\n |      Series\n |          Indexes of minima along the specified axis.\n |      \n |      Raises\n |      ------\n |      ValueError\n |          * If the row/column is empty\n |      \n |      See Also\n |      --------\n |      Series.idxmin\n |      \n |      Notes\n |      -----\n |      This method is the DataFrame version of ``ndarray.argmin``.\n |  \n |  info(self, verbose=None, buf=None, max_cols=None, memory_usage=None, null_counts=None)\n |      Print a concise summary of a DataFrame.\n |      \n |      This method prints information about a DataFrame including\n |      the index dtype and column dtypes, non-null values and memory usage.\n |      \n |      Parameters\n |      ----------\n |      verbose : bool, optional\n |          Whether to print the full summary. By default, the setting in\n |          ``pandas.options.display.max_info_columns`` is followed.\n |      buf : writable buffer, defaults to sys.stdout\n |          Where to send the output. By default, the output is printed to\n |          sys.stdout. Pass a writable buffer if you need to further process\n |          the output.\n |      max_cols : int, optional\n |          When to switch from the verbose to the truncated output. If the\n |          DataFrame has more than `max_cols` columns, the truncated output\n |          is used. By default, the setting in\n |          ``pandas.options.display.max_info_columns`` is used.\n |      memory_usage : bool, str, optional\n |          Specifies whether total memory usage of the DataFrame\n |          elements (including the index) should be displayed. By default,\n |          this follows the ``pandas.options.display.memory_usage`` setting.\n |      \n |          True always show memory usage. False never shows memory usage.\n |          A value of 'deep' is equivalent to \"True with deep introspection\".\n |          Memory usage is shown in human-readable units (base-2\n |          representation). Without deep introspection a memory estimation is\n |          made based in column dtype and number of rows assuming values\n |          consume the same memory amount for corresponding dtypes. With deep\n |          memory introspection, a real memory usage calculation is performed\n |          at the cost of computational resources.\n |      null_counts : bool, optional\n |          Whether to show the non-null counts. By default, this is shown\n |          only if the frame is smaller than\n |          ``pandas.options.display.max_info_rows`` and\n |          ``pandas.options.display.max_info_columns``. A value of True always\n |          shows the counts, and False never shows the counts.\n |      \n |      Returns\n |      -------\n |      None\n |          This method prints a summary of a DataFrame and returns None.\n |      \n |      See Also\n |      --------\n |      DataFrame.describe: Generate descriptive statistics of DataFrame\n |          columns.\n |      DataFrame.memory_usage: Memory usage of DataFrame columns.\n |      \n |      Examples\n |      --------\n |      >>> int_values = [1, 2, 3, 4, 5]\n |      >>> text_values = ['alpha', 'beta', 'gamma', 'delta', 'epsilon']\n |      >>> float_values = [0.0, 0.25, 0.5, 0.75, 1.0]\n |      >>> df = pd.DataFrame({\"int_col\": int_values, \"text_col\": text_values,\n |      ...                   \"float_col\": float_values})\n |      >>> df\n |         int_col text_col  float_col\n |      0        1    alpha       0.00\n |      1        2     beta       0.25\n |      2        3    gamma       0.50\n |      3        4    delta       0.75\n |      4        5  epsilon       1.00\n |      \n |      Prints information of all columns:\n |      \n |      >>> df.info(verbose=True)\n |      <class 'pandas.core.frame.DataFrame'>\n |      RangeIndex: 5 entries, 0 to 4\n |      Data columns (total 3 columns):\n |      int_col      5 non-null int64\n |      text_col     5 non-null object\n |      float_col    5 non-null float64\n |      dtypes: float64(1), int64(1), object(1)\n |      memory usage: 248.0+ bytes\n |      \n |      Prints a summary of columns count and its dtypes but not per column\n |      information:\n |      \n |      >>> df.info(verbose=False)\n |      <class 'pandas.core.frame.DataFrame'>\n |      RangeIndex: 5 entries, 0 to 4\n |      Columns: 3 entries, int_col to float_col\n |      dtypes: float64(1), int64(1), object(1)\n |      memory usage: 248.0+ bytes\n |      \n |      Pipe output of DataFrame.info to buffer instead of sys.stdout, get\n |      buffer content and writes to a text file:\n |      \n |      >>> import io\n |      >>> buffer = io.StringIO()\n |      >>> df.info(buf=buffer)\n |      >>> s = buffer.getvalue()\n |      >>> with open(\"df_info.txt\", \"w\",\n |      ...           encoding=\"utf-8\") as f:  # doctest: +SKIP\n |      ...     f.write(s)\n |      260\n |      \n |      The `memory_usage` parameter allows deep introspection mode, specially\n |      useful for big DataFrames and fine-tune memory optimization:\n |      \n |      >>> random_strings_array = np.random.choice(['a', 'b', 'c'], 10 ** 6)\n |      >>> df = pd.DataFrame({\n |      ...     'column_1': np.random.choice(['a', 'b', 'c'], 10 ** 6),\n |      ...     'column_2': np.random.choice(['a', 'b', 'c'], 10 ** 6),\n |      ...     'column_3': np.random.choice(['a', 'b', 'c'], 10 ** 6)\n |      ... })\n |      >>> df.info()\n |      <class 'pandas.core.frame.DataFrame'>\n |      RangeIndex: 1000000 entries, 0 to 999999\n |      Data columns (total 3 columns):\n |      column_1    1000000 non-null object\n |      column_2    1000000 non-null object\n |      column_3    1000000 non-null object\n |      dtypes: object(3)\n |      memory usage: 22.9+ MB\n |      \n |      >>> df.info(memory_usage='deep')\n |      <class 'pandas.core.frame.DataFrame'>\n |      RangeIndex: 1000000 entries, 0 to 999999\n |      Data columns (total 3 columns):\n |      column_1    1000000 non-null object\n |      column_2    1000000 non-null object\n |      column_3    1000000 non-null object\n |      dtypes: object(3)\n |      memory usage: 188.8 MB\n |  \n |  insert(self, loc, column, value, allow_duplicates=False)\n |      Insert column into DataFrame at specified location.\n |      \n |      Raises a ValueError if `column` is already contained in the DataFrame,\n |      unless `allow_duplicates` is set to True.\n |      \n |      Parameters\n |      ----------\n |      loc : int\n |          Insertion index. Must verify 0 <= loc <= len(columns)\n |      column : string, number, or hashable object\n |          label of the inserted column\n |      value : int, Series, or array-like\n |      allow_duplicates : bool, optional\n |  \n |  isin(self, values)\n |      Whether each element in the DataFrame is contained in values.\n |      \n |      Parameters\n |      ----------\n |      values : iterable, Series, DataFrame or dict\n |          The result will only be true at a location if all the\n |          labels match. If `values` is a Series, that's the index. If\n |          `values` is a dict, the keys must be the column names,\n |          which must match. If `values` is a DataFrame,\n |          then both the index and column labels must match.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          DataFrame of booleans showing whether each element in the DataFrame\n |          is contained in values.\n |      \n |      See Also\n |      --------\n |      DataFrame.eq: Equality test for DataFrame.\n |      Series.isin: Equivalent method on Series.\n |      Series.str.contains: Test if pattern or regex is contained within a\n |          string of a Series or Index.\n |      \n |      Examples\n |      --------\n |      \n |      >>> df = pd.DataFrame({'num_legs': [2, 4], 'num_wings': [2, 0]},\n |      ...                   index=['falcon', 'dog'])\n |      >>> df\n |              num_legs  num_wings\n |      falcon         2          2\n |      dog            4          0\n |      \n |      When ``values`` is a list check whether every value in the DataFrame\n |      is present in the list (which animals have 0 or 2 legs or wings)\n |      \n |      >>> df.isin([0, 2])\n |              num_legs  num_wings\n |      falcon      True       True\n |      dog        False       True\n |      \n |      When ``values`` is a dict, we can pass values to check for each\n |      column separately:\n |      \n |      >>> df.isin({'num_wings': [0, 3]})\n |              num_legs  num_wings\n |      falcon     False      False\n |      dog        False       True\n |      \n |      When ``values`` is a Series or DataFrame the index and column must\n |      match. Note that 'falcon' does not match based on the number of legs\n |      in df2.\n |      \n |      >>> other = pd.DataFrame({'num_legs': [8, 2], 'num_wings': [0, 2]},\n |      ...                      index=['spider', 'falcon'])\n |      >>> df.isin(other)\n |              num_legs  num_wings\n |      falcon      True       True\n |      dog        False      False\n |  \n |  isna(self)\n |      Detect missing values.\n |      \n |      Return a boolean same-sized object indicating if the values are NA.\n |      NA values, such as None or :attr:`numpy.NaN`, gets mapped to True\n |      values.\n |      Everything else gets mapped to False values. Characters such as empty\n |      strings ``''`` or :attr:`numpy.inf` are not considered NA values\n |      (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          Mask of bool values for each element in DataFrame that\n |          indicates whether an element is not an NA value.\n |      \n |      See Also\n |      --------\n |      DataFrame.isnull : Alias of isna.\n |      DataFrame.notna : Boolean inverse of isna.\n |      DataFrame.dropna : Omit axes labels with missing values.\n |      isna : Top-level isna.\n |      \n |      Examples\n |      --------\n |      Show which entries in a DataFrame are NA.\n |      \n |      >>> df = pd.DataFrame({'age': [5, 6, np.NaN],\n |      ...                    'born': [pd.NaT, pd.Timestamp('1939-05-27'),\n |      ...                             pd.Timestamp('1940-04-25')],\n |      ...                    'name': ['Alfred', 'Batman', ''],\n |      ...                    'toy': [None, 'Batmobile', 'Joker']})\n |      >>> df\n |         age       born    name        toy\n |      0  5.0        NaT  Alfred       None\n |      1  6.0 1939-05-27  Batman  Batmobile\n |      2  NaN 1940-04-25              Joker\n |      \n |      >>> df.isna()\n |           age   born   name    toy\n |      0  False   True  False   True\n |      1  False  False  False  False\n |      2   True  False  False  False\n |      \n |      Show which entries in a Series are NA.\n |      \n |      >>> ser = pd.Series([5, 6, np.NaN])\n |      >>> ser\n |      0    5.0\n |      1    6.0\n |      2    NaN\n |      dtype: float64\n |      \n |      >>> ser.isna()\n |      0    False\n |      1    False\n |      2     True\n |      dtype: bool\n |  \n |  isnull(self)\n |      Detect missing values.\n |      \n |      Return a boolean same-sized object indicating if the values are NA.\n |      NA values, such as None or :attr:`numpy.NaN`, gets mapped to True\n |      values.\n |      Everything else gets mapped to False values. Characters such as empty\n |      strings ``''`` or :attr:`numpy.inf` are not considered NA values\n |      (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          Mask of bool values for each element in DataFrame that\n |          indicates whether an element is not an NA value.\n |      \n |      See Also\n |      --------\n |      DataFrame.isnull : Alias of isna.\n |      DataFrame.notna : Boolean inverse of isna.\n |      DataFrame.dropna : Omit axes labels with missing values.\n |      isna : Top-level isna.\n |      \n |      Examples\n |      --------\n |      Show which entries in a DataFrame are NA.\n |      \n |      >>> df = pd.DataFrame({'age': [5, 6, np.NaN],\n |      ...                    'born': [pd.NaT, pd.Timestamp('1939-05-27'),\n |      ...                             pd.Timestamp('1940-04-25')],\n |      ...                    'name': ['Alfred', 'Batman', ''],\n |      ...                    'toy': [None, 'Batmobile', 'Joker']})\n |      >>> df\n |         age       born    name        toy\n |      0  5.0        NaT  Alfred       None\n |      1  6.0 1939-05-27  Batman  Batmobile\n |      2  NaN 1940-04-25              Joker\n |      \n |      >>> df.isna()\n |           age   born   name    toy\n |      0  False   True  False   True\n |      1  False  False  False  False\n |      2   True  False  False  False\n |      \n |      Show which entries in a Series are NA.\n |      \n |      >>> ser = pd.Series([5, 6, np.NaN])\n |      >>> ser\n |      0    5.0\n |      1    6.0\n |      2    NaN\n |      dtype: float64\n |      \n |      >>> ser.isna()\n |      0    False\n |      1    False\n |      2     True\n |      dtype: bool\n |  \n |  items(self)\n |      Iterator over (column name, Series) pairs.\n |      \n |      Iterates over the DataFrame columns, returning a tuple with\n |      the column name and the content as a Series.\n |      \n |      Yields\n |      ------\n |      label : object\n |          The column names for the DataFrame being iterated over.\n |      content : Series\n |          The column entries belonging to each label, as a Series.\n |      \n |      See Also\n |      --------\n |      DataFrame.iterrows : Iterate over DataFrame rows as\n |          (index, Series) pairs.\n |      DataFrame.itertuples : Iterate over DataFrame rows as namedtuples\n |          of the values.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'species': ['bear', 'bear', 'marsupial'],\n |      ...                   'population': [1864, 22000, 80000]},\n |      ...                   index=['panda', 'polar', 'koala'])\n |      >>> df\n |              species   population\n |      panda   bear      1864\n |      polar   bear      22000\n |      koala   marsupial 80000\n |      >>> for label, content in df.items():\n |      ...     print('label:', label)\n |      ...     print('content:', content, sep='\\n')\n |      ...\n |      label: species\n |      content:\n |      panda         bear\n |      polar         bear\n |      koala    marsupial\n |      Name: species, dtype: object\n |      label: population\n |      content:\n |      panda     1864\n |      polar    22000\n |      koala    80000\n |      Name: population, dtype: int64\n |  \n |  iteritems(self)\n |      Iterator over (column name, Series) pairs.\n |      \n |      Iterates over the DataFrame columns, returning a tuple with\n |      the column name and the content as a Series.\n |      \n |      Returns\n |      -------\n |      label : object\n |          The column names for the DataFrame being iterated over.\n |      content : Series\n |          The column entries belonging to each label, as a Series.\n |      \n |      See Also\n |      --------\n |      DataFrame.iterrows : Iterate over DataFrame rows as\n |          (index, Series) pairs.\n |      DataFrame.itertuples : Iterate over DataFrame rows as namedtuples\n |          of the values.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'species': ['bear', 'bear', 'marsupial'],\n |      ...                   'population': [1864, 22000, 80000]},\n |      ...                   index=['panda', 'polar', 'koala'])\n |      >>> df\n |              species   population\n |      panda   bear      1864\n |      polar   bear      22000\n |      koala   marsupial 80000\n |      >>> for label, content in df.items():\n |      ...     print('label:', label)\n |      ...     print('content:', content, sep='\\n')\n |      ...\n |      label: species\n |      content:\n |      panda         bear\n |      polar         bear\n |      koala    marsupial\n |      Name: species, dtype: object\n |      label: population\n |      content:\n |      panda     1864\n |      polar    22000\n |      koala    80000\n |      Name: population, dtype: int64\n |  \n |  iterrows(self)\n |      Iterate over DataFrame rows as (index, Series) pairs.\n |      \n |      Yields\n |      ------\n |      index : label or tuple of label\n |          The index of the row. A tuple for a `MultiIndex`.\n |      data : Series\n |          The data of the row as a Series.\n |      \n |      it : generator\n |          A generator that iterates over the rows of the frame.\n |      \n |      See Also\n |      --------\n |      itertuples : Iterate over DataFrame rows as namedtuples of the values.\n |      items : Iterate over (column name, Series) pairs.\n |      \n |      Notes\n |      -----\n |      \n |      1. Because ``iterrows`` returns a Series for each row,\n |         it does **not** preserve dtypes across the rows (dtypes are\n |         preserved across columns for DataFrames). For example,\n |      \n |         >>> df = pd.DataFrame([[1, 1.5]], columns=['int', 'float'])\n |         >>> row = next(df.iterrows())[1]\n |         >>> row\n |         int      1.0\n |         float    1.5\n |         Name: 0, dtype: float64\n |         >>> print(row['int'].dtype)\n |         float64\n |         >>> print(df['int'].dtype)\n |         int64\n |      \n |         To preserve dtypes while iterating over the rows, it is better\n |         to use :meth:`itertuples` which returns namedtuples of the values\n |         and which is generally faster than ``iterrows``.\n |      \n |      2. You should **never modify** something you are iterating over.\n |         This is not guaranteed to work in all cases. Depending on the\n |         data types, the iterator returns a copy and not a view, and writing\n |         to it will have no effect.\n |  \n |  itertuples(self, index=True, name='Pandas')\n |      Iterate over DataFrame rows as namedtuples.\n |      \n |      Parameters\n |      ----------\n |      index : bool, default True\n |          If True, return the index as the first element of the tuple.\n |      name : str or None, default \"Pandas\"\n |          The name of the returned namedtuples or None to return regular\n |          tuples.\n |      \n |      Returns\n |      -------\n |      iterator\n |          An object to iterate over namedtuples for each row in the\n |          DataFrame with the first field possibly being the index and\n |          following fields being the column values.\n |      \n |      See Also\n |      --------\n |      DataFrame.iterrows : Iterate over DataFrame rows as (index, Series)\n |          pairs.\n |      DataFrame.items : Iterate over (column name, Series) pairs.\n |      \n |      Notes\n |      -----\n |      The column names will be renamed to positional names if they are\n |      invalid Python identifiers, repeated, or start with an underscore.\n |      With a large number of columns (>255), regular tuples are returned.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'num_legs': [4, 2], 'num_wings': [0, 2]},\n |      ...                   index=['dog', 'hawk'])\n |      >>> df\n |            num_legs  num_wings\n |      dog          4          0\n |      hawk         2          2\n |      >>> for row in df.itertuples():\n |      ...     print(row)\n |      ...\n |      Pandas(Index='dog', num_legs=4, num_wings=0)\n |      Pandas(Index='hawk', num_legs=2, num_wings=2)\n |      \n |      By setting the `index` parameter to False we can remove the index\n |      as the first element of the tuple:\n |      \n |      >>> for row in df.itertuples(index=False):\n |      ...     print(row)\n |      ...\n |      Pandas(num_legs=4, num_wings=0)\n |      Pandas(num_legs=2, num_wings=2)\n |      \n |      With the `name` parameter set we set a custom name for the yielded\n |      namedtuples:\n |      \n |      >>> for row in df.itertuples(name='Animal'):\n |      ...     print(row)\n |      ...\n |      Animal(Index='dog', num_legs=4, num_wings=0)\n |      Animal(Index='hawk', num_legs=2, num_wings=2)\n |  \n |  join(self, other, on=None, how='left', lsuffix='', rsuffix='', sort=False)\n |      Join columns of another DataFrame.\n |      \n |      Join columns with `other` DataFrame either on index or on a key\n |      column. Efficiently join multiple DataFrame objects by index at once by\n |      passing a list.\n |      \n |      Parameters\n |      ----------\n |      other : DataFrame, Series, or list of DataFrame\n |          Index should be similar to one of the columns in this one. If a\n |          Series is passed, its name attribute must be set, and that will be\n |          used as the column name in the resulting joined DataFrame.\n |      on : str, list of str, or array-like, optional\n |          Column or index level name(s) in the caller to join on the index\n |          in `other`, otherwise joins index-on-index. If multiple\n |          values given, the `other` DataFrame must have a MultiIndex. Can\n |          pass an array as the join key if it is not already contained in\n |          the calling DataFrame. Like an Excel VLOOKUP operation.\n |      how : {'left', 'right', 'outer', 'inner'}, default 'left'\n |          How to handle the operation of the two objects.\n |      \n |          * left: use calling frame's index (or column if on is specified)\n |          * right: use `other`'s index.\n |          * outer: form union of calling frame's index (or column if on is\n |            specified) with `other`'s index, and sort it.\n |            lexicographically.\n |          * inner: form intersection of calling frame's index (or column if\n |            on is specified) with `other`'s index, preserving the order\n |            of the calling's one.\n |      lsuffix : str, default ''\n |          Suffix to use from left frame's overlapping columns.\n |      rsuffix : str, default ''\n |          Suffix to use from right frame's overlapping columns.\n |      sort : bool, default False\n |          Order result DataFrame lexicographically by the join key. If False,\n |          the order of the join key depends on the join type (how keyword).\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          A dataframe containing columns from both the caller and `other`.\n |      \n |      See Also\n |      --------\n |      DataFrame.merge : For column(s)-on-columns(s) operations.\n |      \n |      Notes\n |      -----\n |      Parameters `on`, `lsuffix`, and `rsuffix` are not supported when\n |      passing a list of `DataFrame` objects.\n |      \n |      Support for specifying index levels as the `on` parameter was added\n |      in version 0.23.0.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3', 'K4', 'K5'],\n |      ...                    'A': ['A0', 'A1', 'A2', 'A3', 'A4', 'A5']})\n |      \n |      >>> df\n |        key   A\n |      0  K0  A0\n |      1  K1  A1\n |      2  K2  A2\n |      3  K3  A3\n |      4  K4  A4\n |      5  K5  A5\n |      \n |      >>> other = pd.DataFrame({'key': ['K0', 'K1', 'K2'],\n |      ...                       'B': ['B0', 'B1', 'B2']})\n |      \n |      >>> other\n |        key   B\n |      0  K0  B0\n |      1  K1  B1\n |      2  K2  B2\n |      \n |      Join DataFrames using their indexes.\n |      \n |      >>> df.join(other, lsuffix='_caller', rsuffix='_other')\n |        key_caller   A key_other    B\n |      0         K0  A0        K0   B0\n |      1         K1  A1        K1   B1\n |      2         K2  A2        K2   B2\n |      3         K3  A3       NaN  NaN\n |      4         K4  A4       NaN  NaN\n |      5         K5  A5       NaN  NaN\n |      \n |      If we want to join using the key columns, we need to set key to be\n |      the index in both `df` and `other`. The joined DataFrame will have\n |      key as its index.\n |      \n |      >>> df.set_index('key').join(other.set_index('key'))\n |            A    B\n |      key\n |      K0   A0   B0\n |      K1   A1   B1\n |      K2   A2   B2\n |      K3   A3  NaN\n |      K4   A4  NaN\n |      K5   A5  NaN\n |      \n |      Another option to join using the key columns is to use the `on`\n |      parameter. DataFrame.join always uses `other`'s index but we can use\n |      any column in `df`. This method preserves the original DataFrame's\n |      index in the result.\n |      \n |      >>> df.join(other.set_index('key'), on='key')\n |        key   A    B\n |      0  K0  A0   B0\n |      1  K1  A1   B1\n |      2  K2  A2   B2\n |      3  K3  A3  NaN\n |      4  K4  A4  NaN\n |      5  K5  A5  NaN\n |  \n |  kurt(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs)\n |      Return unbiased kurtosis over requested axis using Fisher's definition of\n |      kurtosis (kurtosis of normal == 0.0). Normalized by N-1.\n |      \n |      Parameters\n |      ----------\n |      axis : {index (0), columns (1)}\n |          Axis for the function to be applied on.\n |      skipna : bool, default True\n |          Exclude NA/null values when computing the result.\n |      level : int or level name, default None\n |          If the axis is a MultiIndex (hierarchical), count along a\n |          particular level, collapsing into a Series.\n |      numeric_only : bool, default None\n |          Include only float, int, boolean columns. If None, will attempt to use\n |          everything, then use only numeric data. Not implemented for Series.\n |      **kwargs\n |          Additional keyword arguments to be passed to the function.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame (if level specified)\n |  \n |  kurtosis = kurt(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs)\n |  \n |  le(self, other, axis='columns', level=None)\n |      Get Less than or equal to of dataframe and other, element-wise (binary operator `le`).\n |      \n |      Among flexible wrappers (`eq`, `ne`, `le`, `lt`, `ge`, `gt`) to comparison\n |      operators.\n |      \n |      Equivalent to `==`, `=!`, `<=`, `<`, `>=`, `>` with support to choose axis\n |      (rows or columns) and level for comparison.\n |      \n |      Parameters\n |      ----------\n |      other : scalar, sequence, Series, or DataFrame\n |          Any single or multiple element data structure, or list-like object.\n |      axis :  {0 or 'index', 1 or 'columns'}, default 'columns'\n |          Whether to compare by the index (0 or 'index') or columns\n |          (1 or 'columns').\n |      level : int or label\n |          Broadcast across a level, matching Index values on the passed\n |          MultiIndex level.\n |      \n |      Returns\n |      -------\n |      DataFrame of bool\n |          Result of the comparison.\n |      \n |      See Also\n |      --------\n |      DataFrame.eq : Compare DataFrames for equality elementwise.\n |      DataFrame.ne : Compare DataFrames for inequality elementwise.\n |      DataFrame.le : Compare DataFrames for less than inequality\n |          or equality elementwise.\n |      DataFrame.lt : Compare DataFrames for strictly less than\n |          inequality elementwise.\n |      DataFrame.ge : Compare DataFrames for greater than inequality\n |          or equality elementwise.\n |      DataFrame.gt : Compare DataFrames for strictly greater than\n |          inequality elementwise.\n |      \n |      Notes\n |      -----\n |      Mismatched indices will be unioned together.\n |      `NaN` values are considered different (i.e. `NaN` != `NaN`).\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'cost': [250, 150, 100],\n |      ...                    'revenue': [100, 250, 300]},\n |      ...                   index=['A', 'B', 'C'])\n |      >>> df\n |         cost  revenue\n |      A   250      100\n |      B   150      250\n |      C   100      300\n |      \n |      Comparison with a scalar, using either the operator or method:\n |      \n |      >>> df == 100\n |          cost  revenue\n |      A  False     True\n |      B  False    False\n |      C   True    False\n |      \n |      >>> df.eq(100)\n |          cost  revenue\n |      A  False     True\n |      B  False    False\n |      C   True    False\n |      \n |      When `other` is a :class:`Series`, the columns of a DataFrame are aligned\n |      with the index of `other` and broadcast:\n |      \n |      >>> df != pd.Series([100, 250], index=[\"cost\", \"revenue\"])\n |          cost  revenue\n |      A   True     True\n |      B   True    False\n |      C  False     True\n |      \n |      Use the method to control the broadcast axis:\n |      \n |      >>> df.ne(pd.Series([100, 300], index=[\"A\", \"D\"]), axis='index')\n |         cost  revenue\n |      A  True    False\n |      B  True     True\n |      C  True     True\n |      D  True     True\n |      \n |      When comparing to an arbitrary sequence, the number of columns must\n |      match the number elements in `other`:\n |      \n |      >>> df == [250, 100]\n |          cost  revenue\n |      A   True     True\n |      B  False    False\n |      C  False    False\n |      \n |      Use the method to control the axis:\n |      \n |      >>> df.eq([250, 250, 100], axis='index')\n |          cost  revenue\n |      A   True    False\n |      B  False     True\n |      C   True    False\n |      \n |      Compare to a DataFrame of different shape.\n |      \n |      >>> other = pd.DataFrame({'revenue': [300, 250, 100, 150]},\n |      ...                      index=['A', 'B', 'C', 'D'])\n |      >>> other\n |         revenue\n |      A      300\n |      B      250\n |      C      100\n |      D      150\n |      \n |      >>> df.gt(other)\n |          cost  revenue\n |      A  False    False\n |      B  False    False\n |      C  False     True\n |      D  False    False\n |      \n |      Compare to a MultiIndex by level.\n |      \n |      >>> df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220],\n |      ...                              'revenue': [100, 250, 300, 200, 175, 225]},\n |      ...                             index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'],\n |      ...                                    ['A', 'B', 'C', 'A', 'B', 'C']])\n |      >>> df_multindex\n |            cost  revenue\n |      Q1 A   250      100\n |         B   150      250\n |         C   100      300\n |      Q2 A   150      200\n |         B   300      175\n |         C   220      225\n |      \n |      >>> df.le(df_multindex, level=1)\n |             cost  revenue\n |      Q1 A   True     True\n |         B   True     True\n |         C   True     True\n |      Q2 A  False     True\n |         B   True    False\n |         C   True    False\n |  \n |  lookup(self, row_labels, col_labels)\n |      Label-based \"fancy indexing\" function for DataFrame.\n |      \n |      Given equal-length arrays of row and column labels, return an\n |      array of the values corresponding to each (row, col) pair.\n |      \n |      Parameters\n |      ----------\n |      row_labels : sequence\n |          The row labels to use for lookup\n |      col_labels : sequence\n |          The column labels to use for lookup\n |      \n |      Returns\n |      -------\n |      numpy.ndarray\n |      \n |      Notes\n |      -----\n |      Akin to::\n |      \n |          result = [df.get_value(row, col)\n |                    for row, col in zip(row_labels, col_labels)]\n |      \n |      Examples\n |      --------\n |      values : ndarray\n |          The found values\n |  \n |  lt(self, other, axis='columns', level=None)\n |      Get Less than of dataframe and other, element-wise (binary operator `lt`).\n |      \n |      Among flexible wrappers (`eq`, `ne`, `le`, `lt`, `ge`, `gt`) to comparison\n |      operators.\n |      \n |      Equivalent to `==`, `=!`, `<=`, `<`, `>=`, `>` with support to choose axis\n |      (rows or columns) and level for comparison.\n |      \n |      Parameters\n |      ----------\n |      other : scalar, sequence, Series, or DataFrame\n |          Any single or multiple element data structure, or list-like object.\n |      axis :  {0 or 'index', 1 or 'columns'}, default 'columns'\n |          Whether to compare by the index (0 or 'index') or columns\n |          (1 or 'columns').\n |      level : int or label\n |          Broadcast across a level, matching Index values on the passed\n |          MultiIndex level.\n |      \n |      Returns\n |      -------\n |      DataFrame of bool\n |          Result of the comparison.\n |      \n |      See Also\n |      --------\n |      DataFrame.eq : Compare DataFrames for equality elementwise.\n |      DataFrame.ne : Compare DataFrames for inequality elementwise.\n |      DataFrame.le : Compare DataFrames for less than inequality\n |          or equality elementwise.\n |      DataFrame.lt : Compare DataFrames for strictly less than\n |          inequality elementwise.\n |      DataFrame.ge : Compare DataFrames for greater than inequality\n |          or equality elementwise.\n |      DataFrame.gt : Compare DataFrames for strictly greater than\n |          inequality elementwise.\n |      \n |      Notes\n |      -----\n |      Mismatched indices will be unioned together.\n |      `NaN` values are considered different (i.e. `NaN` != `NaN`).\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'cost': [250, 150, 100],\n |      ...                    'revenue': [100, 250, 300]},\n |      ...                   index=['A', 'B', 'C'])\n |      >>> df\n |         cost  revenue\n |      A   250      100\n |      B   150      250\n |      C   100      300\n |      \n |      Comparison with a scalar, using either the operator or method:\n |      \n |      >>> df == 100\n |          cost  revenue\n |      A  False     True\n |      B  False    False\n |      C   True    False\n |      \n |      >>> df.eq(100)\n |          cost  revenue\n |      A  False     True\n |      B  False    False\n |      C   True    False\n |      \n |      When `other` is a :class:`Series`, the columns of a DataFrame are aligned\n |      with the index of `other` and broadcast:\n |      \n |      >>> df != pd.Series([100, 250], index=[\"cost\", \"revenue\"])\n |          cost  revenue\n |      A   True     True\n |      B   True    False\n |      C  False     True\n |      \n |      Use the method to control the broadcast axis:\n |      \n |      >>> df.ne(pd.Series([100, 300], index=[\"A\", \"D\"]), axis='index')\n |         cost  revenue\n |      A  True    False\n |      B  True     True\n |      C  True     True\n |      D  True     True\n |      \n |      When comparing to an arbitrary sequence, the number of columns must\n |      match the number elements in `other`:\n |      \n |      >>> df == [250, 100]\n |          cost  revenue\n |      A   True     True\n |      B  False    False\n |      C  False    False\n |      \n |      Use the method to control the axis:\n |      \n |      >>> df.eq([250, 250, 100], axis='index')\n |          cost  revenue\n |      A   True    False\n |      B  False     True\n |      C   True    False\n |      \n |      Compare to a DataFrame of different shape.\n |      \n |      >>> other = pd.DataFrame({'revenue': [300, 250, 100, 150]},\n |      ...                      index=['A', 'B', 'C', 'D'])\n |      >>> other\n |         revenue\n |      A      300\n |      B      250\n |      C      100\n |      D      150\n |      \n |      >>> df.gt(other)\n |          cost  revenue\n |      A  False    False\n |      B  False    False\n |      C  False     True\n |      D  False    False\n |      \n |      Compare to a MultiIndex by level.\n |      \n |      >>> df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220],\n |      ...                              'revenue': [100, 250, 300, 200, 175, 225]},\n |      ...                             index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'],\n |      ...                                    ['A', 'B', 'C', 'A', 'B', 'C']])\n |      >>> df_multindex\n |            cost  revenue\n |      Q1 A   250      100\n |         B   150      250\n |         C   100      300\n |      Q2 A   150      200\n |         B   300      175\n |         C   220      225\n |      \n |      >>> df.le(df_multindex, level=1)\n |             cost  revenue\n |      Q1 A   True     True\n |         B   True     True\n |         C   True     True\n |      Q2 A  False     True\n |         B   True    False\n |         C   True    False\n |  \n |  mad(self, axis=None, skipna=None, level=None)\n |      Return the mean absolute deviation of the values for the requested axis.\n |      \n |      Parameters\n |      ----------\n |      axis : {index (0), columns (1)}\n |          Axis for the function to be applied on.\n |      skipna : bool, default True\n |          Exclude NA/null values when computing the result.\n |      level : int or level name, default None\n |          If the axis is a MultiIndex (hierarchical), count along a\n |          particular level, collapsing into a Series.\n |      numeric_only : bool, default None\n |          Include only float, int, boolean columns. If None, will attempt to use\n |          everything, then use only numeric data. Not implemented for Series.\n |      **kwargs\n |          Additional keyword arguments to be passed to the function.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame (if level specified)\n |  \n |  max(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs)\n |      Return the maximum of the values for the requested axis.\n |      \n |                  If you want the *index* of the maximum, use ``idxmax``. This is\n |                  the equivalent of the ``numpy.ndarray`` method ``argmax``.\n |      \n |      Parameters\n |      ----------\n |      axis : {index (0), columns (1)}\n |          Axis for the function to be applied on.\n |      skipna : bool, default True\n |          Exclude NA/null values when computing the result.\n |      level : int or level name, default None\n |          If the axis is a MultiIndex (hierarchical), count along a\n |          particular level, collapsing into a Series.\n |      numeric_only : bool, default None\n |          Include only float, int, boolean columns. If None, will attempt to use\n |          everything, then use only numeric data. Not implemented for Series.\n |      **kwargs\n |          Additional keyword arguments to be passed to the function.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame (if level specified)\n |      \n |      See Also\n |      --------\n |      Series.sum : Return the sum.\n |      Series.min : Return the minimum.\n |      Series.max : Return the maximum.\n |      Series.idxmin : Return the index of the minimum.\n |      Series.idxmax : Return the index of the maximum.\n |      DataFrame.sum : Return the sum over the requested axis.\n |      DataFrame.min : Return the minimum over the requested axis.\n |      DataFrame.max : Return the maximum over the requested axis.\n |      DataFrame.idxmin : Return the index of the minimum over the requested axis.\n |      DataFrame.idxmax : Return the index of the maximum over the requested axis.\n |      \n |      Examples\n |      --------\n |      >>> idx = pd.MultiIndex.from_arrays([\n |      ...     ['warm', 'warm', 'cold', 'cold'],\n |      ...     ['dog', 'falcon', 'fish', 'spider']],\n |      ...     names=['blooded', 'animal'])\n |      >>> s = pd.Series([4, 2, 0, 8], name='legs', index=idx)\n |      >>> s\n |      blooded  animal\n |      warm     dog       4\n |               falcon    2\n |      cold     fish      0\n |               spider    8\n |      Name: legs, dtype: int64\n |      \n |      >>> s.max()\n |      8\n |      \n |      Max using level names, as well as indices.\n |      \n |      >>> s.max(level='blooded')\n |      blooded\n |      warm    4\n |      cold    8\n |      Name: legs, dtype: int64\n |      \n |      >>> s.max(level=0)\n |      blooded\n |      warm    4\n |      cold    8\n |      Name: legs, dtype: int64\n |  \n |  mean(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs)\n |      Return the mean of the values for the requested axis.\n |      \n |      Parameters\n |      ----------\n |      axis : {index (0), columns (1)}\n |          Axis for the function to be applied on.\n |      skipna : bool, default True\n |          Exclude NA/null values when computing the result.\n |      level : int or level name, default None\n |          If the axis is a MultiIndex (hierarchical), count along a\n |          particular level, collapsing into a Series.\n |      numeric_only : bool, default None\n |          Include only float, int, boolean columns. If None, will attempt to use\n |          everything, then use only numeric data. Not implemented for Series.\n |      **kwargs\n |          Additional keyword arguments to be passed to the function.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame (if level specified)\n |  \n |  median(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs)\n |      Return the median of the values for the requested axis.\n |      \n |      Parameters\n |      ----------\n |      axis : {index (0), columns (1)}\n |          Axis for the function to be applied on.\n |      skipna : bool, default True\n |          Exclude NA/null values when computing the result.\n |      level : int or level name, default None\n |          If the axis is a MultiIndex (hierarchical), count along a\n |          particular level, collapsing into a Series.\n |      numeric_only : bool, default None\n |          Include only float, int, boolean columns. If None, will attempt to use\n |          everything, then use only numeric data. Not implemented for Series.\n |      **kwargs\n |          Additional keyword arguments to be passed to the function.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame (if level specified)\n |  \n |  melt(self, id_vars=None, value_vars=None, var_name=None, value_name='value', col_level=None)\n |      Unpivot a DataFrame from wide format to long format, optionally\n |      leaving identifier variables set.\n |      \n |      This function is useful to massage a DataFrame into a format where one\n |      or more columns are identifier variables (`id_vars`), while all other\n |      columns, considered measured variables (`value_vars`), are \"unpivoted\" to\n |      the row axis, leaving just two non-identifier columns, 'variable' and\n |      'value'.\n |      .. versionadded:: 0.20.0\n |      \n |      Parameters\n |      ----------\n |      frame : DataFrame\n |      id_vars : tuple, list, or ndarray, optional\n |          Column(s) to use as identifier variables.\n |      value_vars : tuple, list, or ndarray, optional\n |          Column(s) to unpivot. If not specified, uses all columns that\n |          are not set as `id_vars`.\n |      var_name : scalar\n |          Name to use for the 'variable' column. If None it uses\n |          ``frame.columns.name`` or 'variable'.\n |      value_name : scalar, default 'value'\n |          Name to use for the 'value' column.\n |      col_level : int or string, optional\n |          If columns are a MultiIndex then use this level to melt.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          Unpivoted DataFrame.\n |      \n |      See Also\n |      --------\n |      melt\n |      pivot_table\n |      DataFrame.pivot\n |      Series.explode\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c'},\n |      ...                    'B': {0: 1, 1: 3, 2: 5},\n |      ...                    'C': {0: 2, 1: 4, 2: 6}})\n |      >>> df\n |         A  B  C\n |      0  a  1  2\n |      1  b  3  4\n |      2  c  5  6\n |      \n |      >>> df.melt(id_vars=['A'], value_vars=['B'])\n |         A variable  value\n |      0  a        B      1\n |      1  b        B      3\n |      2  c        B      5\n |      \n |      >>> df.melt(id_vars=['A'], value_vars=['B', 'C'])\n |         A variable  value\n |      0  a        B      1\n |      1  b        B      3\n |      2  c        B      5\n |      3  a        C      2\n |      4  b        C      4\n |      5  c        C      6\n |      \n |      The names of 'variable' and 'value' columns can be customized:\n |      \n |      >>> df.melt(id_vars=['A'], value_vars=['B'],\n |      ...         var_name='myVarname', value_name='myValname')\n |         A myVarname  myValname\n |      0  a         B          1\n |      1  b         B          3\n |      2  c         B          5\n |      \n |      If you have multi-index columns:\n |      \n |      >>> df.columns = [list('ABC'), list('DEF')]\n |      >>> df\n |         A  B  C\n |         D  E  F\n |      0  a  1  2\n |      1  b  3  4\n |      2  c  5  6\n |      \n |      >>> df.melt(col_level=0, id_vars=['A'], value_vars=['B'])\n |         A variable  value\n |      0  a        B      1\n |      1  b        B      3\n |      2  c        B      5\n |      \n |      >>> df.melt(id_vars=[('A', 'D')], value_vars=[('B', 'E')])\n |        (A, D) variable_0 variable_1  value\n |      0      a          B          E      1\n |      1      b          B          E      3\n |      2      c          B          E      5\n |  \n |  memory_usage(self, index=True, deep=False)\n |      Return the memory usage of each column in bytes.\n |      \n |      The memory usage can optionally include the contribution of\n |      the index and elements of `object` dtype.\n |      \n |      This value is displayed in `DataFrame.info` by default. This can be\n |      suppressed by setting ``pandas.options.display.memory_usage`` to False.\n |      \n |      Parameters\n |      ----------\n |      index : bool, default True\n |          Specifies whether to include the memory usage of the DataFrame's\n |          index in returned Series. If ``index=True``, the memory usage of\n |          the index is the first item in the output.\n |      deep : bool, default False\n |          If True, introspect the data deeply by interrogating\n |          `object` dtypes for system-level memory consumption, and include\n |          it in the returned values.\n |      \n |      Returns\n |      -------\n |      Series\n |          A Series whose index is the original column names and whose values\n |          is the memory usage of each column in bytes.\n |      \n |      See Also\n |      --------\n |      numpy.ndarray.nbytes : Total bytes consumed by the elements of an\n |          ndarray.\n |      Series.memory_usage : Bytes consumed by a Series.\n |      Categorical : Memory-efficient array for string values with\n |          many repeated values.\n |      DataFrame.info : Concise summary of a DataFrame.\n |      \n |      Examples\n |      --------\n |      >>> dtypes = ['int64', 'float64', 'complex128', 'object', 'bool']\n |      >>> data = dict([(t, np.ones(shape=5000).astype(t))\n |      ...              for t in dtypes])\n |      >>> df = pd.DataFrame(data)\n |      >>> df.head()\n |         int64  float64            complex128  object  bool\n |      0      1      1.0    1.000000+0.000000j       1  True\n |      1      1      1.0    1.000000+0.000000j       1  True\n |      2      1      1.0    1.000000+0.000000j       1  True\n |      3      1      1.0    1.000000+0.000000j       1  True\n |      4      1      1.0    1.000000+0.000000j       1  True\n |      \n |      >>> df.memory_usage()\n |      Index           128\n |      int64         40000\n |      float64       40000\n |      complex128    80000\n |      object        40000\n |      bool           5000\n |      dtype: int64\n |      \n |      >>> df.memory_usage(index=False)\n |      int64         40000\n |      float64       40000\n |      complex128    80000\n |      object        40000\n |      bool           5000\n |      dtype: int64\n |      \n |      The memory footprint of `object` dtype columns is ignored by default:\n |      \n |      >>> df.memory_usage(deep=True)\n |      Index            128\n |      int64          40000\n |      float64        40000\n |      complex128     80000\n |      object        160000\n |      bool            5000\n |      dtype: int64\n |      \n |      Use a Categorical for efficient storage of an object-dtype column with\n |      many repeated values.\n |      \n |      >>> df['object'].astype('category').memory_usage(deep=True)\n |      5216\n |  \n |  merge(self, right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=False, suffixes=('_x', '_y'), copy=True, indicator=False, validate=None)\n |      Merge DataFrame or named Series objects with a database-style join.\n |      \n |      The join is done on columns or indexes. If joining columns on\n |      columns, the DataFrame indexes *will be ignored*. Otherwise if joining indexes\n |      on indexes or indexes on a column or columns, the index will be passed on.\n |      \n |      Parameters\n |      ----------\n |      right : DataFrame or named Series\n |          Object to merge with.\n |      how : {'left', 'right', 'outer', 'inner'}, default 'inner'\n |          Type of merge to be performed.\n |      \n |          * left: use only keys from left frame, similar to a SQL left outer join;\n |            preserve key order.\n |          * right: use only keys from right frame, similar to a SQL right outer join;\n |            preserve key order.\n |          * outer: use union of keys from both frames, similar to a SQL full outer\n |            join; sort keys lexicographically.\n |          * inner: use intersection of keys from both frames, similar to a SQL inner\n |            join; preserve the order of the left keys.\n |      on : label or list\n |          Column or index level names to join on. These must be found in both\n |          DataFrames. If `on` is None and not merging on indexes then this defaults\n |          to the intersection of the columns in both DataFrames.\n |      left_on : label or list, or array-like\n |          Column or index level names to join on in the left DataFrame. Can also\n |          be an array or list of arrays of the length of the left DataFrame.\n |          These arrays are treated as if they are columns.\n |      right_on : label or list, or array-like\n |          Column or index level names to join on in the right DataFrame. Can also\n |          be an array or list of arrays of the length of the right DataFrame.\n |          These arrays are treated as if they are columns.\n |      left_index : bool, default False\n |          Use the index from the left DataFrame as the join key(s). If it is a\n |          MultiIndex, the number of keys in the other DataFrame (either the index\n |          or a number of columns) must match the number of levels.\n |      right_index : bool, default False\n |          Use the index from the right DataFrame as the join key. Same caveats as\n |          left_index.\n |      sort : bool, default False\n |          Sort the join keys lexicographically in the result DataFrame. If False,\n |          the order of the join keys depends on the join type (how keyword).\n |      suffixes : tuple of (str, str), default ('_x', '_y')\n |          Suffix to apply to overlapping column names in the left and right\n |          side, respectively. To raise an exception on overlapping columns use\n |          (False, False).\n |      copy : bool, default True\n |          If False, avoid copy if possible.\n |      indicator : bool or str, default False\n |          If True, adds a column to output DataFrame called \"_merge\" with\n |          information on the source of each row.\n |          If string, column with information on source of each row will be added to\n |          output DataFrame, and column will be named value of string.\n |          Information column is Categorical-type and takes on a value of \"left_only\"\n |          for observations whose merge key only appears in 'left' DataFrame,\n |          \"right_only\" for observations whose merge key only appears in 'right'\n |          DataFrame, and \"both\" if the observation's merge key is found in both.\n |      \n |      validate : str, optional\n |          If specified, checks if merge is of specified type.\n |      \n |          * \"one_to_one\" or \"1:1\": check if merge keys are unique in both\n |            left and right datasets.\n |          * \"one_to_many\" or \"1:m\": check if merge keys are unique in left\n |            dataset.\n |          * \"many_to_one\" or \"m:1\": check if merge keys are unique in right\n |            dataset.\n |          * \"many_to_many\" or \"m:m\": allowed, but does not result in checks.\n |      \n |          .. versionadded:: 0.21.0\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          A DataFrame of the two merged objects.\n |      \n |      See Also\n |      --------\n |      merge_ordered : Merge with optional filling/interpolation.\n |      merge_asof : Merge on nearest keys.\n |      DataFrame.join : Similar method using indices.\n |      \n |      Notes\n |      -----\n |      Support for specifying index levels as the `on`, `left_on`, and\n |      `right_on` parameters was added in version 0.23.0\n |      Support for merging named Series objects was added in version 0.24.0\n |      \n |      Examples\n |      --------\n |      \n |      >>> df1 = pd.DataFrame({'lkey': ['foo', 'bar', 'baz', 'foo'],\n |      ...                     'value': [1, 2, 3, 5]})\n |      >>> df2 = pd.DataFrame({'rkey': ['foo', 'bar', 'baz', 'foo'],\n |      ...                     'value': [5, 6, 7, 8]})\n |      >>> df1\n |          lkey value\n |      0   foo      1\n |      1   bar      2\n |      2   baz      3\n |      3   foo      5\n |      >>> df2\n |          rkey value\n |      0   foo      5\n |      1   bar      6\n |      2   baz      7\n |      3   foo      8\n |      \n |      Merge df1 and df2 on the lkey and rkey columns. The value columns have\n |      the default suffixes, _x and _y, appended.\n |      \n |      >>> df1.merge(df2, left_on='lkey', right_on='rkey')\n |        lkey  value_x rkey  value_y\n |      0  foo        1  foo        5\n |      1  foo        1  foo        8\n |      2  foo        5  foo        5\n |      3  foo        5  foo        8\n |      4  bar        2  bar        6\n |      5  baz        3  baz        7\n |      \n |      Merge DataFrames df1 and df2 with specified left and right suffixes\n |      appended to any overlapping columns.\n |      \n |      >>> df1.merge(df2, left_on='lkey', right_on='rkey',\n |      ...           suffixes=('_left', '_right'))\n |        lkey  value_left rkey  value_right\n |      0  foo           1  foo            5\n |      1  foo           1  foo            8\n |      2  foo           5  foo            5\n |      3  foo           5  foo            8\n |      4  bar           2  bar            6\n |      5  baz           3  baz            7\n |      \n |      Merge DataFrames df1 and df2, but raise an exception if the DataFrames have\n |      any overlapping columns.\n |      \n |      >>> df1.merge(df2, left_on='lkey', right_on='rkey', suffixes=(False, False))\n |      Traceback (most recent call last):\n |      ...\n |      ValueError: columns overlap but no suffix specified:\n |          Index(['value'], dtype='object')\n |  \n |  min(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs)\n |      Return the minimum of the values for the requested axis.\n |      \n |                  If you want the *index* of the minimum, use ``idxmin``. This is\n |                  the equivalent of the ``numpy.ndarray`` method ``argmin``.\n |      \n |      Parameters\n |      ----------\n |      axis : {index (0), columns (1)}\n |          Axis for the function to be applied on.\n |      skipna : bool, default True\n |          Exclude NA/null values when computing the result.\n |      level : int or level name, default None\n |          If the axis is a MultiIndex (hierarchical), count along a\n |          particular level, collapsing into a Series.\n |      numeric_only : bool, default None\n |          Include only float, int, boolean columns. If None, will attempt to use\n |          everything, then use only numeric data. Not implemented for Series.\n |      **kwargs\n |          Additional keyword arguments to be passed to the function.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame (if level specified)\n |      \n |      See Also\n |      --------\n |      Series.sum : Return the sum.\n |      Series.min : Return the minimum.\n |      Series.max : Return the maximum.\n |      Series.idxmin : Return the index of the minimum.\n |      Series.idxmax : Return the index of the maximum.\n |      DataFrame.sum : Return the sum over the requested axis.\n |      DataFrame.min : Return the minimum over the requested axis.\n |      DataFrame.max : Return the maximum over the requested axis.\n |      DataFrame.idxmin : Return the index of the minimum over the requested axis.\n |      DataFrame.idxmax : Return the index of the maximum over the requested axis.\n |      \n |      Examples\n |      --------\n |      >>> idx = pd.MultiIndex.from_arrays([\n |      ...     ['warm', 'warm', 'cold', 'cold'],\n |      ...     ['dog', 'falcon', 'fish', 'spider']],\n |      ...     names=['blooded', 'animal'])\n |      >>> s = pd.Series([4, 2, 0, 8], name='legs', index=idx)\n |      >>> s\n |      blooded  animal\n |      warm     dog       4\n |               falcon    2\n |      cold     fish      0\n |               spider    8\n |      Name: legs, dtype: int64\n |      \n |      >>> s.min()\n |      0\n |      \n |      Min using level names, as well as indices.\n |      \n |      >>> s.min(level='blooded')\n |      blooded\n |      warm    2\n |      cold    0\n |      Name: legs, dtype: int64\n |      \n |      >>> s.min(level=0)\n |      blooded\n |      warm    2\n |      cold    0\n |      Name: legs, dtype: int64\n |  \n |  mod(self, other, axis='columns', level=None, fill_value=None)\n |      Get Modulo of dataframe and other, element-wise (binary operator `mod`).\n |      \n |      Equivalent to ``dataframe % other``, but with support to substitute a fill_value\n |      for missing data in one of the inputs. With reverse version, `rmod`.\n |      \n |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `mod`, `pow`) to\n |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n |      \n |      Parameters\n |      ----------\n |      other : scalar, sequence, Series, or DataFrame\n |          Any single or multiple element data structure, or list-like object.\n |      axis :  {0 or 'index', 1 or 'columns'}\n |          Whether to compare by the index (0 or 'index') or columns\n |          (1 or 'columns'). For Series input, axis to match Series index on.\n |      level : int or label\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level.\n |      fill_value : float or None, default None\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful DataFrame alignment, with this value before computation.\n |          If data in both corresponding DataFrame locations is missing\n |          the result will be missing.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          Result of the arithmetic operation.\n |      \n |      See Also\n |      --------\n |      DataFrame.add : Add DataFrames.\n |      DataFrame.sub : Subtract DataFrames.\n |      DataFrame.mul : Multiply DataFrames.\n |      DataFrame.div : Divide DataFrames (float division).\n |      DataFrame.truediv : Divide DataFrames (float division).\n |      DataFrame.floordiv : Divide DataFrames (integer division).\n |      DataFrame.mod : Calculate modulo (remainder after division).\n |      DataFrame.pow : Calculate exponential power.\n |      \n |      Notes\n |      -----\n |      Mismatched indices will be unioned together.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'angles': [0, 3, 4],\n |      ...                    'degrees': [360, 180, 360]},\n |      ...                   index=['circle', 'triangle', 'rectangle'])\n |      >>> df\n |                 angles  degrees\n |      circle          0      360\n |      triangle        3      180\n |      rectangle       4      360\n |      \n |      Add a scalar with operator version which return the same\n |      results.\n |      \n |      >>> df + 1\n |                 angles  degrees\n |      circle          1      361\n |      triangle        4      181\n |      rectangle       5      361\n |      \n |      >>> df.add(1)\n |                 angles  degrees\n |      circle          1      361\n |      triangle        4      181\n |      rectangle       5      361\n |      \n |      Divide by constant with reverse version.\n |      \n |      >>> df.div(10)\n |                 angles  degrees\n |      circle        0.0     36.0\n |      triangle      0.3     18.0\n |      rectangle     0.4     36.0\n |      \n |      >>> df.rdiv(10)\n |                   angles   degrees\n |      circle          inf  0.027778\n |      triangle   3.333333  0.055556\n |      rectangle  2.500000  0.027778\n |      \n |      Subtract a list and Series by axis with operator version.\n |      \n |      >>> df - [1, 2]\n |                 angles  degrees\n |      circle         -1      358\n |      triangle        2      178\n |      rectangle       3      358\n |      \n |      >>> df.sub([1, 2], axis='columns')\n |                 angles  degrees\n |      circle         -1      358\n |      triangle        2      178\n |      rectangle       3      358\n |      \n |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n |      ...        axis='index')\n |                 angles  degrees\n |      circle         -1      359\n |      triangle        2      179\n |      rectangle       3      359\n |      \n |      Multiply a DataFrame of different shape with operator version.\n |      \n |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},\n |      ...                      index=['circle', 'triangle', 'rectangle'])\n |      >>> other\n |                 angles\n |      circle          0\n |      triangle        3\n |      rectangle       4\n |      \n |      >>> df * other\n |                 angles  degrees\n |      circle          0      NaN\n |      triangle        9      NaN\n |      rectangle      16      NaN\n |      \n |      >>> df.mul(other, fill_value=0)\n |                 angles  degrees\n |      circle          0      0.0\n |      triangle        9      0.0\n |      rectangle      16      0.0\n |      \n |      Divide by a MultiIndex by level.\n |      \n |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n |      ...                                    ['circle', 'triangle', 'rectangle',\n |      ...                                     'square', 'pentagon', 'hexagon']])\n |      >>> df_multindex\n |                   angles  degrees\n |      A circle          0      360\n |        triangle        3      180\n |        rectangle       4      360\n |      B square          4      360\n |        pentagon        5      540\n |        hexagon         6      720\n |      \n |      >>> df.div(df_multindex, level=1, fill_value=0)\n |                   angles  degrees\n |      A circle        NaN      1.0\n |        triangle      1.0      1.0\n |        rectangle     1.0      1.0\n |      B square        0.0      0.0\n |        pentagon      0.0      0.0\n |        hexagon       0.0      0.0\n |  \n |  mode(self, axis=0, numeric_only=False, dropna=True)\n |      Get the mode(s) of each element along the selected axis.\n |      \n |      The mode of a set of values is the value that appears most often.\n |      It can be multiple values.\n |      \n |      Parameters\n |      ----------\n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          The axis to iterate over while searching for the mode:\n |      \n |          * 0 or 'index' : get mode of each column\n |          * 1 or 'columns' : get mode of each row\n |      numeric_only : bool, default False\n |          If True, only apply to numeric columns.\n |      dropna : bool, default True\n |          Don't consider counts of NaN/NaT.\n |      \n |          .. versionadded:: 0.24.0\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          The modes of each column or row.\n |      \n |      See Also\n |      --------\n |      Series.mode : Return the highest frequency value in a Series.\n |      Series.value_counts : Return the counts of values in a Series.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame([('bird', 2, 2),\n |      ...                    ('mammal', 4, np.nan),\n |      ...                    ('arthropod', 8, 0),\n |      ...                    ('bird', 2, np.nan)],\n |      ...                   index=('falcon', 'horse', 'spider', 'ostrich'),\n |      ...                   columns=('species', 'legs', 'wings'))\n |      >>> df\n |                 species  legs  wings\n |      falcon        bird     2    2.0\n |      horse       mammal     4    NaN\n |      spider   arthropod     8    0.0\n |      ostrich       bird     2    NaN\n |      \n |      By default, missing values are not considered, and the mode of wings\n |      are both 0 and 2. The second row of species and legs contains ``NaN``,\n |      because they have only one mode, but the DataFrame has two rows.\n |      \n |      >>> df.mode()\n |        species  legs  wings\n |      0    bird   2.0    0.0\n |      1     NaN   NaN    2.0\n |      \n |      Setting ``dropna=False`` ``NaN`` values are considered and they can be\n |      the mode (like for wings).\n |      \n |      >>> df.mode(dropna=False)\n |        species  legs  wings\n |      0    bird     2    NaN\n |      \n |      Setting ``numeric_only=True``, only the mode of numeric columns is\n |      computed, and columns of other types are ignored.\n |      \n |      >>> df.mode(numeric_only=True)\n |         legs  wings\n |      0   2.0    0.0\n |      1   NaN    2.0\n |      \n |      To compute the mode over columns and not rows, use the axis parameter:\n |      \n |      >>> df.mode(axis='columns', numeric_only=True)\n |                 0    1\n |      falcon   2.0  NaN\n |      horse    4.0  NaN\n |      spider   0.0  8.0\n |      ostrich  2.0  NaN\n |  \n |  mul(self, other, axis='columns', level=None, fill_value=None)\n |      Get Multiplication of dataframe and other, element-wise (binary operator `mul`).\n |      \n |      Equivalent to ``dataframe * other``, but with support to substitute a fill_value\n |      for missing data in one of the inputs. With reverse version, `rmul`.\n |      \n |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `mod`, `pow`) to\n |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n |      \n |      Parameters\n |      ----------\n |      other : scalar, sequence, Series, or DataFrame\n |          Any single or multiple element data structure, or list-like object.\n |      axis :  {0 or 'index', 1 or 'columns'}\n |          Whether to compare by the index (0 or 'index') or columns\n |          (1 or 'columns'). For Series input, axis to match Series index on.\n |      level : int or label\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level.\n |      fill_value : float or None, default None\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful DataFrame alignment, with this value before computation.\n |          If data in both corresponding DataFrame locations is missing\n |          the result will be missing.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          Result of the arithmetic operation.\n |      \n |      See Also\n |      --------\n |      DataFrame.add : Add DataFrames.\n |      DataFrame.sub : Subtract DataFrames.\n |      DataFrame.mul : Multiply DataFrames.\n |      DataFrame.div : Divide DataFrames (float division).\n |      DataFrame.truediv : Divide DataFrames (float division).\n |      DataFrame.floordiv : Divide DataFrames (integer division).\n |      DataFrame.mod : Calculate modulo (remainder after division).\n |      DataFrame.pow : Calculate exponential power.\n |      \n |      Notes\n |      -----\n |      Mismatched indices will be unioned together.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'angles': [0, 3, 4],\n |      ...                    'degrees': [360, 180, 360]},\n |      ...                   index=['circle', 'triangle', 'rectangle'])\n |      >>> df\n |                 angles  degrees\n |      circle          0      360\n |      triangle        3      180\n |      rectangle       4      360\n |      \n |      Add a scalar with operator version which return the same\n |      results.\n |      \n |      >>> df + 1\n |                 angles  degrees\n |      circle          1      361\n |      triangle        4      181\n |      rectangle       5      361\n |      \n |      >>> df.add(1)\n |                 angles  degrees\n |      circle          1      361\n |      triangle        4      181\n |      rectangle       5      361\n |      \n |      Divide by constant with reverse version.\n |      \n |      >>> df.div(10)\n |                 angles  degrees\n |      circle        0.0     36.0\n |      triangle      0.3     18.0\n |      rectangle     0.4     36.0\n |      \n |      >>> df.rdiv(10)\n |                   angles   degrees\n |      circle          inf  0.027778\n |      triangle   3.333333  0.055556\n |      rectangle  2.500000  0.027778\n |      \n |      Subtract a list and Series by axis with operator version.\n |      \n |      >>> df - [1, 2]\n |                 angles  degrees\n |      circle         -1      358\n |      triangle        2      178\n |      rectangle       3      358\n |      \n |      >>> df.sub([1, 2], axis='columns')\n |                 angles  degrees\n |      circle         -1      358\n |      triangle        2      178\n |      rectangle       3      358\n |      \n |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n |      ...        axis='index')\n |                 angles  degrees\n |      circle         -1      359\n |      triangle        2      179\n |      rectangle       3      359\n |      \n |      Multiply a DataFrame of different shape with operator version.\n |      \n |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},\n |      ...                      index=['circle', 'triangle', 'rectangle'])\n |      >>> other\n |                 angles\n |      circle          0\n |      triangle        3\n |      rectangle       4\n |      \n |      >>> df * other\n |                 angles  degrees\n |      circle          0      NaN\n |      triangle        9      NaN\n |      rectangle      16      NaN\n |      \n |      >>> df.mul(other, fill_value=0)\n |                 angles  degrees\n |      circle          0      0.0\n |      triangle        9      0.0\n |      rectangle      16      0.0\n |      \n |      Divide by a MultiIndex by level.\n |      \n |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n |      ...                                    ['circle', 'triangle', 'rectangle',\n |      ...                                     'square', 'pentagon', 'hexagon']])\n |      >>> df_multindex\n |                   angles  degrees\n |      A circle          0      360\n |        triangle        3      180\n |        rectangle       4      360\n |      B square          4      360\n |        pentagon        5      540\n |        hexagon         6      720\n |      \n |      >>> df.div(df_multindex, level=1, fill_value=0)\n |                   angles  degrees\n |      A circle        NaN      1.0\n |        triangle      1.0      1.0\n |        rectangle     1.0      1.0\n |      B square        0.0      0.0\n |        pentagon      0.0      0.0\n |        hexagon       0.0      0.0\n |  \n |  multiply = mul(self, other, axis='columns', level=None, fill_value=None)\n |  \n |  ne(self, other, axis='columns', level=None)\n |      Get Not equal to of dataframe and other, element-wise (binary operator `ne`).\n |      \n |      Among flexible wrappers (`eq`, `ne`, `le`, `lt`, `ge`, `gt`) to comparison\n |      operators.\n |      \n |      Equivalent to `==`, `=!`, `<=`, `<`, `>=`, `>` with support to choose axis\n |      (rows or columns) and level for comparison.\n |      \n |      Parameters\n |      ----------\n |      other : scalar, sequence, Series, or DataFrame\n |          Any single or multiple element data structure, or list-like object.\n |      axis :  {0 or 'index', 1 or 'columns'}, default 'columns'\n |          Whether to compare by the index (0 or 'index') or columns\n |          (1 or 'columns').\n |      level : int or label\n |          Broadcast across a level, matching Index values on the passed\n |          MultiIndex level.\n |      \n |      Returns\n |      -------\n |      DataFrame of bool\n |          Result of the comparison.\n |      \n |      See Also\n |      --------\n |      DataFrame.eq : Compare DataFrames for equality elementwise.\n |      DataFrame.ne : Compare DataFrames for inequality elementwise.\n |      DataFrame.le : Compare DataFrames for less than inequality\n |          or equality elementwise.\n |      DataFrame.lt : Compare DataFrames for strictly less than\n |          inequality elementwise.\n |      DataFrame.ge : Compare DataFrames for greater than inequality\n |          or equality elementwise.\n |      DataFrame.gt : Compare DataFrames for strictly greater than\n |          inequality elementwise.\n |      \n |      Notes\n |      -----\n |      Mismatched indices will be unioned together.\n |      `NaN` values are considered different (i.e. `NaN` != `NaN`).\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'cost': [250, 150, 100],\n |      ...                    'revenue': [100, 250, 300]},\n |      ...                   index=['A', 'B', 'C'])\n |      >>> df\n |         cost  revenue\n |      A   250      100\n |      B   150      250\n |      C   100      300\n |      \n |      Comparison with a scalar, using either the operator or method:\n |      \n |      >>> df == 100\n |          cost  revenue\n |      A  False     True\n |      B  False    False\n |      C   True    False\n |      \n |      >>> df.eq(100)\n |          cost  revenue\n |      A  False     True\n |      B  False    False\n |      C   True    False\n |      \n |      When `other` is a :class:`Series`, the columns of a DataFrame are aligned\n |      with the index of `other` and broadcast:\n |      \n |      >>> df != pd.Series([100, 250], index=[\"cost\", \"revenue\"])\n |          cost  revenue\n |      A   True     True\n |      B   True    False\n |      C  False     True\n |      \n |      Use the method to control the broadcast axis:\n |      \n |      >>> df.ne(pd.Series([100, 300], index=[\"A\", \"D\"]), axis='index')\n |         cost  revenue\n |      A  True    False\n |      B  True     True\n |      C  True     True\n |      D  True     True\n |      \n |      When comparing to an arbitrary sequence, the number of columns must\n |      match the number elements in `other`:\n |      \n |      >>> df == [250, 100]\n |          cost  revenue\n |      A   True     True\n |      B  False    False\n |      C  False    False\n |      \n |      Use the method to control the axis:\n |      \n |      >>> df.eq([250, 250, 100], axis='index')\n |          cost  revenue\n |      A   True    False\n |      B  False     True\n |      C   True    False\n |      \n |      Compare to a DataFrame of different shape.\n |      \n |      >>> other = pd.DataFrame({'revenue': [300, 250, 100, 150]},\n |      ...                      index=['A', 'B', 'C', 'D'])\n |      >>> other\n |         revenue\n |      A      300\n |      B      250\n |      C      100\n |      D      150\n |      \n |      >>> df.gt(other)\n |          cost  revenue\n |      A  False    False\n |      B  False    False\n |      C  False     True\n |      D  False    False\n |      \n |      Compare to a MultiIndex by level.\n |      \n |      >>> df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220],\n |      ...                              'revenue': [100, 250, 300, 200, 175, 225]},\n |      ...                             index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'],\n |      ...                                    ['A', 'B', 'C', 'A', 'B', 'C']])\n |      >>> df_multindex\n |            cost  revenue\n |      Q1 A   250      100\n |         B   150      250\n |         C   100      300\n |      Q2 A   150      200\n |         B   300      175\n |         C   220      225\n |      \n |      >>> df.le(df_multindex, level=1)\n |             cost  revenue\n |      Q1 A   True     True\n |         B   True     True\n |         C   True     True\n |      Q2 A  False     True\n |         B   True    False\n |         C   True    False\n |  \n |  nlargest(self, n, columns, keep='first')\n |      Return the first `n` rows ordered by `columns` in descending order.\n |      \n |      Return the first `n` rows with the largest values in `columns`, in\n |      descending order. The columns that are not specified are returned as\n |      well, but not used for ordering.\n |      \n |      This method is equivalent to\n |      ``df.sort_values(columns, ascending=False).head(n)``, but more\n |      performant.\n |      \n |      Parameters\n |      ----------\n |      n : int\n |          Number of rows to return.\n |      columns : label or list of labels\n |          Column label(s) to order by.\n |      keep : {'first', 'last', 'all'}, default 'first'\n |          Where there are duplicate values:\n |      \n |          - `first` : prioritize the first occurrence(s)\n |          - `last` : prioritize the last occurrence(s)\n |          - ``all`` : do not drop any duplicates, even it means\n |                      selecting more than `n` items.\n |      \n |          .. versionadded:: 0.24.0\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          The first `n` rows ordered by the given columns in descending\n |          order.\n |      \n |      See Also\n |      --------\n |      DataFrame.nsmallest : Return the first `n` rows ordered by `columns` in\n |          ascending order.\n |      DataFrame.sort_values : Sort DataFrame by the values.\n |      DataFrame.head : Return the first `n` rows without re-ordering.\n |      \n |      Notes\n |      -----\n |      This function cannot be used with all column types. For example, when\n |      specifying columns with `object` or `category` dtypes, ``TypeError`` is\n |      raised.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'population': [59000000, 65000000, 434000,\n |      ...                                   434000, 434000, 337000, 11300,\n |      ...                                   11300, 11300],\n |      ...                    'GDP': [1937894, 2583560 , 12011, 4520, 12128,\n |      ...                            17036, 182, 38, 311],\n |      ...                    'alpha-2': [\"IT\", \"FR\", \"MT\", \"MV\", \"BN\",\n |      ...                                \"IS\", \"NR\", \"TV\", \"AI\"]},\n |      ...                   index=[\"Italy\", \"France\", \"Malta\",\n |      ...                          \"Maldives\", \"Brunei\", \"Iceland\",\n |      ...                          \"Nauru\", \"Tuvalu\", \"Anguilla\"])\n |      >>> df\n |                population      GDP alpha-2\n |      Italy       59000000  1937894      IT\n |      France      65000000  2583560      FR\n |      Malta         434000    12011      MT\n |      Maldives      434000     4520      MV\n |      Brunei        434000    12128      BN\n |      Iceland       337000    17036      IS\n |      Nauru          11300      182      NR\n |      Tuvalu         11300       38      TV\n |      Anguilla       11300      311      AI\n |      \n |      In the following example, we will use ``nlargest`` to select the three\n |      rows having the largest values in column \"population\".\n |      \n |      >>> df.nlargest(3, 'population')\n |              population      GDP alpha-2\n |      France    65000000  2583560      FR\n |      Italy     59000000  1937894      IT\n |      Malta       434000    12011      MT\n |      \n |      When using ``keep='last'``, ties are resolved in reverse order:\n |      \n |      >>> df.nlargest(3, 'population', keep='last')\n |              population      GDP alpha-2\n |      France    65000000  2583560      FR\n |      Italy     59000000  1937894      IT\n |      Brunei      434000    12128      BN\n |      \n |      When using ``keep='all'``, all duplicate items are maintained:\n |      \n |      >>> df.nlargest(3, 'population', keep='all')\n |                population      GDP alpha-2\n |      France      65000000  2583560      FR\n |      Italy       59000000  1937894      IT\n |      Malta         434000    12011      MT\n |      Maldives      434000     4520      MV\n |      Brunei        434000    12128      BN\n |      \n |      To order by the largest values in column \"population\" and then \"GDP\",\n |      we can specify multiple columns like in the next example.\n |      \n |      >>> df.nlargest(3, ['population', 'GDP'])\n |              population      GDP alpha-2\n |      France    65000000  2583560      FR\n |      Italy     59000000  1937894      IT\n |      Brunei      434000    12128      BN\n |  \n |  notna(self)\n |      Detect existing (non-missing) values.\n |      \n |      Return a boolean same-sized object indicating if the values are not NA.\n |      Non-missing values get mapped to True. Characters such as empty\n |      strings ``''`` or :attr:`numpy.inf` are not considered NA values\n |      (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n |      NA values, such as None or :attr:`numpy.NaN`, get mapped to False\n |      values.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          Mask of bool values for each element in DataFrame that\n |          indicates whether an element is not an NA value.\n |      \n |      See Also\n |      --------\n |      DataFrame.notnull : Alias of notna.\n |      DataFrame.isna : Boolean inverse of notna.\n |      DataFrame.dropna : Omit axes labels with missing values.\n |      notna : Top-level notna.\n |      \n |      Examples\n |      --------\n |      Show which entries in a DataFrame are not NA.\n |      \n |      >>> df = pd.DataFrame({'age': [5, 6, np.NaN],\n |      ...                    'born': [pd.NaT, pd.Timestamp('1939-05-27'),\n |      ...                             pd.Timestamp('1940-04-25')],\n |      ...                    'name': ['Alfred', 'Batman', ''],\n |      ...                    'toy': [None, 'Batmobile', 'Joker']})\n |      >>> df\n |         age       born    name        toy\n |      0  5.0        NaT  Alfred       None\n |      1  6.0 1939-05-27  Batman  Batmobile\n |      2  NaN 1940-04-25              Joker\n |      \n |      >>> df.notna()\n |           age   born  name    toy\n |      0   True  False  True  False\n |      1   True   True  True   True\n |      2  False   True  True   True\n |      \n |      Show which entries in a Series are not NA.\n |      \n |      >>> ser = pd.Series([5, 6, np.NaN])\n |      >>> ser\n |      0    5.0\n |      1    6.0\n |      2    NaN\n |      dtype: float64\n |      \n |      >>> ser.notna()\n |      0     True\n |      1     True\n |      2    False\n |      dtype: bool\n |  \n |  notnull(self)\n |      Detect existing (non-missing) values.\n |      \n |      Return a boolean same-sized object indicating if the values are not NA.\n |      Non-missing values get mapped to True. Characters such as empty\n |      strings ``''`` or :attr:`numpy.inf` are not considered NA values\n |      (unless you set ``pandas.options.mode.use_inf_as_na = True``).\n |      NA values, such as None or :attr:`numpy.NaN`, get mapped to False\n |      values.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          Mask of bool values for each element in DataFrame that\n |          indicates whether an element is not an NA value.\n |      \n |      See Also\n |      --------\n |      DataFrame.notnull : Alias of notna.\n |      DataFrame.isna : Boolean inverse of notna.\n |      DataFrame.dropna : Omit axes labels with missing values.\n |      notna : Top-level notna.\n |      \n |      Examples\n |      --------\n |      Show which entries in a DataFrame are not NA.\n |      \n |      >>> df = pd.DataFrame({'age': [5, 6, np.NaN],\n |      ...                    'born': [pd.NaT, pd.Timestamp('1939-05-27'),\n |      ...                             pd.Timestamp('1940-04-25')],\n |      ...                    'name': ['Alfred', 'Batman', ''],\n |      ...                    'toy': [None, 'Batmobile', 'Joker']})\n |      >>> df\n |         age       born    name        toy\n |      0  5.0        NaT  Alfred       None\n |      1  6.0 1939-05-27  Batman  Batmobile\n |      2  NaN 1940-04-25              Joker\n |      \n |      >>> df.notna()\n |           age   born  name    toy\n |      0   True  False  True  False\n |      1   True   True  True   True\n |      2  False   True  True   True\n |      \n |      Show which entries in a Series are not NA.\n |      \n |      >>> ser = pd.Series([5, 6, np.NaN])\n |      >>> ser\n |      0    5.0\n |      1    6.0\n |      2    NaN\n |      dtype: float64\n |      \n |      >>> ser.notna()\n |      0     True\n |      1     True\n |      2    False\n |      dtype: bool\n |  \n |  nsmallest(self, n, columns, keep='first')\n |      Return the first `n` rows ordered by `columns` in ascending order.\n |      \n |      Return the first `n` rows with the smallest values in `columns`, in\n |      ascending order. The columns that are not specified are returned as\n |      well, but not used for ordering.\n |      \n |      This method is equivalent to\n |      ``df.sort_values(columns, ascending=True).head(n)``, but more\n |      performant.\n |      \n |      Parameters\n |      ----------\n |      n : int\n |          Number of items to retrieve.\n |      columns : list or str\n |          Column name or names to order by.\n |      keep : {'first', 'last', 'all'}, default 'first'\n |          Where there are duplicate values:\n |      \n |          - ``first`` : take the first occurrence.\n |          - ``last`` : take the last occurrence.\n |          - ``all`` : do not drop any duplicates, even it means\n |            selecting more than `n` items.\n |      \n |          .. versionadded:: 0.24.0\n |      \n |      Returns\n |      -------\n |      DataFrame\n |      \n |      See Also\n |      --------\n |      DataFrame.nlargest : Return the first `n` rows ordered by `columns` in\n |          descending order.\n |      DataFrame.sort_values : Sort DataFrame by the values.\n |      DataFrame.head : Return the first `n` rows without re-ordering.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'population': [59000000, 65000000, 434000,\n |      ...                                   434000, 434000, 337000, 11300,\n |      ...                                   11300, 11300],\n |      ...                    'GDP': [1937894, 2583560 , 12011, 4520, 12128,\n |      ...                            17036, 182, 38, 311],\n |      ...                    'alpha-2': [\"IT\", \"FR\", \"MT\", \"MV\", \"BN\",\n |      ...                                \"IS\", \"NR\", \"TV\", \"AI\"]},\n |      ...                   index=[\"Italy\", \"France\", \"Malta\",\n |      ...                          \"Maldives\", \"Brunei\", \"Iceland\",\n |      ...                          \"Nauru\", \"Tuvalu\", \"Anguilla\"])\n |      >>> df\n |                population      GDP alpha-2\n |      Italy       59000000  1937894      IT\n |      France      65000000  2583560      FR\n |      Malta         434000    12011      MT\n |      Maldives      434000     4520      MV\n |      Brunei        434000    12128      BN\n |      Iceland       337000    17036      IS\n |      Nauru          11300      182      NR\n |      Tuvalu         11300       38      TV\n |      Anguilla       11300      311      AI\n |      \n |      In the following example, we will use ``nsmallest`` to select the\n |      three rows having the smallest values in column \"a\".\n |      \n |      >>> df.nsmallest(3, 'population')\n |                population  GDP alpha-2\n |      Nauru          11300  182      NR\n |      Tuvalu         11300   38      TV\n |      Anguilla       11300  311      AI\n |      \n |      When using ``keep='last'``, ties are resolved in reverse order:\n |      \n |      >>> df.nsmallest(3, 'population', keep='last')\n |                population  GDP alpha-2\n |      Anguilla       11300  311      AI\n |      Tuvalu         11300   38      TV\n |      Nauru          11300  182      NR\n |      \n |      When using ``keep='all'``, all duplicate items are maintained:\n |      \n |      >>> df.nsmallest(3, 'population', keep='all')\n |                population  GDP alpha-2\n |      Nauru          11300  182      NR\n |      Tuvalu         11300   38      TV\n |      Anguilla       11300  311      AI\n |      \n |      To order by the largest values in column \"a\" and then \"c\", we can\n |      specify multiple columns like in the next example.\n |      \n |      >>> df.nsmallest(3, ['population', 'GDP'])\n |                population  GDP alpha-2\n |      Tuvalu         11300   38      TV\n |      Nauru          11300  182      NR\n |      Anguilla       11300  311      AI\n |  \n |  nunique(self, axis=0, dropna=True)\n |      Count distinct observations over requested axis.\n |      \n |      Return Series with number of distinct observations. Can ignore NaN\n |      values.\n |      \n |      .. versionadded:: 0.20.0\n |      \n |      Parameters\n |      ----------\n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for\n |          column-wise.\n |      dropna : bool, default True\n |          Don't include NaN in the counts.\n |      \n |      Returns\n |      -------\n |      Series\n |      \n |      See Also\n |      --------\n |      Series.nunique: Method nunique for Series.\n |      DataFrame.count: Count non-NA cells for each column or row.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [1, 1, 1]})\n |      >>> df.nunique()\n |      A    3\n |      B    1\n |      dtype: int64\n |      \n |      >>> df.nunique(axis=1)\n |      0    1\n |      1    2\n |      2    2\n |      dtype: int64\n |  \n |  pivot(self, index=None, columns=None, values=None)\n |      Return reshaped DataFrame organized by given index / column values.\n |      \n |      Reshape data (produce a \"pivot\" table) based on column values. Uses\n |      unique values from specified `index` / `columns` to form axes of the\n |      resulting DataFrame. This function does not support data\n |      aggregation, multiple values will result in a MultiIndex in the\n |      columns. See the :ref:`User Guide <reshaping>` for more on reshaping.\n |      \n |      Parameters\n |      ----------\n |      index : string or object, optional\n |          Column to use to make new frame's index. If None, uses\n |          existing index.\n |      columns : string or object\n |          Column to use to make new frame's columns.\n |      values : string, object or a list of the previous, optional\n |          Column(s) to use for populating new frame's values. If not\n |          specified, all remaining columns will be used and the result will\n |          have hierarchically indexed columns.\n |      \n |          .. versionchanged :: 0.23.0\n |             Also accept list of column names.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          Returns reshaped DataFrame.\n |      \n |      Raises\n |      ------\n |      ValueError:\n |          When there are any `index`, `columns` combinations with multiple\n |          values. `DataFrame.pivot_table` when you need to aggregate.\n |      \n |      See Also\n |      --------\n |      DataFrame.pivot_table : Generalization of pivot that can handle\n |          duplicate values for one index/column pair.\n |      DataFrame.unstack : Pivot based on the index values instead of a\n |          column.\n |      \n |      Notes\n |      -----\n |      For finer-tuned control, see hierarchical indexing documentation along\n |      with the related stack/unstack methods.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'foo': ['one', 'one', 'one', 'two', 'two',\n |      ...                            'two'],\n |      ...                    'bar': ['A', 'B', 'C', 'A', 'B', 'C'],\n |      ...                    'baz': [1, 2, 3, 4, 5, 6],\n |      ...                    'zoo': ['x', 'y', 'z', 'q', 'w', 't']})\n |      >>> df\n |          foo   bar  baz  zoo\n |      0   one   A    1    x\n |      1   one   B    2    y\n |      2   one   C    3    z\n |      3   two   A    4    q\n |      4   two   B    5    w\n |      5   two   C    6    t\n |      \n |      >>> df.pivot(index='foo', columns='bar', values='baz')\n |      bar  A   B   C\n |      foo\n |      one  1   2   3\n |      two  4   5   6\n |      \n |      >>> df.pivot(index='foo', columns='bar')['baz']\n |      bar  A   B   C\n |      foo\n |      one  1   2   3\n |      two  4   5   6\n |      \n |      >>> df.pivot(index='foo', columns='bar', values=['baz', 'zoo'])\n |            baz       zoo\n |      bar   A  B  C   A  B  C\n |      foo\n |      one   1  2  3   x  y  z\n |      two   4  5  6   q  w  t\n |      \n |      A ValueError is raised if there are any duplicates.\n |      \n |      >>> df = pd.DataFrame({\"foo\": ['one', 'one', 'two', 'two'],\n |      ...                    \"bar\": ['A', 'A', 'B', 'C'],\n |      ...                    \"baz\": [1, 2, 3, 4]})\n |      >>> df\n |         foo bar  baz\n |      0  one   A    1\n |      1  one   A    2\n |      2  two   B    3\n |      3  two   C    4\n |      \n |      Notice that the first two rows are the same for our `index`\n |      and `columns` arguments.\n |      \n |      >>> df.pivot(index='foo', columns='bar', values='baz')\n |      Traceback (most recent call last):\n |         ...\n |      ValueError: Index contains duplicate entries, cannot reshape\n |  \n |  pivot_table(self, values=None, index=None, columns=None, aggfunc='mean', fill_value=None, margins=False, dropna=True, margins_name='All', observed=False)\n |      Create a spreadsheet-style pivot table as a DataFrame. The levels in\n |      the pivot table will be stored in MultiIndex objects (hierarchical\n |      indexes) on the index and columns of the result DataFrame.\n |      \n |      Parameters\n |      ----------\n |      values : column to aggregate, optional\n |      index : column, Grouper, array, or list of the previous\n |          If an array is passed, it must be the same length as the data. The\n |          list can contain any of the other types (except list).\n |          Keys to group by on the pivot table index.  If an array is passed,\n |          it is being used as the same manner as column values.\n |      columns : column, Grouper, array, or list of the previous\n |          If an array is passed, it must be the same length as the data. The\n |          list can contain any of the other types (except list).\n |          Keys to group by on the pivot table column.  If an array is passed,\n |          it is being used as the same manner as column values.\n |      aggfunc : function, list of functions, dict, default numpy.mean\n |          If list of functions passed, the resulting pivot table will have\n |          hierarchical columns whose top level are the function names\n |          (inferred from the function objects themselves)\n |          If dict is passed, the key is column to aggregate and value\n |          is function or list of functions\n |      fill_value : scalar, default None\n |          Value to replace missing values with\n |      margins : boolean, default False\n |          Add all row / columns (e.g. for subtotal / grand totals)\n |      dropna : boolean, default True\n |          Do not include columns whose entries are all NaN\n |      margins_name : string, default 'All'\n |          Name of the row / column that will contain the totals\n |          when margins is True.\n |      observed : boolean, default False\n |          This only applies if any of the groupers are Categoricals.\n |          If True: only show observed values for categorical groupers.\n |          If False: show all values for categorical groupers.\n |      \n |          .. versionchanged :: 0.25.0\n |      \n |      Returns\n |      -------\n |      DataFrame\n |      \n |      See Also\n |      --------\n |      DataFrame.pivot : Pivot without aggregation that can handle\n |          non-numeric data.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({\"A\": [\"foo\", \"foo\", \"foo\", \"foo\", \"foo\",\n |      ...                          \"bar\", \"bar\", \"bar\", \"bar\"],\n |      ...                    \"B\": [\"one\", \"one\", \"one\", \"two\", \"two\",\n |      ...                          \"one\", \"one\", \"two\", \"two\"],\n |      ...                    \"C\": [\"small\", \"large\", \"large\", \"small\",\n |      ...                          \"small\", \"large\", \"small\", \"small\",\n |      ...                          \"large\"],\n |      ...                    \"D\": [1, 2, 2, 3, 3, 4, 5, 6, 7],\n |      ...                    \"E\": [2, 4, 5, 5, 6, 6, 8, 9, 9]})\n |      >>> df\n |           A    B      C  D  E\n |      0  foo  one  small  1  2\n |      1  foo  one  large  2  4\n |      2  foo  one  large  2  5\n |      3  foo  two  small  3  5\n |      4  foo  two  small  3  6\n |      5  bar  one  large  4  6\n |      6  bar  one  small  5  8\n |      7  bar  two  small  6  9\n |      8  bar  two  large  7  9\n |      \n |      This first example aggregates values by taking the sum.\n |      \n |      >>> table = pd.pivot_table(df, values='D', index=['A', 'B'],\n |      ...                     columns=['C'], aggfunc=np.sum)\n |      >>> table\n |      C        large  small\n |      A   B\n |      bar one    4.0    5.0\n |          two    7.0    6.0\n |      foo one    4.0    1.0\n |          two    NaN    6.0\n |      \n |      We can also fill missing values using the `fill_value` parameter.\n |      \n |      >>> table = pd.pivot_table(df, values='D', index=['A', 'B'],\n |      ...                     columns=['C'], aggfunc=np.sum, fill_value=0)\n |      >>> table\n |      C        large  small\n |      A   B\n |      bar one      4      5\n |          two      7      6\n |      foo one      4      1\n |          two      0      6\n |      \n |      The next example aggregates by taking the mean across multiple columns.\n |      \n |      >>> table = pd.pivot_table(df, values=['D', 'E'], index=['A', 'C'],\n |      ...                     aggfunc={'D': np.mean,\n |      ...                              'E': np.mean})\n |      >>> table\n |                      D         E\n |      A   C\n |      bar large  5.500000  7.500000\n |          small  5.500000  8.500000\n |      foo large  2.000000  4.500000\n |          small  2.333333  4.333333\n |      \n |      We can also calculate multiple types of aggregations for any given\n |      value column.\n |      \n |      >>> table = pd.pivot_table(df, values=['D', 'E'], index=['A', 'C'],\n |      ...                     aggfunc={'D': np.mean,\n |      ...                              'E': [min, max, np.mean]})\n |      >>> table\n |                      D    E\n |                  mean  max      mean  min\n |      A   C\n |      bar large  5.500000  9.0  7.500000  6.0\n |          small  5.500000  9.0  8.500000  8.0\n |      foo large  2.000000  5.0  4.500000  4.0\n |          small  2.333333  6.0  4.333333  2.0\n |  \n |  pow(self, other, axis='columns', level=None, fill_value=None)\n |      Get Exponential power of dataframe and other, element-wise (binary operator `pow`).\n |      \n |      Equivalent to ``dataframe ** other``, but with support to substitute a fill_value\n |      for missing data in one of the inputs. With reverse version, `rpow`.\n |      \n |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `mod`, `pow`) to\n |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n |      \n |      Parameters\n |      ----------\n |      other : scalar, sequence, Series, or DataFrame\n |          Any single or multiple element data structure, or list-like object.\n |      axis :  {0 or 'index', 1 or 'columns'}\n |          Whether to compare by the index (0 or 'index') or columns\n |          (1 or 'columns'). For Series input, axis to match Series index on.\n |      level : int or label\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level.\n |      fill_value : float or None, default None\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful DataFrame alignment, with this value before computation.\n |          If data in both corresponding DataFrame locations is missing\n |          the result will be missing.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          Result of the arithmetic operation.\n |      \n |      See Also\n |      --------\n |      DataFrame.add : Add DataFrames.\n |      DataFrame.sub : Subtract DataFrames.\n |      DataFrame.mul : Multiply DataFrames.\n |      DataFrame.div : Divide DataFrames (float division).\n |      DataFrame.truediv : Divide DataFrames (float division).\n |      DataFrame.floordiv : Divide DataFrames (integer division).\n |      DataFrame.mod : Calculate modulo (remainder after division).\n |      DataFrame.pow : Calculate exponential power.\n |      \n |      Notes\n |      -----\n |      Mismatched indices will be unioned together.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'angles': [0, 3, 4],\n |      ...                    'degrees': [360, 180, 360]},\n |      ...                   index=['circle', 'triangle', 'rectangle'])\n |      >>> df\n |                 angles  degrees\n |      circle          0      360\n |      triangle        3      180\n |      rectangle       4      360\n |      \n |      Add a scalar with operator version which return the same\n |      results.\n |      \n |      >>> df + 1\n |                 angles  degrees\n |      circle          1      361\n |      triangle        4      181\n |      rectangle       5      361\n |      \n |      >>> df.add(1)\n |                 angles  degrees\n |      circle          1      361\n |      triangle        4      181\n |      rectangle       5      361\n |      \n |      Divide by constant with reverse version.\n |      \n |      >>> df.div(10)\n |                 angles  degrees\n |      circle        0.0     36.0\n |      triangle      0.3     18.0\n |      rectangle     0.4     36.0\n |      \n |      >>> df.rdiv(10)\n |                   angles   degrees\n |      circle          inf  0.027778\n |      triangle   3.333333  0.055556\n |      rectangle  2.500000  0.027778\n |      \n |      Subtract a list and Series by axis with operator version.\n |      \n |      >>> df - [1, 2]\n |                 angles  degrees\n |      circle         -1      358\n |      triangle        2      178\n |      rectangle       3      358\n |      \n |      >>> df.sub([1, 2], axis='columns')\n |                 angles  degrees\n |      circle         -1      358\n |      triangle        2      178\n |      rectangle       3      358\n |      \n |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n |      ...        axis='index')\n |                 angles  degrees\n |      circle         -1      359\n |      triangle        2      179\n |      rectangle       3      359\n |      \n |      Multiply a DataFrame of different shape with operator version.\n |      \n |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},\n |      ...                      index=['circle', 'triangle', 'rectangle'])\n |      >>> other\n |                 angles\n |      circle          0\n |      triangle        3\n |      rectangle       4\n |      \n |      >>> df * other\n |                 angles  degrees\n |      circle          0      NaN\n |      triangle        9      NaN\n |      rectangle      16      NaN\n |      \n |      >>> df.mul(other, fill_value=0)\n |                 angles  degrees\n |      circle          0      0.0\n |      triangle        9      0.0\n |      rectangle      16      0.0\n |      \n |      Divide by a MultiIndex by level.\n |      \n |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n |      ...                                    ['circle', 'triangle', 'rectangle',\n |      ...                                     'square', 'pentagon', 'hexagon']])\n |      >>> df_multindex\n |                   angles  degrees\n |      A circle          0      360\n |        triangle        3      180\n |        rectangle       4      360\n |      B square          4      360\n |        pentagon        5      540\n |        hexagon         6      720\n |      \n |      >>> df.div(df_multindex, level=1, fill_value=0)\n |                   angles  degrees\n |      A circle        NaN      1.0\n |        triangle      1.0      1.0\n |        rectangle     1.0      1.0\n |      B square        0.0      0.0\n |        pentagon      0.0      0.0\n |        hexagon       0.0      0.0\n |  \n |  prod(self, axis=None, skipna=None, level=None, numeric_only=None, min_count=0, **kwargs)\n |      Return the product of the values for the requested axis.\n |      \n |      Parameters\n |      ----------\n |      axis : {index (0), columns (1)}\n |          Axis for the function to be applied on.\n |      skipna : bool, default True\n |          Exclude NA/null values when computing the result.\n |      level : int or level name, default None\n |          If the axis is a MultiIndex (hierarchical), count along a\n |          particular level, collapsing into a Series.\n |      numeric_only : bool, default None\n |          Include only float, int, boolean columns. If None, will attempt to use\n |          everything, then use only numeric data. Not implemented for Series.\n |      min_count : int, default 0\n |          The required number of valid values to perform the operation. If fewer than\n |          ``min_count`` non-NA values are present the result will be NA.\n |      \n |          .. versionadded :: 0.22.0\n |      \n |             Added with the default being 0. This means the sum of an all-NA\n |             or empty Series is 0, and the product of an all-NA or empty\n |             Series is 1.\n |      **kwargs\n |          Additional keyword arguments to be passed to the function.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame (if level specified)\n |      \n |      Examples\n |      --------\n |      By default, the product of an empty or all-NA Series is ``1``\n |      \n |      >>> pd.Series([]).prod()\n |      1.0\n |      \n |      This can be controlled with the ``min_count`` parameter\n |      \n |      >>> pd.Series([]).prod(min_count=1)\n |      nan\n |      \n |      Thanks to the ``skipna`` parameter, ``min_count`` handles all-NA and\n |      empty series identically.\n |      \n |      >>> pd.Series([np.nan]).prod()\n |      1.0\n |      \n |      >>> pd.Series([np.nan]).prod(min_count=1)\n |      nan\n |  \n |  product = prod(self, axis=None, skipna=None, level=None, numeric_only=None, min_count=0, **kwargs)\n |  \n |  quantile(self, q=0.5, axis=0, numeric_only=True, interpolation='linear')\n |      Return values at the given quantile over requested axis.\n |      \n |      Parameters\n |      ----------\n |      q : float or array-like, default 0.5 (50% quantile)\n |          Value between 0 <= q <= 1, the quantile(s) to compute.\n |      axis : {0, 1, 'index', 'columns'} (default 0)\n |          Equals 0 or 'index' for row-wise, 1 or 'columns' for column-wise.\n |      numeric_only : bool, default True\n |          If False, the quantile of datetime and timedelta data will be\n |          computed as well.\n |      interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n |          This optional parameter specifies the interpolation method to use,\n |          when the desired quantile lies between two data points `i` and `j`:\n |      \n |          * linear: `i + (j - i) * fraction`, where `fraction` is the\n |            fractional part of the index surrounded by `i` and `j`.\n |          * lower: `i`.\n |          * higher: `j`.\n |          * nearest: `i` or `j` whichever is nearest.\n |          * midpoint: (`i` + `j`) / 2.\n |      \n |          .. versionadded:: 0.18.0\n |      \n |      Returns\n |      -------\n |      Series or DataFrame\n |      \n |          If ``q`` is an array, a DataFrame will be returned where the\n |            index is ``q``, the columns are the columns of self, and the\n |            values are the quantiles.\n |          If ``q`` is a float, a Series will be returned where the\n |            index is the columns of self and the values are the quantiles.\n |      \n |      See Also\n |      --------\n |      core.window.Rolling.quantile: Rolling quantile.\n |      numpy.percentile: Numpy function to compute the percentile.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame(np.array([[1, 1], [2, 10], [3, 100], [4, 100]]),\n |      ...                   columns=['a', 'b'])\n |      >>> df.quantile(.1)\n |      a    1.3\n |      b    3.7\n |      Name: 0.1, dtype: float64\n |      >>> df.quantile([.1, .5])\n |             a     b\n |      0.1  1.3   3.7\n |      0.5  2.5  55.0\n |      \n |      Specifying `numeric_only=False` will also compute the quantile of\n |      datetime and timedelta data.\n |      \n |      >>> df = pd.DataFrame({'A': [1, 2],\n |      ...                    'B': [pd.Timestamp('2010'),\n |      ...                          pd.Timestamp('2011')],\n |      ...                    'C': [pd.Timedelta('1 days'),\n |      ...                          pd.Timedelta('2 days')]})\n |      >>> df.quantile(0.5, numeric_only=False)\n |      A                    1.5\n |      B    2010-07-02 12:00:00\n |      C        1 days 12:00:00\n |      Name: 0.5, dtype: object\n |  \n |  query(self, expr, inplace=False, **kwargs)\n |      Query the columns of a DataFrame with a boolean expression.\n |      \n |      Parameters\n |      ----------\n |      expr : str\n |          The query string to evaluate.  You can refer to variables\n |          in the environment by prefixing them with an '@' character like\n |          ``@a + b``.\n |      \n |          .. versionadded:: 0.25.0\n |      \n |          You can refer to column names that contain spaces by surrounding\n |          them in backticks.\n |      \n |          For example, if one of your columns is called ``a a`` and you want\n |          to sum it with ``b``, your query should be ```a a` + b``.\n |      \n |      inplace : bool\n |          Whether the query should modify the data in place or return\n |          a modified copy.\n |      **kwargs\n |          See the documentation for :func:`eval` for complete details\n |          on the keyword arguments accepted by :meth:`DataFrame.query`.\n |      \n |          .. versionadded:: 0.18.0\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          DataFrame resulting from the provided query expression.\n |      \n |      See Also\n |      --------\n |      eval : Evaluate a string describing operations on\n |          DataFrame columns.\n |      DataFrame.eval : Evaluate a string describing operations on\n |          DataFrame columns.\n |      \n |      Notes\n |      -----\n |      The result of the evaluation of this expression is first passed to\n |      :attr:`DataFrame.loc` and if that fails because of a\n |      multidimensional key (e.g., a DataFrame) then the result will be passed\n |      to :meth:`DataFrame.__getitem__`.\n |      \n |      This method uses the top-level :func:`eval` function to\n |      evaluate the passed query.\n |      \n |      The :meth:`~pandas.DataFrame.query` method uses a slightly\n |      modified Python syntax by default. For example, the ``&`` and ``|``\n |      (bitwise) operators have the precedence of their boolean cousins,\n |      :keyword:`and` and :keyword:`or`. This *is* syntactically valid Python,\n |      however the semantics are different.\n |      \n |      You can change the semantics of the expression by passing the keyword\n |      argument ``parser='python'``. This enforces the same semantics as\n |      evaluation in Python space. Likewise, you can pass ``engine='python'``\n |      to evaluate an expression using Python itself as a backend. This is not\n |      recommended as it is inefficient compared to using ``numexpr`` as the\n |      engine.\n |      \n |      The :attr:`DataFrame.index` and\n |      :attr:`DataFrame.columns` attributes of the\n |      :class:`~pandas.DataFrame` instance are placed in the query namespace\n |      by default, which allows you to treat both the index and columns of the\n |      frame as a column in the frame.\n |      The identifier ``index`` is used for the frame index; you can also\n |      use the name of the index to identify it in a query. Please note that\n |      Python keywords may not be used as identifiers.\n |      \n |      For further details and examples see the ``query`` documentation in\n |      :ref:`indexing <indexing.query>`.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'A': range(1, 6),\n |      ...                    'B': range(10, 0, -2),\n |      ...                    'C C': range(10, 5, -1)})\n |      >>> df\n |         A   B  C C\n |      0  1  10   10\n |      1  2   8    9\n |      2  3   6    8\n |      3  4   4    7\n |      4  5   2    6\n |      >>> df.query('A > B')\n |         A  B  C C\n |      4  5  2    6\n |      \n |      The previous expression is equivalent to\n |      \n |      >>> df[df.A > df.B]\n |         A  B  C C\n |      4  5  2    6\n |      \n |      For columns with spaces in their name, you can use backtick quoting.\n |      \n |      >>> df.query('B == `C C`')\n |         A   B  C C\n |      0  1  10   10\n |      \n |      The previous expression is equivalent to\n |      \n |      >>> df[df.B == df['C C']]\n |         A   B  C C\n |      0  1  10   10\n |  \n |  radd(self, other, axis='columns', level=None, fill_value=None)\n |      Get Addition of dataframe and other, element-wise (binary operator `radd`).\n |      \n |      Equivalent to ``other + dataframe``, but with support to substitute a fill_value\n |      for missing data in one of the inputs. With reverse version, `add`.\n |      \n |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `mod`, `pow`) to\n |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n |      \n |      Parameters\n |      ----------\n |      other : scalar, sequence, Series, or DataFrame\n |          Any single or multiple element data structure, or list-like object.\n |      axis :  {0 or 'index', 1 or 'columns'}\n |          Whether to compare by the index (0 or 'index') or columns\n |          (1 or 'columns'). For Series input, axis to match Series index on.\n |      level : int or label\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level.\n |      fill_value : float or None, default None\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful DataFrame alignment, with this value before computation.\n |          If data in both corresponding DataFrame locations is missing\n |          the result will be missing.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          Result of the arithmetic operation.\n |      \n |      See Also\n |      --------\n |      DataFrame.add : Add DataFrames.\n |      DataFrame.sub : Subtract DataFrames.\n |      DataFrame.mul : Multiply DataFrames.\n |      DataFrame.div : Divide DataFrames (float division).\n |      DataFrame.truediv : Divide DataFrames (float division).\n |      DataFrame.floordiv : Divide DataFrames (integer division).\n |      DataFrame.mod : Calculate modulo (remainder after division).\n |      DataFrame.pow : Calculate exponential power.\n |      \n |      Notes\n |      -----\n |      Mismatched indices will be unioned together.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'angles': [0, 3, 4],\n |      ...                    'degrees': [360, 180, 360]},\n |      ...                   index=['circle', 'triangle', 'rectangle'])\n |      >>> df\n |                 angles  degrees\n |      circle          0      360\n |      triangle        3      180\n |      rectangle       4      360\n |      \n |      Add a scalar with operator version which return the same\n |      results.\n |      \n |      >>> df + 1\n |                 angles  degrees\n |      circle          1      361\n |      triangle        4      181\n |      rectangle       5      361\n |      \n |      >>> df.add(1)\n |                 angles  degrees\n |      circle          1      361\n |      triangle        4      181\n |      rectangle       5      361\n |      \n |      Divide by constant with reverse version.\n |      \n |      >>> df.div(10)\n |                 angles  degrees\n |      circle        0.0     36.0\n |      triangle      0.3     18.0\n |      rectangle     0.4     36.0\n |      \n |      >>> df.rdiv(10)\n |                   angles   degrees\n |      circle          inf  0.027778\n |      triangle   3.333333  0.055556\n |      rectangle  2.500000  0.027778\n |      \n |      Subtract a list and Series by axis with operator version.\n |      \n |      >>> df - [1, 2]\n |                 angles  degrees\n |      circle         -1      358\n |      triangle        2      178\n |      rectangle       3      358\n |      \n |      >>> df.sub([1, 2], axis='columns')\n |                 angles  degrees\n |      circle         -1      358\n |      triangle        2      178\n |      rectangle       3      358\n |      \n |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n |      ...        axis='index')\n |                 angles  degrees\n |      circle         -1      359\n |      triangle        2      179\n |      rectangle       3      359\n |      \n |      Multiply a DataFrame of different shape with operator version.\n |      \n |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},\n |      ...                      index=['circle', 'triangle', 'rectangle'])\n |      >>> other\n |                 angles\n |      circle          0\n |      triangle        3\n |      rectangle       4\n |      \n |      >>> df * other\n |                 angles  degrees\n |      circle          0      NaN\n |      triangle        9      NaN\n |      rectangle      16      NaN\n |      \n |      >>> df.mul(other, fill_value=0)\n |                 angles  degrees\n |      circle          0      0.0\n |      triangle        9      0.0\n |      rectangle      16      0.0\n |      \n |      Divide by a MultiIndex by level.\n |      \n |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n |      ...                                    ['circle', 'triangle', 'rectangle',\n |      ...                                     'square', 'pentagon', 'hexagon']])\n |      >>> df_multindex\n |                   angles  degrees\n |      A circle          0      360\n |        triangle        3      180\n |        rectangle       4      360\n |      B square          4      360\n |        pentagon        5      540\n |        hexagon         6      720\n |      \n |      >>> df.div(df_multindex, level=1, fill_value=0)\n |                   angles  degrees\n |      A circle        NaN      1.0\n |        triangle      1.0      1.0\n |        rectangle     1.0      1.0\n |      B square        0.0      0.0\n |        pentagon      0.0      0.0\n |        hexagon       0.0      0.0\n |  \n |  rdiv = rtruediv(self, other, axis='columns', level=None, fill_value=None)\n |  \n |  reindex(self, labels=None, index=None, columns=None, axis=None, method=None, copy=True, level=None, fill_value=nan, limit=None, tolerance=None)\n |      Conform DataFrame to new index with optional filling logic, placing\n |      NA/NaN in locations having no value in the previous index. A new object\n |      is produced unless the new index is equivalent to the current one and\n |      ``copy=False``.\n |      \n |      Parameters\n |      ----------\n |      labels : array-like, optional\n |                  New labels / index to conform the axis specified by 'axis' to.\n |      index, columns : array-like, optional\n |          New labels / index to conform to, should be specified using\n |          keywords. Preferably an Index object to avoid duplicating data\n |      axis : int or str, optional\n |                  Axis to target. Can be either the axis name ('index', 'columns')\n |                  or number (0, 1).\n |      method : {None, 'backfill'/'bfill', 'pad'/'ffill', 'nearest'}\n |          Method to use for filling holes in reindexed DataFrame.\n |          Please note: this is only applicable to DataFrames/Series with a\n |          monotonically increasing/decreasing index.\n |      \n |          * None (default): don't fill gaps\n |          * pad / ffill: propagate last valid observation forward to next\n |            valid\n |          * backfill / bfill: use next valid observation to fill gap\n |          * nearest: use nearest valid observations to fill gap\n |      \n |      copy : bool, default True\n |          Return a new object, even if the passed indexes are the same.\n |      level : int or name\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level.\n |      fill_value : scalar, default np.NaN\n |          Value to use for missing values. Defaults to NaN, but can be any\n |          \"compatible\" value.\n |      limit : int, default None\n |          Maximum number of consecutive elements to forward or backward fill.\n |      tolerance : optional\n |          Maximum distance between original and new labels for inexact\n |          matches. The values of the index at the matching locations most\n |          satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n |      \n |          Tolerance may be a scalar value, which applies the same tolerance\n |          to all values, or list-like, which applies variable tolerance per\n |          element. List-like includes list, tuple, array, Series, and must be\n |          the same size as the index and its dtype must exactly match the\n |          index's type.\n |      \n |          .. versionadded:: 0.21.0 (list-like tolerance)\n |      \n |      Returns\n |      -------\n |      DataFrame with changed index.\n |      \n |      See Also\n |      --------\n |      DataFrame.set_index : Set row labels.\n |      DataFrame.reset_index : Remove row labels or move them to new columns.\n |      DataFrame.reindex_like : Change to same indices as other DataFrame.\n |      \n |      Examples\n |      --------\n |      \n |      ``DataFrame.reindex`` supports two calling conventions\n |      \n |      * ``(index=index_labels, columns=column_labels, ...)``\n |      * ``(labels, axis={'index', 'columns'}, ...)``\n |      \n |      We *highly* recommend using keyword arguments to clarify your\n |      intent.\n |      \n |      Create a dataframe with some fictional data.\n |      \n |      >>> index = ['Firefox', 'Chrome', 'Safari', 'IE10', 'Konqueror']\n |      >>> df = pd.DataFrame({\n |      ...      'http_status': [200,200,404,404,301],\n |      ...      'response_time': [0.04, 0.02, 0.07, 0.08, 1.0]},\n |      ...       index=index)\n |      >>> df\n |                 http_status  response_time\n |      Firefox            200           0.04\n |      Chrome             200           0.02\n |      Safari             404           0.07\n |      IE10               404           0.08\n |      Konqueror          301           1.00\n |      \n |      Create a new index and reindex the dataframe. By default\n |      values in the new index that do not have corresponding\n |      records in the dataframe are assigned ``NaN``.\n |      \n |      >>> new_index= ['Safari', 'Iceweasel', 'Comodo Dragon', 'IE10',\n |      ...             'Chrome']\n |      >>> df.reindex(new_index)\n |                     http_status  response_time\n |      Safari               404.0           0.07\n |      Iceweasel              NaN            NaN\n |      Comodo Dragon          NaN            NaN\n |      IE10                 404.0           0.08\n |      Chrome               200.0           0.02\n |      \n |      We can fill in the missing values by passing a value to\n |      the keyword ``fill_value``. Because the index is not monotonically\n |      increasing or decreasing, we cannot use arguments to the keyword\n |      ``method`` to fill the ``NaN`` values.\n |      \n |      >>> df.reindex(new_index, fill_value=0)\n |                     http_status  response_time\n |      Safari                 404           0.07\n |      Iceweasel                0           0.00\n |      Comodo Dragon            0           0.00\n |      IE10                   404           0.08\n |      Chrome                 200           0.02\n |      \n |      >>> df.reindex(new_index, fill_value='missing')\n |                    http_status response_time\n |      Safari                404          0.07\n |      Iceweasel         missing       missing\n |      Comodo Dragon     missing       missing\n |      IE10                  404          0.08\n |      Chrome                200          0.02\n |      \n |      We can also reindex the columns.\n |      \n |      >>> df.reindex(columns=['http_status', 'user_agent'])\n |                 http_status  user_agent\n |      Firefox            200         NaN\n |      Chrome             200         NaN\n |      Safari             404         NaN\n |      IE10               404         NaN\n |      Konqueror          301         NaN\n |      \n |      Or we can use \"axis-style\" keyword arguments\n |      \n |      >>> df.reindex(['http_status', 'user_agent'], axis=\"columns\")\n |                 http_status  user_agent\n |      Firefox            200         NaN\n |      Chrome             200         NaN\n |      Safari             404         NaN\n |      IE10               404         NaN\n |      Konqueror          301         NaN\n |      \n |      To further illustrate the filling functionality in\n |      ``reindex``, we will create a dataframe with a\n |      monotonically increasing index (for example, a sequence\n |      of dates).\n |      \n |      >>> date_index = pd.date_range('1/1/2010', periods=6, freq='D')\n |      >>> df2 = pd.DataFrame({\"prices\": [100, 101, np.nan, 100, 89, 88]},\n |      ...                    index=date_index)\n |      >>> df2\n |                  prices\n |      2010-01-01   100.0\n |      2010-01-02   101.0\n |      2010-01-03     NaN\n |      2010-01-04   100.0\n |      2010-01-05    89.0\n |      2010-01-06    88.0\n |      \n |      Suppose we decide to expand the dataframe to cover a wider\n |      date range.\n |      \n |      >>> date_index2 = pd.date_range('12/29/2009', periods=10, freq='D')\n |      >>> df2.reindex(date_index2)\n |                  prices\n |      2009-12-29     NaN\n |      2009-12-30     NaN\n |      2009-12-31     NaN\n |      2010-01-01   100.0\n |      2010-01-02   101.0\n |      2010-01-03     NaN\n |      2010-01-04   100.0\n |      2010-01-05    89.0\n |      2010-01-06    88.0\n |      2010-01-07     NaN\n |      \n |      The index entries that did not have a value in the original data frame\n |      (for example, '2009-12-29') are by default filled with ``NaN``.\n |      If desired, we can fill in the missing values using one of several\n |      options.\n |      \n |      For example, to back-propagate the last valid value to fill the ``NaN``\n |      values, pass ``bfill`` as an argument to the ``method`` keyword.\n |      \n |      >>> df2.reindex(date_index2, method='bfill')\n |                  prices\n |      2009-12-29   100.0\n |      2009-12-30   100.0\n |      2009-12-31   100.0\n |      2010-01-01   100.0\n |      2010-01-02   101.0\n |      2010-01-03     NaN\n |      2010-01-04   100.0\n |      2010-01-05    89.0\n |      2010-01-06    88.0\n |      2010-01-07     NaN\n |      \n |      Please note that the ``NaN`` value present in the original dataframe\n |      (at index value 2010-01-03) will not be filled by any of the\n |      value propagation schemes. This is because filling while reindexing\n |      does not look at dataframe values, but only compares the original and\n |      desired indexes. If you do want to fill in the ``NaN`` values present\n |      in the original dataframe, use the ``fillna()`` method.\n |      \n |      See the :ref:`user guide <basics.reindexing>` for more.\n |  \n |  rename(self, mapper=None, index=None, columns=None, axis=None, copy=True, inplace=False, level=None, errors='ignore')\n |      Alter axes labels.\n |      \n |      Function / dict values must be unique (1-to-1). Labels not contained in\n |      a dict / Series will be left as-is. Extra labels listed don't throw an\n |      error.\n |      \n |      See the :ref:`user guide <basics.rename>` for more.\n |      \n |      Parameters\n |      ----------\n |      mapper : dict-like or function\n |          Dict-like or functions transformations to apply to\n |          that axis' values. Use either ``mapper`` and ``axis`` to\n |          specify the axis to target with ``mapper``, or ``index`` and\n |          ``columns``.\n |      index : dict-like or function\n |          Alternative to specifying axis (``mapper, axis=0``\n |          is equivalent to ``index=mapper``).\n |      columns : dict-like or function\n |          Alternative to specifying axis (``mapper, axis=1``\n |          is equivalent to ``columns=mapper``).\n |      axis : int or str\n |          Axis to target with ``mapper``. Can be either the axis name\n |          ('index', 'columns') or number (0, 1). The default is 'index'.\n |      copy : bool, default True\n |          Also copy underlying data.\n |      inplace : bool, default False\n |          Whether to return a new DataFrame. If True then value of copy is\n |          ignored.\n |      level : int or level name, default None\n |          In case of a MultiIndex, only rename labels in the specified\n |          level.\n |      errors : {'ignore', 'raise'}, default 'ignore'\n |          If 'raise', raise a `KeyError` when a dict-like `mapper`, `index`,\n |          or `columns` contains labels that are not present in the Index\n |          being transformed.\n |          If 'ignore', existing keys will be renamed and extra keys will be\n |          ignored.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          DataFrame with the renamed axis labels.\n |      \n |      Raises\n |      ------\n |      KeyError\n |          If any of the labels is not found in the selected axis and\n |          \"errors='raise'\".\n |      \n |      See Also\n |      --------\n |      DataFrame.rename_axis : Set the name of the axis.\n |      \n |      Examples\n |      --------\n |      \n |      ``DataFrame.rename`` supports two calling conventions\n |      \n |      * ``(index=index_mapper, columns=columns_mapper, ...)``\n |      * ``(mapper, axis={'index', 'columns'}, ...)``\n |      \n |      We *highly* recommend using keyword arguments to clarify your\n |      intent.\n |      \n |      Rename columns using a mapping:\n |      \n |      >>> df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n |      >>> df.rename(columns={\"A\": \"a\", \"B\": \"c\"})\n |         a  c\n |      0  1  4\n |      1  2  5\n |      2  3  6\n |      \n |      Rename index using a mapping:\n |      \n |      >>> df.rename(index={0: \"x\", 1: \"y\", 2: \"z\"})\n |         A  B\n |      x  1  4\n |      y  2  5\n |      z  3  6\n |      \n |      Cast index labels to a different type:\n |      \n |      >>> df.index\n |      RangeIndex(start=0, stop=3, step=1)\n |      >>> df.rename(index=str).index\n |      Index(['0', '1', '2'], dtype='object')\n |      \n |      >>> df.rename(columns={\"A\": \"a\", \"B\": \"b\", \"C\": \"c\"}, errors=\"raise\")\n |      Traceback (most recent call last):\n |      KeyError: ['C'] not found in axis\n |      \n |      Using axis-style parameters\n |      \n |      >>> df.rename(str.lower, axis='columns')\n |         a  b\n |      0  1  4\n |      1  2  5\n |      2  3  6\n |      \n |      >>> df.rename({1: 2, 2: 4}, axis='index')\n |         A  B\n |      0  1  4\n |      2  2  5\n |      4  3  6\n |  \n |  reorder_levels(self, order, axis=0)\n |      Rearrange index levels using input order. May not drop or\n |      duplicate levels.\n |      \n |      Parameters\n |      ----------\n |      order : list of int or list of str\n |          List representing new level order. Reference level by number\n |          (position) or by key (label).\n |      axis : int\n |          Where to reorder levels.\n |      \n |      Returns\n |      -------\n |      type of caller (new object)\n |  \n |  replace(self, to_replace=None, value=None, inplace=False, limit=None, regex=False, method='pad')\n |      Replace values given in `to_replace` with `value`.\n |      \n |      Values of the DataFrame are replaced with other values dynamically.\n |      This differs from updating with ``.loc`` or ``.iloc``, which require\n |      you to specify a location to update with some value.\n |      \n |      Parameters\n |      ----------\n |      to_replace : str, regex, list, dict, Series, int, float, or None\n |          How to find the values that will be replaced.\n |      \n |          * numeric, str or regex:\n |      \n |              - numeric: numeric values equal to `to_replace` will be\n |                replaced with `value`\n |              - str: string exactly matching `to_replace` will be replaced\n |                with `value`\n |              - regex: regexs matching `to_replace` will be replaced with\n |                `value`\n |      \n |          * list of str, regex, or numeric:\n |      \n |              - First, if `to_replace` and `value` are both lists, they\n |                **must** be the same length.\n |              - Second, if ``regex=True`` then all of the strings in **both**\n |                lists will be interpreted as regexs otherwise they will match\n |                directly. This doesn't matter much for `value` since there\n |                are only a few possible substitution regexes you can use.\n |              - str, regex and numeric rules apply as above.\n |      \n |          * dict:\n |      \n |              - Dicts can be used to specify different replacement values\n |                for different existing values. For example,\n |                ``{'a': 'b', 'y': 'z'}`` replaces the value 'a' with 'b' and\n |                'y' with 'z'. To use a dict in this way the `value`\n |                parameter should be `None`.\n |              - For a DataFrame a dict can specify that different values\n |                should be replaced in different columns. For example,\n |                ``{'a': 1, 'b': 'z'}`` looks for the value 1 in column 'a'\n |                and the value 'z' in column 'b' and replaces these values\n |                with whatever is specified in `value`. The `value` parameter\n |                should not be ``None`` in this case. You can treat this as a\n |                special case of passing two lists except that you are\n |                specifying the column to search in.\n |              - For a DataFrame nested dictionaries, e.g.,\n |                ``{'a': {'b': np.nan}}``, are read as follows: look in column\n |                'a' for the value 'b' and replace it with NaN. The `value`\n |                parameter should be ``None`` to use a nested dict in this\n |                way. You can nest regular expressions as well. Note that\n |                column names (the top-level dictionary keys in a nested\n |                dictionary) **cannot** be regular expressions.\n |      \n |          * None:\n |      \n |              - This means that the `regex` argument must be a string,\n |                compiled regular expression, or list, dict, ndarray or\n |                Series of such elements. If `value` is also ``None`` then\n |                this **must** be a nested dictionary or Series.\n |      \n |          See the examples section for examples of each of these.\n |      value : scalar, dict, list, str, regex, default None\n |          Value to replace any values matching `to_replace` with.\n |          For a DataFrame a dict of values can be used to specify which\n |          value to use for each column (columns not in the dict will not be\n |          filled). Regular expressions, strings and lists or dicts of such\n |          objects are also allowed.\n |      inplace : bool, default False\n |          If True, in place. Note: this will modify any\n |          other views on this object (e.g. a column from a DataFrame).\n |          Returns the caller if this is True.\n |      limit : int, default None\n |          Maximum size gap to forward or backward fill.\n |      regex : bool or same types as `to_replace`, default False\n |          Whether to interpret `to_replace` and/or `value` as regular\n |          expressions. If this is ``True`` then `to_replace` *must* be a\n |          string. Alternatively, this could be a regular expression or a\n |          list, dict, or array of regular expressions in which case\n |          `to_replace` must be ``None``.\n |      method : {'pad', 'ffill', 'bfill', `None`}\n |          The method to use when for replacement, when `to_replace` is a\n |          scalar, list or tuple and `value` is ``None``.\n |      \n |          .. versionchanged:: 0.23.0\n |              Added to DataFrame.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          Object after replacement.\n |      \n |      Raises\n |      ------\n |      AssertionError\n |          * If `regex` is not a ``bool`` and `to_replace` is not\n |            ``None``.\n |      TypeError\n |          * If `to_replace` is a ``dict`` and `value` is not a ``list``,\n |            ``dict``, ``ndarray``, or ``Series``\n |          * If `to_replace` is ``None`` and `regex` is not compilable\n |            into a regular expression or is a list, dict, ndarray, or\n |            Series.\n |          * When replacing multiple ``bool`` or ``datetime64`` objects and\n |            the arguments to `to_replace` does not match the type of the\n |            value being replaced\n |      ValueError\n |          * If a ``list`` or an ``ndarray`` is passed to `to_replace` and\n |            `value` but they are not the same length.\n |      \n |      See Also\n |      --------\n |      DataFrame.fillna : Fill NA values.\n |      DataFrame.where : Replace values based on boolean condition.\n |      Series.str.replace : Simple string replacement.\n |      \n |      Notes\n |      -----\n |      * Regex substitution is performed under the hood with ``re.sub``. The\n |        rules for substitution for ``re.sub`` are the same.\n |      * Regular expressions will only substitute on strings, meaning you\n |        cannot provide, for example, a regular expression matching floating\n |        point numbers and expect the columns in your frame that have a\n |        numeric dtype to be matched. However, if those floating point\n |        numbers *are* strings, then you can do this.\n |      * This method has *a lot* of options. You are encouraged to experiment\n |        and play with this method to gain intuition about how it works.\n |      * When dict is used as the `to_replace` value, it is like\n |        key(s) in the dict are the to_replace part and\n |        value(s) in the dict are the value parameter.\n |      \n |      Examples\n |      --------\n |      \n |      **Scalar `to_replace` and `value`**\n |      \n |      >>> s = pd.Series([0, 1, 2, 3, 4])\n |      >>> s.replace(0, 5)\n |      0    5\n |      1    1\n |      2    2\n |      3    3\n |      4    4\n |      dtype: int64\n |      \n |      >>> df = pd.DataFrame({'A': [0, 1, 2, 3, 4],\n |      ...                    'B': [5, 6, 7, 8, 9],\n |      ...                    'C': ['a', 'b', 'c', 'd', 'e']})\n |      >>> df.replace(0, 5)\n |         A  B  C\n |      0  5  5  a\n |      1  1  6  b\n |      2  2  7  c\n |      3  3  8  d\n |      4  4  9  e\n |      \n |      **List-like `to_replace`**\n |      \n |      >>> df.replace([0, 1, 2, 3], 4)\n |         A  B  C\n |      0  4  5  a\n |      1  4  6  b\n |      2  4  7  c\n |      3  4  8  d\n |      4  4  9  e\n |      \n |      >>> df.replace([0, 1, 2, 3], [4, 3, 2, 1])\n |         A  B  C\n |      0  4  5  a\n |      1  3  6  b\n |      2  2  7  c\n |      3  1  8  d\n |      4  4  9  e\n |      \n |      >>> s.replace([1, 2], method='bfill')\n |      0    0\n |      1    3\n |      2    3\n |      3    3\n |      4    4\n |      dtype: int64\n |      \n |      **dict-like `to_replace`**\n |      \n |      >>> df.replace({0: 10, 1: 100})\n |           A  B  C\n |      0   10  5  a\n |      1  100  6  b\n |      2    2  7  c\n |      3    3  8  d\n |      4    4  9  e\n |      \n |      >>> df.replace({'A': 0, 'B': 5}, 100)\n |           A    B  C\n |      0  100  100  a\n |      1    1    6  b\n |      2    2    7  c\n |      3    3    8  d\n |      4    4    9  e\n |      \n |      >>> df.replace({'A': {0: 100, 4: 400}})\n |           A  B  C\n |      0  100  5  a\n |      1    1  6  b\n |      2    2  7  c\n |      3    3  8  d\n |      4  400  9  e\n |      \n |      **Regular expression `to_replace`**\n |      \n |      >>> df = pd.DataFrame({'A': ['bat', 'foo', 'bait'],\n |      ...                    'B': ['abc', 'bar', 'xyz']})\n |      >>> df.replace(to_replace=r'^ba.$', value='new', regex=True)\n |            A    B\n |      0   new  abc\n |      1   foo  new\n |      2  bait  xyz\n |      \n |      >>> df.replace({'A': r'^ba.$'}, {'A': 'new'}, regex=True)\n |            A    B\n |      0   new  abc\n |      1   foo  bar\n |      2  bait  xyz\n |      \n |      >>> df.replace(regex=r'^ba.$', value='new')\n |            A    B\n |      0   new  abc\n |      1   foo  new\n |      2  bait  xyz\n |      \n |      >>> df.replace(regex={r'^ba.$': 'new', 'foo': 'xyz'})\n |            A    B\n |      0   new  abc\n |      1   xyz  new\n |      2  bait  xyz\n |      \n |      >>> df.replace(regex=[r'^ba.$', 'foo'], value='new')\n |            A    B\n |      0   new  abc\n |      1   new  new\n |      2  bait  xyz\n |      \n |      Note that when replacing multiple ``bool`` or ``datetime64`` objects,\n |      the data types in the `to_replace` parameter must match the data\n |      type of the value being replaced:\n |      \n |      >>> df = pd.DataFrame({'A': [True, False, True],\n |      ...                    'B': [False, True, False]})\n |      >>> df.replace({'a string': 'new value', True: False})  # raises\n |      Traceback (most recent call last):\n |          ...\n |      TypeError: Cannot compare types 'ndarray(dtype=bool)' and 'str'\n |      \n |      This raises a ``TypeError`` because one of the ``dict`` keys is not of\n |      the correct type for replacement.\n |      \n |      Compare the behavior of ``s.replace({'a': None})`` and\n |      ``s.replace('a', None)`` to understand the peculiarities\n |      of the `to_replace` parameter:\n |      \n |      >>> s = pd.Series([10, 'a', 'a', 'b', 'a'])\n |      \n |      When one uses a dict as the `to_replace` value, it is like the\n |      value(s) in the dict are equal to the `value` parameter.\n |      ``s.replace({'a': None})`` is equivalent to\n |      ``s.replace(to_replace={'a': None}, value=None, method=None)``:\n |      \n |      >>> s.replace({'a': None})\n |      0      10\n |      1    None\n |      2    None\n |      3       b\n |      4    None\n |      dtype: object\n |      \n |      When ``value=None`` and `to_replace` is a scalar, list or\n |      tuple, `replace` uses the method parameter (default 'pad') to do the\n |      replacement. So this is why the 'a' values are being replaced by 10\n |      in rows 1 and 2 and 'b' in row 4 in this case.\n |      The command ``s.replace('a', None)`` is actually equivalent to\n |      ``s.replace(to_replace='a', value=None, method='pad')``:\n |      \n |      >>> s.replace('a', None)\n |      0    10\n |      1    10\n |      2    10\n |      3     b\n |      4     b\n |      dtype: object\n |  \n |  reset_index(self, level=None, drop=False, inplace=False, col_level=0, col_fill='')\n |      Reset the index, or a level of it.\n |      \n |      Reset the index of the DataFrame, and use the default one instead.\n |      If the DataFrame has a MultiIndex, this method can remove one or more\n |      levels.\n |      \n |      Parameters\n |      ----------\n |      level : int, str, tuple, or list, default None\n |          Only remove the given levels from the index. Removes all levels by\n |          default.\n |      drop : bool, default False\n |          Do not try to insert index into dataframe columns. This resets\n |          the index to the default integer index.\n |      inplace : bool, default False\n |          Modify the DataFrame in place (do not create a new object).\n |      col_level : int or str, default 0\n |          If the columns have multiple levels, determines which level the\n |          labels are inserted into. By default it is inserted into the first\n |          level.\n |      col_fill : object, default ''\n |          If the columns have multiple levels, determines how the other\n |          levels are named. If None then the index name is repeated.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          DataFrame with the new index.\n |      \n |      See Also\n |      --------\n |      DataFrame.set_index : Opposite of reset_index.\n |      DataFrame.reindex : Change to new indices or expand indices.\n |      DataFrame.reindex_like : Change to same indices as other DataFrame.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame([('bird', 389.0),\n |      ...                    ('bird', 24.0),\n |      ...                    ('mammal', 80.5),\n |      ...                    ('mammal', np.nan)],\n |      ...                   index=['falcon', 'parrot', 'lion', 'monkey'],\n |      ...                   columns=('class', 'max_speed'))\n |      >>> df\n |               class  max_speed\n |      falcon    bird      389.0\n |      parrot    bird       24.0\n |      lion    mammal       80.5\n |      monkey  mammal        NaN\n |      \n |      When we reset the index, the old index is added as a column, and a\n |      new sequential index is used:\n |      \n |      >>> df.reset_index()\n |          index   class  max_speed\n |      0  falcon    bird      389.0\n |      1  parrot    bird       24.0\n |      2    lion  mammal       80.5\n |      3  monkey  mammal        NaN\n |      \n |      We can use the `drop` parameter to avoid the old index being added as\n |      a column:\n |      \n |      >>> df.reset_index(drop=True)\n |          class  max_speed\n |      0    bird      389.0\n |      1    bird       24.0\n |      2  mammal       80.5\n |      3  mammal        NaN\n |      \n |      You can also use `reset_index` with `MultiIndex`.\n |      \n |      >>> index = pd.MultiIndex.from_tuples([('bird', 'falcon'),\n |      ...                                    ('bird', 'parrot'),\n |      ...                                    ('mammal', 'lion'),\n |      ...                                    ('mammal', 'monkey')],\n |      ...                                   names=['class', 'name'])\n |      >>> columns = pd.MultiIndex.from_tuples([('speed', 'max'),\n |      ...                                      ('species', 'type')])\n |      >>> df = pd.DataFrame([(389.0, 'fly'),\n |      ...                    ( 24.0, 'fly'),\n |      ...                    ( 80.5, 'run'),\n |      ...                    (np.nan, 'jump')],\n |      ...                   index=index,\n |      ...                   columns=columns)\n |      >>> df\n |                     speed species\n |                       max    type\n |      class  name\n |      bird   falcon  389.0     fly\n |             parrot   24.0     fly\n |      mammal lion     80.5     run\n |             monkey    NaN    jump\n |      \n |      If the index has multiple levels, we can reset a subset of them:\n |      \n |      >>> df.reset_index(level='class')\n |               class  speed species\n |                        max    type\n |      name\n |      falcon    bird  389.0     fly\n |      parrot    bird   24.0     fly\n |      lion    mammal   80.5     run\n |      monkey  mammal    NaN    jump\n |      \n |      If we are not dropping the index, by default, it is placed in the top\n |      level. We can place it in another level:\n |      \n |      >>> df.reset_index(level='class', col_level=1)\n |                      speed species\n |               class    max    type\n |      name\n |      falcon    bird  389.0     fly\n |      parrot    bird   24.0     fly\n |      lion    mammal   80.5     run\n |      monkey  mammal    NaN    jump\n |      \n |      When the index is inserted under another level, we can specify under\n |      which one with the parameter `col_fill`:\n |      \n |      >>> df.reset_index(level='class', col_level=1, col_fill='species')\n |                    species  speed species\n |                      class    max    type\n |      name\n |      falcon           bird  389.0     fly\n |      parrot           bird   24.0     fly\n |      lion           mammal   80.5     run\n |      monkey         mammal    NaN    jump\n |      \n |      If we specify a nonexistent level for `col_fill`, it is created:\n |      \n |      >>> df.reset_index(level='class', col_level=1, col_fill='genus')\n |                      genus  speed species\n |                      class    max    type\n |      name\n |      falcon           bird  389.0     fly\n |      parrot           bird   24.0     fly\n |      lion           mammal   80.5     run\n |      monkey         mammal    NaN    jump\n |  \n |  rfloordiv(self, other, axis='columns', level=None, fill_value=None)\n |      Get Integer division of dataframe and other, element-wise (binary operator `rfloordiv`).\n |      \n |      Equivalent to ``other // dataframe``, but with support to substitute a fill_value\n |      for missing data in one of the inputs. With reverse version, `floordiv`.\n |      \n |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `mod`, `pow`) to\n |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n |      \n |      Parameters\n |      ----------\n |      other : scalar, sequence, Series, or DataFrame\n |          Any single or multiple element data structure, or list-like object.\n |      axis :  {0 or 'index', 1 or 'columns'}\n |          Whether to compare by the index (0 or 'index') or columns\n |          (1 or 'columns'). For Series input, axis to match Series index on.\n |      level : int or label\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level.\n |      fill_value : float or None, default None\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful DataFrame alignment, with this value before computation.\n |          If data in both corresponding DataFrame locations is missing\n |          the result will be missing.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          Result of the arithmetic operation.\n |      \n |      See Also\n |      --------\n |      DataFrame.add : Add DataFrames.\n |      DataFrame.sub : Subtract DataFrames.\n |      DataFrame.mul : Multiply DataFrames.\n |      DataFrame.div : Divide DataFrames (float division).\n |      DataFrame.truediv : Divide DataFrames (float division).\n |      DataFrame.floordiv : Divide DataFrames (integer division).\n |      DataFrame.mod : Calculate modulo (remainder after division).\n |      DataFrame.pow : Calculate exponential power.\n |      \n |      Notes\n |      -----\n |      Mismatched indices will be unioned together.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'angles': [0, 3, 4],\n |      ...                    'degrees': [360, 180, 360]},\n |      ...                   index=['circle', 'triangle', 'rectangle'])\n |      >>> df\n |                 angles  degrees\n |      circle          0      360\n |      triangle        3      180\n |      rectangle       4      360\n |      \n |      Add a scalar with operator version which return the same\n |      results.\n |      \n |      >>> df + 1\n |                 angles  degrees\n |      circle          1      361\n |      triangle        4      181\n |      rectangle       5      361\n |      \n |      >>> df.add(1)\n |                 angles  degrees\n |      circle          1      361\n |      triangle        4      181\n |      rectangle       5      361\n |      \n |      Divide by constant with reverse version.\n |      \n |      >>> df.div(10)\n |                 angles  degrees\n |      circle        0.0     36.0\n |      triangle      0.3     18.0\n |      rectangle     0.4     36.0\n |      \n |      >>> df.rdiv(10)\n |                   angles   degrees\n |      circle          inf  0.027778\n |      triangle   3.333333  0.055556\n |      rectangle  2.500000  0.027778\n |      \n |      Subtract a list and Series by axis with operator version.\n |      \n |      >>> df - [1, 2]\n |                 angles  degrees\n |      circle         -1      358\n |      triangle        2      178\n |      rectangle       3      358\n |      \n |      >>> df.sub([1, 2], axis='columns')\n |                 angles  degrees\n |      circle         -1      358\n |      triangle        2      178\n |      rectangle       3      358\n |      \n |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n |      ...        axis='index')\n |                 angles  degrees\n |      circle         -1      359\n |      triangle        2      179\n |      rectangle       3      359\n |      \n |      Multiply a DataFrame of different shape with operator version.\n |      \n |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},\n |      ...                      index=['circle', 'triangle', 'rectangle'])\n |      >>> other\n |                 angles\n |      circle          0\n |      triangle        3\n |      rectangle       4\n |      \n |      >>> df * other\n |                 angles  degrees\n |      circle          0      NaN\n |      triangle        9      NaN\n |      rectangle      16      NaN\n |      \n |      >>> df.mul(other, fill_value=0)\n |                 angles  degrees\n |      circle          0      0.0\n |      triangle        9      0.0\n |      rectangle      16      0.0\n |      \n |      Divide by a MultiIndex by level.\n |      \n |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n |      ...                                    ['circle', 'triangle', 'rectangle',\n |      ...                                     'square', 'pentagon', 'hexagon']])\n |      >>> df_multindex\n |                   angles  degrees\n |      A circle          0      360\n |        triangle        3      180\n |        rectangle       4      360\n |      B square          4      360\n |        pentagon        5      540\n |        hexagon         6      720\n |      \n |      >>> df.div(df_multindex, level=1, fill_value=0)\n |                   angles  degrees\n |      A circle        NaN      1.0\n |        triangle      1.0      1.0\n |        rectangle     1.0      1.0\n |      B square        0.0      0.0\n |        pentagon      0.0      0.0\n |        hexagon       0.0      0.0\n |  \n |  rmod(self, other, axis='columns', level=None, fill_value=None)\n |      Get Modulo of dataframe and other, element-wise (binary operator `rmod`).\n |      \n |      Equivalent to ``other % dataframe``, but with support to substitute a fill_value\n |      for missing data in one of the inputs. With reverse version, `mod`.\n |      \n |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `mod`, `pow`) to\n |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n |      \n |      Parameters\n |      ----------\n |      other : scalar, sequence, Series, or DataFrame\n |          Any single or multiple element data structure, or list-like object.\n |      axis :  {0 or 'index', 1 or 'columns'}\n |          Whether to compare by the index (0 or 'index') or columns\n |          (1 or 'columns'). For Series input, axis to match Series index on.\n |      level : int or label\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level.\n |      fill_value : float or None, default None\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful DataFrame alignment, with this value before computation.\n |          If data in both corresponding DataFrame locations is missing\n |          the result will be missing.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          Result of the arithmetic operation.\n |      \n |      See Also\n |      --------\n |      DataFrame.add : Add DataFrames.\n |      DataFrame.sub : Subtract DataFrames.\n |      DataFrame.mul : Multiply DataFrames.\n |      DataFrame.div : Divide DataFrames (float division).\n |      DataFrame.truediv : Divide DataFrames (float division).\n |      DataFrame.floordiv : Divide DataFrames (integer division).\n |      DataFrame.mod : Calculate modulo (remainder after division).\n |      DataFrame.pow : Calculate exponential power.\n |      \n |      Notes\n |      -----\n |      Mismatched indices will be unioned together.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'angles': [0, 3, 4],\n |      ...                    'degrees': [360, 180, 360]},\n |      ...                   index=['circle', 'triangle', 'rectangle'])\n |      >>> df\n |                 angles  degrees\n |      circle          0      360\n |      triangle        3      180\n |      rectangle       4      360\n |      \n |      Add a scalar with operator version which return the same\n |      results.\n |      \n |      >>> df + 1\n |                 angles  degrees\n |      circle          1      361\n |      triangle        4      181\n |      rectangle       5      361\n |      \n |      >>> df.add(1)\n |                 angles  degrees\n |      circle          1      361\n |      triangle        4      181\n |      rectangle       5      361\n |      \n |      Divide by constant with reverse version.\n |      \n |      >>> df.div(10)\n |                 angles  degrees\n |      circle        0.0     36.0\n |      triangle      0.3     18.0\n |      rectangle     0.4     36.0\n |      \n |      >>> df.rdiv(10)\n |                   angles   degrees\n |      circle          inf  0.027778\n |      triangle   3.333333  0.055556\n |      rectangle  2.500000  0.027778\n |      \n |      Subtract a list and Series by axis with operator version.\n |      \n |      >>> df - [1, 2]\n |                 angles  degrees\n |      circle         -1      358\n |      triangle        2      178\n |      rectangle       3      358\n |      \n |      >>> df.sub([1, 2], axis='columns')\n |                 angles  degrees\n |      circle         -1      358\n |      triangle        2      178\n |      rectangle       3      358\n |      \n |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n |      ...        axis='index')\n |                 angles  degrees\n |      circle         -1      359\n |      triangle        2      179\n |      rectangle       3      359\n |      \n |      Multiply a DataFrame of different shape with operator version.\n |      \n |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},\n |      ...                      index=['circle', 'triangle', 'rectangle'])\n |      >>> other\n |                 angles\n |      circle          0\n |      triangle        3\n |      rectangle       4\n |      \n |      >>> df * other\n |                 angles  degrees\n |      circle          0      NaN\n |      triangle        9      NaN\n |      rectangle      16      NaN\n |      \n |      >>> df.mul(other, fill_value=0)\n |                 angles  degrees\n |      circle          0      0.0\n |      triangle        9      0.0\n |      rectangle      16      0.0\n |      \n |      Divide by a MultiIndex by level.\n |      \n |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n |      ...                                    ['circle', 'triangle', 'rectangle',\n |      ...                                     'square', 'pentagon', 'hexagon']])\n |      >>> df_multindex\n |                   angles  degrees\n |      A circle          0      360\n |        triangle        3      180\n |        rectangle       4      360\n |      B square          4      360\n |        pentagon        5      540\n |        hexagon         6      720\n |      \n |      >>> df.div(df_multindex, level=1, fill_value=0)\n |                   angles  degrees\n |      A circle        NaN      1.0\n |        triangle      1.0      1.0\n |        rectangle     1.0      1.0\n |      B square        0.0      0.0\n |        pentagon      0.0      0.0\n |        hexagon       0.0      0.0\n |  \n |  rmul(self, other, axis='columns', level=None, fill_value=None)\n |      Get Multiplication of dataframe and other, element-wise (binary operator `rmul`).\n |      \n |      Equivalent to ``other * dataframe``, but with support to substitute a fill_value\n |      for missing data in one of the inputs. With reverse version, `mul`.\n |      \n |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `mod`, `pow`) to\n |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n |      \n |      Parameters\n |      ----------\n |      other : scalar, sequence, Series, or DataFrame\n |          Any single or multiple element data structure, or list-like object.\n |      axis :  {0 or 'index', 1 or 'columns'}\n |          Whether to compare by the index (0 or 'index') or columns\n |          (1 or 'columns'). For Series input, axis to match Series index on.\n |      level : int or label\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level.\n |      fill_value : float or None, default None\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful DataFrame alignment, with this value before computation.\n |          If data in both corresponding DataFrame locations is missing\n |          the result will be missing.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          Result of the arithmetic operation.\n |      \n |      See Also\n |      --------\n |      DataFrame.add : Add DataFrames.\n |      DataFrame.sub : Subtract DataFrames.\n |      DataFrame.mul : Multiply DataFrames.\n |      DataFrame.div : Divide DataFrames (float division).\n |      DataFrame.truediv : Divide DataFrames (float division).\n |      DataFrame.floordiv : Divide DataFrames (integer division).\n |      DataFrame.mod : Calculate modulo (remainder after division).\n |      DataFrame.pow : Calculate exponential power.\n |      \n |      Notes\n |      -----\n |      Mismatched indices will be unioned together.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'angles': [0, 3, 4],\n |      ...                    'degrees': [360, 180, 360]},\n |      ...                   index=['circle', 'triangle', 'rectangle'])\n |      >>> df\n |                 angles  degrees\n |      circle          0      360\n |      triangle        3      180\n |      rectangle       4      360\n |      \n |      Add a scalar with operator version which return the same\n |      results.\n |      \n |      >>> df + 1\n |                 angles  degrees\n |      circle          1      361\n |      triangle        4      181\n |      rectangle       5      361\n |      \n |      >>> df.add(1)\n |                 angles  degrees\n |      circle          1      361\n |      triangle        4      181\n |      rectangle       5      361\n |      \n |      Divide by constant with reverse version.\n |      \n |      >>> df.div(10)\n |                 angles  degrees\n |      circle        0.0     36.0\n |      triangle      0.3     18.0\n |      rectangle     0.4     36.0\n |      \n |      >>> df.rdiv(10)\n |                   angles   degrees\n |      circle          inf  0.027778\n |      triangle   3.333333  0.055556\n |      rectangle  2.500000  0.027778\n |      \n |      Subtract a list and Series by axis with operator version.\n |      \n |      >>> df - [1, 2]\n |                 angles  degrees\n |      circle         -1      358\n |      triangle        2      178\n |      rectangle       3      358\n |      \n |      >>> df.sub([1, 2], axis='columns')\n |                 angles  degrees\n |      circle         -1      358\n |      triangle        2      178\n |      rectangle       3      358\n |      \n |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n |      ...        axis='index')\n |                 angles  degrees\n |      circle         -1      359\n |      triangle        2      179\n |      rectangle       3      359\n |      \n |      Multiply a DataFrame of different shape with operator version.\n |      \n |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},\n |      ...                      index=['circle', 'triangle', 'rectangle'])\n |      >>> other\n |                 angles\n |      circle          0\n |      triangle        3\n |      rectangle       4\n |      \n |      >>> df * other\n |                 angles  degrees\n |      circle          0      NaN\n |      triangle        9      NaN\n |      rectangle      16      NaN\n |      \n |      >>> df.mul(other, fill_value=0)\n |                 angles  degrees\n |      circle          0      0.0\n |      triangle        9      0.0\n |      rectangle      16      0.0\n |      \n |      Divide by a MultiIndex by level.\n |      \n |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n |      ...                                    ['circle', 'triangle', 'rectangle',\n |      ...                                     'square', 'pentagon', 'hexagon']])\n |      >>> df_multindex\n |                   angles  degrees\n |      A circle          0      360\n |        triangle        3      180\n |        rectangle       4      360\n |      B square          4      360\n |        pentagon        5      540\n |        hexagon         6      720\n |      \n |      >>> df.div(df_multindex, level=1, fill_value=0)\n |                   angles  degrees\n |      A circle        NaN      1.0\n |        triangle      1.0      1.0\n |        rectangle     1.0      1.0\n |      B square        0.0      0.0\n |        pentagon      0.0      0.0\n |        hexagon       0.0      0.0\n |  \n |  rolling(self, window, min_periods=None, center=False, win_type=None, on=None, axis=0, closed=None)\n |      Provide rolling window calculations.\n |      \n |      .. versionadded:: 0.18.0\n |      \n |      Parameters\n |      ----------\n |      window : int, or offset\n |          Size of the moving window. This is the number of observations used for\n |          calculating the statistic. Each window will be a fixed size.\n |      \n |          If its an offset then this will be the time period of each window. Each\n |          window will be a variable sized based on the observations included in\n |          the time-period. This is only valid for datetimelike indexes. This is\n |          new in 0.19.0\n |      min_periods : int, default None\n |          Minimum number of observations in window required to have a value\n |          (otherwise result is NA). For a window that is specified by an offset,\n |          `min_periods` will default to 1. Otherwise, `min_periods` will default\n |          to the size of the window.\n |      center : bool, default False\n |          Set the labels at the center of the window.\n |      win_type : str, default None\n |          Provide a window type. If ``None``, all points are evenly weighted.\n |          See the notes below for further information.\n |      on : str, optional\n |          For a DataFrame, a datetime-like column on which to calculate the rolling\n |          window, rather than the DataFrame's index. Provided integer column is\n |          ignored and excluded from result since an integer index is not used to\n |          calculate the rolling window.\n |      axis : int or str, default 0\n |      closed : str, default None\n |          Make the interval closed on the 'right', 'left', 'both' or\n |          'neither' endpoints.\n |          For offset-based windows, it defaults to 'right'.\n |          For fixed windows, defaults to 'both'. Remaining cases not implemented\n |          for fixed windows.\n |      \n |          .. versionadded:: 0.20.0\n |      \n |      Returns\n |      -------\n |      a Window or Rolling sub-classed for the particular operation\n |      \n |      See Also\n |      --------\n |      expanding : Provides expanding transformations.\n |      ewm : Provides exponential weighted functions.\n |      \n |      Notes\n |      -----\n |      By default, the result is set to the right edge of the window. This can be\n |      changed to the center of the window by setting ``center=True``.\n |      \n |      To learn more about the offsets & frequency strings, please see `this link\n |      <http://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__.\n |      \n |      The recognized win_types are:\n |      \n |      * ``boxcar``\n |      * ``triang``\n |      * ``blackman``\n |      * ``hamming``\n |      * ``bartlett``\n |      * ``parzen``\n |      * ``bohman``\n |      * ``blackmanharris``\n |      * ``nuttall``\n |      * ``barthann``\n |      * ``kaiser`` (needs beta)\n |      * ``gaussian`` (needs std)\n |      * ``general_gaussian`` (needs power, width)\n |      * ``slepian`` (needs width)\n |      * ``exponential`` (needs tau), center is set to None.\n |      \n |      If ``win_type=None`` all points are evenly weighted. To learn more about\n |      different window types see `scipy.signal window functions\n |      <https://docs.scipy.org/doc/scipy/reference/signal.html#window-functions>`__.\n |      \n |      Examples\n |      --------\n |      \n |      >>> df = pd.DataFrame({'B': [0, 1, 2, np.nan, 4]})\n |      >>> df\n |           B\n |      0  0.0\n |      1  1.0\n |      2  2.0\n |      3  NaN\n |      4  4.0\n |      \n |      Rolling sum with a window length of 2, using the 'triang'\n |      window type.\n |      \n |      >>> df.rolling(2, win_type='triang').sum()\n |           B\n |      0  NaN\n |      1  0.5\n |      2  1.5\n |      3  NaN\n |      4  NaN\n |      \n |      Rolling sum with a window length of 2, min_periods defaults\n |      to the window length.\n |      \n |      >>> df.rolling(2).sum()\n |           B\n |      0  NaN\n |      1  1.0\n |      2  3.0\n |      3  NaN\n |      4  NaN\n |      \n |      Same as above, but explicitly set the min_periods\n |      \n |      >>> df.rolling(2, min_periods=1).sum()\n |           B\n |      0  0.0\n |      1  1.0\n |      2  3.0\n |      3  2.0\n |      4  4.0\n |      \n |      A ragged (meaning not-a-regular frequency), time-indexed DataFrame\n |      \n |      >>> df = pd.DataFrame({'B': [0, 1, 2, np.nan, 4]},\n |      ...                   index = [pd.Timestamp('20130101 09:00:00'),\n |      ...                            pd.Timestamp('20130101 09:00:02'),\n |      ...                            pd.Timestamp('20130101 09:00:03'),\n |      ...                            pd.Timestamp('20130101 09:00:05'),\n |      ...                            pd.Timestamp('20130101 09:00:06')])\n |      \n |      >>> df\n |                             B\n |      2013-01-01 09:00:00  0.0\n |      2013-01-01 09:00:02  1.0\n |      2013-01-01 09:00:03  2.0\n |      2013-01-01 09:00:05  NaN\n |      2013-01-01 09:00:06  4.0\n |      \n |      Contrasting to an integer rolling window, this will roll a variable\n |      length window corresponding to the time period.\n |      The default for min_periods is 1.\n |      \n |      >>> df.rolling('2s').sum()\n |                             B\n |      2013-01-01 09:00:00  0.0\n |      2013-01-01 09:00:02  1.0\n |      2013-01-01 09:00:03  3.0\n |      2013-01-01 09:00:05  NaN\n |      2013-01-01 09:00:06  4.0\n |  \n |  round(self, decimals=0, *args, **kwargs)\n |      Round a DataFrame to a variable number of decimal places.\n |      \n |      Parameters\n |      ----------\n |      decimals : int, dict, Series\n |          Number of decimal places to round each column to. If an int is\n |          given, round each column to the same number of places.\n |          Otherwise dict and Series round to variable numbers of places.\n |          Column names should be in the keys if `decimals` is a\n |          dict-like, or in the index if `decimals` is a Series. Any\n |          columns not included in `decimals` will be left as is. Elements\n |          of `decimals` which are not columns of the input will be\n |          ignored.\n |      *args\n |          Additional keywords have no effect but might be accepted for\n |          compatibility with numpy.\n |      **kwargs\n |          Additional keywords have no effect but might be accepted for\n |          compatibility with numpy.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          A DataFrame with the affected columns rounded to the specified\n |          number of decimal places.\n |      \n |      See Also\n |      --------\n |      numpy.around : Round a numpy array to the given number of decimals.\n |      Series.round : Round a Series to the given number of decimals.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame([(.21, .32), (.01, .67), (.66, .03), (.21, .18)],\n |      ...                   columns=['dogs', 'cats'])\n |      >>> df\n |          dogs  cats\n |      0  0.21  0.32\n |      1  0.01  0.67\n |      2  0.66  0.03\n |      3  0.21  0.18\n |      \n |      By providing an integer each column is rounded to the same number\n |      of decimal places\n |      \n |      >>> df.round(1)\n |          dogs  cats\n |      0   0.2   0.3\n |      1   0.0   0.7\n |      2   0.7   0.0\n |      3   0.2   0.2\n |      \n |      With a dict, the number of places for specific columns can be\n |      specified with the column names as key and the number of decimal\n |      places as value\n |      \n |      >>> df.round({'dogs': 1, 'cats': 0})\n |          dogs  cats\n |      0   0.2   0.0\n |      1   0.0   1.0\n |      2   0.7   0.0\n |      3   0.2   0.0\n |      \n |      Using a Series, the number of places for specific columns can be\n |      specified with the column names as index and the number of\n |      decimal places as value\n |      \n |      >>> decimals = pd.Series([0, 1], index=['cats', 'dogs'])\n |      >>> df.round(decimals)\n |          dogs  cats\n |      0   0.2   0.0\n |      1   0.0   1.0\n |      2   0.7   0.0\n |      3   0.2   0.0\n |  \n |  rpow(self, other, axis='columns', level=None, fill_value=None)\n |      Get Exponential power of dataframe and other, element-wise (binary operator `rpow`).\n |      \n |      Equivalent to ``other ** dataframe``, but with support to substitute a fill_value\n |      for missing data in one of the inputs. With reverse version, `pow`.\n |      \n |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `mod`, `pow`) to\n |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n |      \n |      Parameters\n |      ----------\n |      other : scalar, sequence, Series, or DataFrame\n |          Any single or multiple element data structure, or list-like object.\n |      axis :  {0 or 'index', 1 or 'columns'}\n |          Whether to compare by the index (0 or 'index') or columns\n |          (1 or 'columns'). For Series input, axis to match Series index on.\n |      level : int or label\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level.\n |      fill_value : float or None, default None\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful DataFrame alignment, with this value before computation.\n |          If data in both corresponding DataFrame locations is missing\n |          the result will be missing.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          Result of the arithmetic operation.\n |      \n |      See Also\n |      --------\n |      DataFrame.add : Add DataFrames.\n |      DataFrame.sub : Subtract DataFrames.\n |      DataFrame.mul : Multiply DataFrames.\n |      DataFrame.div : Divide DataFrames (float division).\n |      DataFrame.truediv : Divide DataFrames (float division).\n |      DataFrame.floordiv : Divide DataFrames (integer division).\n |      DataFrame.mod : Calculate modulo (remainder after division).\n |      DataFrame.pow : Calculate exponential power.\n |      \n |      Notes\n |      -----\n |      Mismatched indices will be unioned together.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'angles': [0, 3, 4],\n |      ...                    'degrees': [360, 180, 360]},\n |      ...                   index=['circle', 'triangle', 'rectangle'])\n |      >>> df\n |                 angles  degrees\n |      circle          0      360\n |      triangle        3      180\n |      rectangle       4      360\n |      \n |      Add a scalar with operator version which return the same\n |      results.\n |      \n |      >>> df + 1\n |                 angles  degrees\n |      circle          1      361\n |      triangle        4      181\n |      rectangle       5      361\n |      \n |      >>> df.add(1)\n |                 angles  degrees\n |      circle          1      361\n |      triangle        4      181\n |      rectangle       5      361\n |      \n |      Divide by constant with reverse version.\n |      \n |      >>> df.div(10)\n |                 angles  degrees\n |      circle        0.0     36.0\n |      triangle      0.3     18.0\n |      rectangle     0.4     36.0\n |      \n |      >>> df.rdiv(10)\n |                   angles   degrees\n |      circle          inf  0.027778\n |      triangle   3.333333  0.055556\n |      rectangle  2.500000  0.027778\n |      \n |      Subtract a list and Series by axis with operator version.\n |      \n |      >>> df - [1, 2]\n |                 angles  degrees\n |      circle         -1      358\n |      triangle        2      178\n |      rectangle       3      358\n |      \n |      >>> df.sub([1, 2], axis='columns')\n |                 angles  degrees\n |      circle         -1      358\n |      triangle        2      178\n |      rectangle       3      358\n |      \n |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n |      ...        axis='index')\n |                 angles  degrees\n |      circle         -1      359\n |      triangle        2      179\n |      rectangle       3      359\n |      \n |      Multiply a DataFrame of different shape with operator version.\n |      \n |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},\n |      ...                      index=['circle', 'triangle', 'rectangle'])\n |      >>> other\n |                 angles\n |      circle          0\n |      triangle        3\n |      rectangle       4\n |      \n |      >>> df * other\n |                 angles  degrees\n |      circle          0      NaN\n |      triangle        9      NaN\n |      rectangle      16      NaN\n |      \n |      >>> df.mul(other, fill_value=0)\n |                 angles  degrees\n |      circle          0      0.0\n |      triangle        9      0.0\n |      rectangle      16      0.0\n |      \n |      Divide by a MultiIndex by level.\n |      \n |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n |      ...                                    ['circle', 'triangle', 'rectangle',\n |      ...                                     'square', 'pentagon', 'hexagon']])\n |      >>> df_multindex\n |                   angles  degrees\n |      A circle          0      360\n |        triangle        3      180\n |        rectangle       4      360\n |      B square          4      360\n |        pentagon        5      540\n |        hexagon         6      720\n |      \n |      >>> df.div(df_multindex, level=1, fill_value=0)\n |                   angles  degrees\n |      A circle        NaN      1.0\n |        triangle      1.0      1.0\n |        rectangle     1.0      1.0\n |      B square        0.0      0.0\n |        pentagon      0.0      0.0\n |        hexagon       0.0      0.0\n |  \n |  rsub(self, other, axis='columns', level=None, fill_value=None)\n |      Get Subtraction of dataframe and other, element-wise (binary operator `rsub`).\n |      \n |      Equivalent to ``other - dataframe``, but with support to substitute a fill_value\n |      for missing data in one of the inputs. With reverse version, `sub`.\n |      \n |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `mod`, `pow`) to\n |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n |      \n |      Parameters\n |      ----------\n |      other : scalar, sequence, Series, or DataFrame\n |          Any single or multiple element data structure, or list-like object.\n |      axis :  {0 or 'index', 1 or 'columns'}\n |          Whether to compare by the index (0 or 'index') or columns\n |          (1 or 'columns'). For Series input, axis to match Series index on.\n |      level : int or label\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level.\n |      fill_value : float or None, default None\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful DataFrame alignment, with this value before computation.\n |          If data in both corresponding DataFrame locations is missing\n |          the result will be missing.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          Result of the arithmetic operation.\n |      \n |      See Also\n |      --------\n |      DataFrame.add : Add DataFrames.\n |      DataFrame.sub : Subtract DataFrames.\n |      DataFrame.mul : Multiply DataFrames.\n |      DataFrame.div : Divide DataFrames (float division).\n |      DataFrame.truediv : Divide DataFrames (float division).\n |      DataFrame.floordiv : Divide DataFrames (integer division).\n |      DataFrame.mod : Calculate modulo (remainder after division).\n |      DataFrame.pow : Calculate exponential power.\n |      \n |      Notes\n |      -----\n |      Mismatched indices will be unioned together.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'angles': [0, 3, 4],\n |      ...                    'degrees': [360, 180, 360]},\n |      ...                   index=['circle', 'triangle', 'rectangle'])\n |      >>> df\n |                 angles  degrees\n |      circle          0      360\n |      triangle        3      180\n |      rectangle       4      360\n |      \n |      Add a scalar with operator version which return the same\n |      results.\n |      \n |      >>> df + 1\n |                 angles  degrees\n |      circle          1      361\n |      triangle        4      181\n |      rectangle       5      361\n |      \n |      >>> df.add(1)\n |                 angles  degrees\n |      circle          1      361\n |      triangle        4      181\n |      rectangle       5      361\n |      \n |      Divide by constant with reverse version.\n |      \n |      >>> df.div(10)\n |                 angles  degrees\n |      circle        0.0     36.0\n |      triangle      0.3     18.0\n |      rectangle     0.4     36.0\n |      \n |      >>> df.rdiv(10)\n |                   angles   degrees\n |      circle          inf  0.027778\n |      triangle   3.333333  0.055556\n |      rectangle  2.500000  0.027778\n |      \n |      Subtract a list and Series by axis with operator version.\n |      \n |      >>> df - [1, 2]\n |                 angles  degrees\n |      circle         -1      358\n |      triangle        2      178\n |      rectangle       3      358\n |      \n |      >>> df.sub([1, 2], axis='columns')\n |                 angles  degrees\n |      circle         -1      358\n |      triangle        2      178\n |      rectangle       3      358\n |      \n |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n |      ...        axis='index')\n |                 angles  degrees\n |      circle         -1      359\n |      triangle        2      179\n |      rectangle       3      359\n |      \n |      Multiply a DataFrame of different shape with operator version.\n |      \n |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},\n |      ...                      index=['circle', 'triangle', 'rectangle'])\n |      >>> other\n |                 angles\n |      circle          0\n |      triangle        3\n |      rectangle       4\n |      \n |      >>> df * other\n |                 angles  degrees\n |      circle          0      NaN\n |      triangle        9      NaN\n |      rectangle      16      NaN\n |      \n |      >>> df.mul(other, fill_value=0)\n |                 angles  degrees\n |      circle          0      0.0\n |      triangle        9      0.0\n |      rectangle      16      0.0\n |      \n |      Divide by a MultiIndex by level.\n |      \n |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n |      ...                                    ['circle', 'triangle', 'rectangle',\n |      ...                                     'square', 'pentagon', 'hexagon']])\n |      >>> df_multindex\n |                   angles  degrees\n |      A circle          0      360\n |        triangle        3      180\n |        rectangle       4      360\n |      B square          4      360\n |        pentagon        5      540\n |        hexagon         6      720\n |      \n |      >>> df.div(df_multindex, level=1, fill_value=0)\n |                   angles  degrees\n |      A circle        NaN      1.0\n |        triangle      1.0      1.0\n |        rectangle     1.0      1.0\n |      B square        0.0      0.0\n |        pentagon      0.0      0.0\n |        hexagon       0.0      0.0\n |  \n |  rtruediv(self, other, axis='columns', level=None, fill_value=None)\n |      Get Floating division of dataframe and other, element-wise (binary operator `rtruediv`).\n |      \n |      Equivalent to ``other / dataframe``, but with support to substitute a fill_value\n |      for missing data in one of the inputs. With reverse version, `truediv`.\n |      \n |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `mod`, `pow`) to\n |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n |      \n |      Parameters\n |      ----------\n |      other : scalar, sequence, Series, or DataFrame\n |          Any single or multiple element data structure, or list-like object.\n |      axis :  {0 or 'index', 1 or 'columns'}\n |          Whether to compare by the index (0 or 'index') or columns\n |          (1 or 'columns'). For Series input, axis to match Series index on.\n |      level : int or label\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level.\n |      fill_value : float or None, default None\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful DataFrame alignment, with this value before computation.\n |          If data in both corresponding DataFrame locations is missing\n |          the result will be missing.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          Result of the arithmetic operation.\n |      \n |      See Also\n |      --------\n |      DataFrame.add : Add DataFrames.\n |      DataFrame.sub : Subtract DataFrames.\n |      DataFrame.mul : Multiply DataFrames.\n |      DataFrame.div : Divide DataFrames (float division).\n |      DataFrame.truediv : Divide DataFrames (float division).\n |      DataFrame.floordiv : Divide DataFrames (integer division).\n |      DataFrame.mod : Calculate modulo (remainder after division).\n |      DataFrame.pow : Calculate exponential power.\n |      \n |      Notes\n |      -----\n |      Mismatched indices will be unioned together.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'angles': [0, 3, 4],\n |      ...                    'degrees': [360, 180, 360]},\n |      ...                   index=['circle', 'triangle', 'rectangle'])\n |      >>> df\n |                 angles  degrees\n |      circle          0      360\n |      triangle        3      180\n |      rectangle       4      360\n |      \n |      Add a scalar with operator version which return the same\n |      results.\n |      \n |      >>> df + 1\n |                 angles  degrees\n |      circle          1      361\n |      triangle        4      181\n |      rectangle       5      361\n |      \n |      >>> df.add(1)\n |                 angles  degrees\n |      circle          1      361\n |      triangle        4      181\n |      rectangle       5      361\n |      \n |      Divide by constant with reverse version.\n |      \n |      >>> df.div(10)\n |                 angles  degrees\n |      circle        0.0     36.0\n |      triangle      0.3     18.0\n |      rectangle     0.4     36.0\n |      \n |      >>> df.rdiv(10)\n |                   angles   degrees\n |      circle          inf  0.027778\n |      triangle   3.333333  0.055556\n |      rectangle  2.500000  0.027778\n |      \n |      Subtract a list and Series by axis with operator version.\n |      \n |      >>> df - [1, 2]\n |                 angles  degrees\n |      circle         -1      358\n |      triangle        2      178\n |      rectangle       3      358\n |      \n |      >>> df.sub([1, 2], axis='columns')\n |                 angles  degrees\n |      circle         -1      358\n |      triangle        2      178\n |      rectangle       3      358\n |      \n |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n |      ...        axis='index')\n |                 angles  degrees\n |      circle         -1      359\n |      triangle        2      179\n |      rectangle       3      359\n |      \n |      Multiply a DataFrame of different shape with operator version.\n |      \n |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},\n |      ...                      index=['circle', 'triangle', 'rectangle'])\n |      >>> other\n |                 angles\n |      circle          0\n |      triangle        3\n |      rectangle       4\n |      \n |      >>> df * other\n |                 angles  degrees\n |      circle          0      NaN\n |      triangle        9      NaN\n |      rectangle      16      NaN\n |      \n |      >>> df.mul(other, fill_value=0)\n |                 angles  degrees\n |      circle          0      0.0\n |      triangle        9      0.0\n |      rectangle      16      0.0\n |      \n |      Divide by a MultiIndex by level.\n |      \n |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n |      ...                                    ['circle', 'triangle', 'rectangle',\n |      ...                                     'square', 'pentagon', 'hexagon']])\n |      >>> df_multindex\n |                   angles  degrees\n |      A circle          0      360\n |        triangle        3      180\n |        rectangle       4      360\n |      B square          4      360\n |        pentagon        5      540\n |        hexagon         6      720\n |      \n |      >>> df.div(df_multindex, level=1, fill_value=0)\n |                   angles  degrees\n |      A circle        NaN      1.0\n |        triangle      1.0      1.0\n |        rectangle     1.0      1.0\n |      B square        0.0      0.0\n |        pentagon      0.0      0.0\n |        hexagon       0.0      0.0\n |  \n |  select_dtypes(self, include=None, exclude=None)\n |      Return a subset of the DataFrame's columns based on the column dtypes.\n |      \n |      Parameters\n |      ----------\n |      include, exclude : scalar or list-like\n |          A selection of dtypes or strings to be included/excluded. At least\n |          one of these parameters must be supplied.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          The subset of the frame including the dtypes in ``include`` and\n |          excluding the dtypes in ``exclude``.\n |      \n |      Raises\n |      ------\n |      ValueError\n |          * If both of ``include`` and ``exclude`` are empty\n |          * If ``include`` and ``exclude`` have overlapping elements\n |          * If any kind of string dtype is passed in.\n |      \n |      Notes\n |      -----\n |      * To select all *numeric* types, use ``np.number`` or ``'number'``\n |      * To select strings you must use the ``object`` dtype, but note that\n |        this will return *all* object dtype columns\n |      * See the `numpy dtype hierarchy\n |        <http://docs.scipy.org/doc/numpy/reference/arrays.scalars.html>`__\n |      * To select datetimes, use ``np.datetime64``, ``'datetime'`` or\n |        ``'datetime64'``\n |      * To select timedeltas, use ``np.timedelta64``, ``'timedelta'`` or\n |        ``'timedelta64'``\n |      * To select Pandas categorical dtypes, use ``'category'``\n |      * To select Pandas datetimetz dtypes, use ``'datetimetz'`` (new in\n |        0.20.0) or ``'datetime64[ns, tz]'``\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'a': [1, 2] * 3,\n |      ...                    'b': [True, False] * 3,\n |      ...                    'c': [1.0, 2.0] * 3})\n |      >>> df\n |              a      b  c\n |      0       1   True  1.0\n |      1       2  False  2.0\n |      2       1   True  1.0\n |      3       2  False  2.0\n |      4       1   True  1.0\n |      5       2  False  2.0\n |      \n |      >>> df.select_dtypes(include='bool')\n |         b\n |      0  True\n |      1  False\n |      2  True\n |      3  False\n |      4  True\n |      5  False\n |      \n |      >>> df.select_dtypes(include=['float64'])\n |         c\n |      0  1.0\n |      1  2.0\n |      2  1.0\n |      3  2.0\n |      4  1.0\n |      5  2.0\n |      \n |      >>> df.select_dtypes(exclude=['int'])\n |             b    c\n |      0   True  1.0\n |      1  False  2.0\n |      2   True  1.0\n |      3  False  2.0\n |      4   True  1.0\n |      5  False  2.0\n |  \n |  sem(self, axis=None, skipna=None, level=None, ddof=1, numeric_only=None, **kwargs)\n |      Return unbiased standard error of the mean over requested axis.\n |      \n |      Normalized by N-1 by default. This can be changed using the ddof argument\n |      \n |      Parameters\n |      ----------\n |      axis : {index (0), columns (1)}\n |      skipna : bool, default True\n |          Exclude NA/null values. If an entire row/column is NA, the result\n |          will be NA\n |      level : int or level name, default None\n |          If the axis is a MultiIndex (hierarchical), count along a\n |          particular level, collapsing into a Series\n |      ddof : int, default 1\n |          Delta Degrees of Freedom. The divisor used in calculations is N - ddof,\n |          where N represents the number of elements.\n |      numeric_only : bool, default None\n |          Include only float, int, boolean columns. If None, will attempt to use\n |          everything, then use only numeric data. Not implemented for Series.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame (if level specified)\n |  \n |  set_index(self, keys, drop=True, append=False, inplace=False, verify_integrity=False)\n |      Set the DataFrame index using existing columns.\n |      \n |      Set the DataFrame index (row labels) using one or more existing\n |      columns or arrays (of the correct length). The index can replace the\n |      existing index or expand on it.\n |      \n |      Parameters\n |      ----------\n |      keys : label or array-like or list of labels/arrays\n |          This parameter can be either a single column key, a single array of\n |          the same length as the calling DataFrame, or a list containing an\n |          arbitrary combination of column keys and arrays. Here, \"array\"\n |          encompasses :class:`Series`, :class:`Index`, ``np.ndarray``, and\n |          instances of :class:`~collections.abc.Iterator`.\n |      drop : bool, default True\n |          Delete columns to be used as the new index.\n |      append : bool, default False\n |          Whether to append columns to existing index.\n |      inplace : bool, default False\n |          Modify the DataFrame in place (do not create a new object).\n |      verify_integrity : bool, default False\n |          Check the new index for duplicates. Otherwise defer the check until\n |          necessary. Setting to False will improve the performance of this\n |          method.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          Changed row labels.\n |      \n |      See Also\n |      --------\n |      DataFrame.reset_index : Opposite of set_index.\n |      DataFrame.reindex : Change to new indices or expand indices.\n |      DataFrame.reindex_like : Change to same indices as other DataFrame.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'month': [1, 4, 7, 10],\n |      ...                    'year': [2012, 2014, 2013, 2014],\n |      ...                    'sale': [55, 40, 84, 31]})\n |      >>> df\n |         month  year  sale\n |      0      1  2012    55\n |      1      4  2014    40\n |      2      7  2013    84\n |      3     10  2014    31\n |      \n |      Set the index to become the 'month' column:\n |      \n |      >>> df.set_index('month')\n |             year  sale\n |      month\n |      1      2012    55\n |      4      2014    40\n |      7      2013    84\n |      10     2014    31\n |      \n |      Create a MultiIndex using columns 'year' and 'month':\n |      \n |      >>> df.set_index(['year', 'month'])\n |                  sale\n |      year  month\n |      2012  1     55\n |      2014  4     40\n |      2013  7     84\n |      2014  10    31\n |      \n |      Create a MultiIndex using an Index and a column:\n |      \n |      >>> df.set_index([pd.Index([1, 2, 3, 4]), 'year'])\n |               month  sale\n |         year\n |      1  2012  1      55\n |      2  2014  4      40\n |      3  2013  7      84\n |      4  2014  10     31\n |      \n |      Create a MultiIndex using two Series:\n |      \n |      >>> s = pd.Series([1, 2, 3, 4])\n |      >>> df.set_index([s, s**2])\n |            month  year  sale\n |      1 1       1  2012    55\n |      2 4       4  2014    40\n |      3 9       7  2013    84\n |      4 16     10  2014    31\n |  \n |  set_value(self, index, col, value, takeable=False)\n |      Put single value at passed column and index.\n |      \n |      .. deprecated:: 0.21.0\n |          Use .at[] or .iat[] accessors instead.\n |      \n |      Parameters\n |      ----------\n |      index : row label\n |      col : column label\n |      value : scalar\n |      takeable : interpret the index/col as indexers, default False\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          If label pair is contained, will be reference to calling DataFrame,\n |          otherwise a new object.\n |  \n |  shift(self, periods=1, freq=None, axis=0, fill_value=None)\n |      Shift index by desired number of periods with an optional time `freq`.\n |      \n |      When `freq` is not passed, shift the index without realigning the data.\n |      If `freq` is passed (in this case, the index must be date or datetime,\n |      or it will raise a `NotImplementedError`), the index will be\n |      increased using the periods and the `freq`.\n |      \n |      Parameters\n |      ----------\n |      periods : int\n |          Number of periods to shift. Can be positive or negative.\n |      freq : DateOffset, tseries.offsets, timedelta, or str, optional\n |          Offset to use from the tseries module or time rule (e.g. 'EOM').\n |          If `freq` is specified then the index values are shifted but the\n |          data is not realigned. That is, use `freq` if you would like to\n |          extend the index when shifting and preserve the original data.\n |      axis : {0 or 'index', 1 or 'columns', None}, default None\n |          Shift direction.\n |      fill_value : object, optional\n |          The scalar value to use for newly introduced missing values.\n |          the default depends on the dtype of `self`.\n |          For numeric data, ``np.nan`` is used.\n |          For datetime, timedelta, or period data, etc. :attr:`NaT` is used.\n |          For extension dtypes, ``self.dtype.na_value`` is used.\n |      \n |          .. versionchanged:: 0.24.0\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          Copy of input object, shifted.\n |      \n |      See Also\n |      --------\n |      Index.shift : Shift values of Index.\n |      DatetimeIndex.shift : Shift values of DatetimeIndex.\n |      PeriodIndex.shift : Shift values of PeriodIndex.\n |      tshift : Shift the time index, using the index's frequency if\n |          available.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'Col1': [10, 20, 15, 30, 45],\n |      ...                    'Col2': [13, 23, 18, 33, 48],\n |      ...                    'Col3': [17, 27, 22, 37, 52]})\n |      \n |      >>> df.shift(periods=3)\n |         Col1  Col2  Col3\n |      0   NaN   NaN   NaN\n |      1   NaN   NaN   NaN\n |      2   NaN   NaN   NaN\n |      3  10.0  13.0  17.0\n |      4  20.0  23.0  27.0\n |      \n |      >>> df.shift(periods=1, axis='columns')\n |         Col1  Col2  Col3\n |      0   NaN  10.0  13.0\n |      1   NaN  20.0  23.0\n |      2   NaN  15.0  18.0\n |      3   NaN  30.0  33.0\n |      4   NaN  45.0  48.0\n |      \n |      >>> df.shift(periods=3, fill_value=0)\n |         Col1  Col2  Col3\n |      0     0     0     0\n |      1     0     0     0\n |      2     0     0     0\n |      3    10    13    17\n |      4    20    23    27\n |  \n |  skew(self, axis=None, skipna=None, level=None, numeric_only=None, **kwargs)\n |      Return unbiased skew over requested axis\n |      Normalized by N-1.\n |      \n |      Parameters\n |      ----------\n |      axis : {index (0), columns (1)}\n |          Axis for the function to be applied on.\n |      skipna : bool, default True\n |          Exclude NA/null values when computing the result.\n |      level : int or level name, default None\n |          If the axis is a MultiIndex (hierarchical), count along a\n |          particular level, collapsing into a Series.\n |      numeric_only : bool, default None\n |          Include only float, int, boolean columns. If None, will attempt to use\n |          everything, then use only numeric data. Not implemented for Series.\n |      **kwargs\n |          Additional keyword arguments to be passed to the function.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame (if level specified)\n |  \n |  sort_index(self, axis=0, level=None, ascending=True, inplace=False, kind='quicksort', na_position='last', sort_remaining=True, by=None)\n |      Sort object by labels (along an axis).\n |      \n |      Parameters\n |      ----------\n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          The axis along which to sort.  The value 0 identifies the rows,\n |          and 1 identifies the columns.\n |      level : int or level name or list of ints or list of level names\n |          If not None, sort on values in specified index level(s).\n |      ascending : bool, default True\n |          Sort ascending vs. descending.\n |      inplace : bool, default False\n |          If True, perform operation in-place.\n |      kind : {'quicksort', 'mergesort', 'heapsort'}, default 'quicksort'\n |          Choice of sorting algorithm. See also ndarray.np.sort for more\n |          information.  `mergesort` is the only stable algorithm. For\n |          DataFrames, this option is only applied when sorting on a single\n |          column or label.\n |      na_position : {'first', 'last'}, default 'last'\n |          Puts NaNs at the beginning if `first`; `last` puts NaNs at the end.\n |          Not implemented for MultiIndex.\n |      sort_remaining : bool, default True\n |          If True and sorting by level and index is multilevel, sort by other\n |          levels too (in order) after sorting by specified level.\n |      \n |      Returns\n |      -------\n |      sorted_obj : DataFrame or None\n |          DataFrame with sorted index if inplace=False, None otherwise.\n |  \n |  sort_values(self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n |      Sort by the values along either axis.\n |      \n |      Parameters\n |      ----------\n |              by : str or list of str\n |                  Name or list of names to sort by.\n |      \n |                  - if `axis` is 0 or `'index'` then `by` may contain index\n |                    levels and/or column labels\n |                  - if `axis` is 1 or `'columns'` then `by` may contain column\n |                    levels and/or index labels\n |      \n |                  .. versionchanged:: 0.23.0\n |                     Allow specifying index or column level names.\n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |           Axis to be sorted.\n |      ascending : bool or list of bool, default True\n |           Sort ascending vs. descending. Specify list for multiple sort\n |           orders.  If this is a list of bools, must match the length of\n |           the by.\n |      inplace : bool, default False\n |           If True, perform operation in-place.\n |      kind : {'quicksort', 'mergesort', 'heapsort'}, default 'quicksort'\n |           Choice of sorting algorithm. See also ndarray.np.sort for more\n |           information.  `mergesort` is the only stable algorithm. For\n |           DataFrames, this option is only applied when sorting on a single\n |           column or label.\n |      na_position : {'first', 'last'}, default 'last'\n |           Puts NaNs at the beginning if `first`; `last` puts NaNs at the\n |           end.\n |      \n |      Returns\n |      -------\n |      sorted_obj : DataFrame or None\n |          DataFrame with sorted values if inplace=False, None otherwise.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({\n |      ...     'col1': ['A', 'A', 'B', np.nan, 'D', 'C'],\n |      ...     'col2': [2, 1, 9, 8, 7, 4],\n |      ...     'col3': [0, 1, 9, 4, 2, 3],\n |      ... })\n |      >>> df\n |          col1 col2 col3\n |      0   A    2    0\n |      1   A    1    1\n |      2   B    9    9\n |      3   NaN  8    4\n |      4   D    7    2\n |      5   C    4    3\n |      \n |      Sort by col1\n |      \n |      >>> df.sort_values(by=['col1'])\n |          col1 col2 col3\n |      0   A    2    0\n |      1   A    1    1\n |      2   B    9    9\n |      5   C    4    3\n |      4   D    7    2\n |      3   NaN  8    4\n |      \n |      Sort by multiple columns\n |      \n |      >>> df.sort_values(by=['col1', 'col2'])\n |          col1 col2 col3\n |      1   A    1    1\n |      0   A    2    0\n |      2   B    9    9\n |      5   C    4    3\n |      4   D    7    2\n |      3   NaN  8    4\n |      \n |      Sort Descending\n |      \n |      >>> df.sort_values(by='col1', ascending=False)\n |          col1 col2 col3\n |      4   D    7    2\n |      5   C    4    3\n |      2   B    9    9\n |      0   A    2    0\n |      1   A    1    1\n |      3   NaN  8    4\n |      \n |      Putting NAs first\n |      \n |      >>> df.sort_values(by='col1', ascending=False, na_position='first')\n |          col1 col2 col3\n |      3   NaN  8    4\n |      4   D    7    2\n |      5   C    4    3\n |      2   B    9    9\n |      0   A    2    0\n |      1   A    1    1\n |  \n |  stack(self, level=-1, dropna=True)\n |      Stack the prescribed level(s) from columns to index.\n |      \n |      Return a reshaped DataFrame or Series having a multi-level\n |      index with one or more new inner-most levels compared to the current\n |      DataFrame. The new inner-most levels are created by pivoting the\n |      columns of the current dataframe:\n |      \n |        - if the columns have a single level, the output is a Series;\n |        - if the columns have multiple levels, the new index\n |          level(s) is (are) taken from the prescribed level(s) and\n |          the output is a DataFrame.\n |      \n |      The new index levels are sorted.\n |      \n |      Parameters\n |      ----------\n |      level : int, str, list, default -1\n |          Level(s) to stack from the column axis onto the index\n |          axis, defined as one index or label, or a list of indices\n |          or labels.\n |      dropna : bool, default True\n |          Whether to drop rows in the resulting Frame/Series with\n |          missing values. Stacking a column level onto the index\n |          axis can create combinations of index and column values\n |          that are missing from the original dataframe. See Examples\n |          section.\n |      \n |      Returns\n |      -------\n |      DataFrame or Series\n |          Stacked dataframe or series.\n |      \n |      See Also\n |      --------\n |      DataFrame.unstack : Unstack prescribed level(s) from index axis\n |           onto column axis.\n |      DataFrame.pivot : Reshape dataframe from long format to wide\n |           format.\n |      DataFrame.pivot_table : Create a spreadsheet-style pivot table\n |           as a DataFrame.\n |      \n |      Notes\n |      -----\n |      The function is named by analogy with a collection of books\n |      being reorganized from being side by side on a horizontal\n |      position (the columns of the dataframe) to being stacked\n |      vertically on top of each other (in the index of the\n |      dataframe).\n |      \n |      Examples\n |      --------\n |      **Single level columns**\n |      \n |      >>> df_single_level_cols = pd.DataFrame([[0, 1], [2, 3]],\n |      ...                                     index=['cat', 'dog'],\n |      ...                                     columns=['weight', 'height'])\n |      \n |      Stacking a dataframe with a single level column axis returns a Series:\n |      \n |      >>> df_single_level_cols\n |           weight height\n |      cat       0      1\n |      dog       2      3\n |      >>> df_single_level_cols.stack()\n |      cat  weight    0\n |           height    1\n |      dog  weight    2\n |           height    3\n |      dtype: int64\n |      \n |      **Multi level columns: simple case**\n |      \n |      >>> multicol1 = pd.MultiIndex.from_tuples([('weight', 'kg'),\n |      ...                                        ('weight', 'pounds')])\n |      >>> df_multi_level_cols1 = pd.DataFrame([[1, 2], [2, 4]],\n |      ...                                     index=['cat', 'dog'],\n |      ...                                     columns=multicol1)\n |      \n |      Stacking a dataframe with a multi-level column axis:\n |      \n |      >>> df_multi_level_cols1\n |           weight\n |               kg    pounds\n |      cat       1        2\n |      dog       2        4\n |      >>> df_multi_level_cols1.stack()\n |                  weight\n |      cat kg           1\n |          pounds       2\n |      dog kg           2\n |          pounds       4\n |      \n |      **Missing values**\n |      \n |      >>> multicol2 = pd.MultiIndex.from_tuples([('weight', 'kg'),\n |      ...                                        ('height', 'm')])\n |      >>> df_multi_level_cols2 = pd.DataFrame([[1.0, 2.0], [3.0, 4.0]],\n |      ...                                     index=['cat', 'dog'],\n |      ...                                     columns=multicol2)\n |      \n |      It is common to have missing values when stacking a dataframe\n |      with multi-level columns, as the stacked dataframe typically\n |      has more values than the original dataframe. Missing values\n |      are filled with NaNs:\n |      \n |      >>> df_multi_level_cols2\n |          weight height\n |              kg      m\n |      cat    1.0    2.0\n |      dog    3.0    4.0\n |      >>> df_multi_level_cols2.stack()\n |              height  weight\n |      cat kg     NaN     1.0\n |          m      2.0     NaN\n |      dog kg     NaN     3.0\n |          m      4.0     NaN\n |      \n |      **Prescribing the level(s) to be stacked**\n |      \n |      The first parameter controls which level or levels are stacked:\n |      \n |      >>> df_multi_level_cols2.stack(0)\n |                   kg    m\n |      cat height  NaN  2.0\n |          weight  1.0  NaN\n |      dog height  NaN  4.0\n |          weight  3.0  NaN\n |      >>> df_multi_level_cols2.stack([0, 1])\n |      cat  height  m     2.0\n |           weight  kg    1.0\n |      dog  height  m     4.0\n |           weight  kg    3.0\n |      dtype: float64\n |      \n |      **Dropping missing values**\n |      \n |      >>> df_multi_level_cols3 = pd.DataFrame([[None, 1.0], [2.0, 3.0]],\n |      ...                                     index=['cat', 'dog'],\n |      ...                                     columns=multicol2)\n |      \n |      Note that rows where all values are missing are dropped by\n |      default but this behaviour can be controlled via the dropna\n |      keyword parameter:\n |      \n |      >>> df_multi_level_cols3\n |          weight height\n |              kg      m\n |      cat    NaN    1.0\n |      dog    2.0    3.0\n |      >>> df_multi_level_cols3.stack(dropna=False)\n |              height  weight\n |      cat kg     NaN     NaN\n |          m      1.0     NaN\n |      dog kg     NaN     2.0\n |          m      3.0     NaN\n |      >>> df_multi_level_cols3.stack(dropna=True)\n |              height  weight\n |      cat m      1.0     NaN\n |      dog kg     NaN     2.0\n |          m      3.0     NaN\n |  \n |  std(self, axis=None, skipna=None, level=None, ddof=1, numeric_only=None, **kwargs)\n |      Return sample standard deviation over requested axis.\n |      \n |      Normalized by N-1 by default. This can be changed using the ddof argument\n |      \n |      Parameters\n |      ----------\n |      axis : {index (0), columns (1)}\n |      skipna : bool, default True\n |          Exclude NA/null values. If an entire row/column is NA, the result\n |          will be NA\n |      level : int or level name, default None\n |          If the axis is a MultiIndex (hierarchical), count along a\n |          particular level, collapsing into a Series\n |      ddof : int, default 1\n |          Delta Degrees of Freedom. The divisor used in calculations is N - ddof,\n |          where N represents the number of elements.\n |      numeric_only : bool, default None\n |          Include only float, int, boolean columns. If None, will attempt to use\n |          everything, then use only numeric data. Not implemented for Series.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame (if level specified)\n |  \n |  sub(self, other, axis='columns', level=None, fill_value=None)\n |      Get Subtraction of dataframe and other, element-wise (binary operator `sub`).\n |      \n |      Equivalent to ``dataframe - other``, but with support to substitute a fill_value\n |      for missing data in one of the inputs. With reverse version, `rsub`.\n |      \n |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `mod`, `pow`) to\n |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n |      \n |      Parameters\n |      ----------\n |      other : scalar, sequence, Series, or DataFrame\n |          Any single or multiple element data structure, or list-like object.\n |      axis :  {0 or 'index', 1 or 'columns'}\n |          Whether to compare by the index (0 or 'index') or columns\n |          (1 or 'columns'). For Series input, axis to match Series index on.\n |      level : int or label\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level.\n |      fill_value : float or None, default None\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful DataFrame alignment, with this value before computation.\n |          If data in both corresponding DataFrame locations is missing\n |          the result will be missing.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          Result of the arithmetic operation.\n |      \n |      See Also\n |      --------\n |      DataFrame.add : Add DataFrames.\n |      DataFrame.sub : Subtract DataFrames.\n |      DataFrame.mul : Multiply DataFrames.\n |      DataFrame.div : Divide DataFrames (float division).\n |      DataFrame.truediv : Divide DataFrames (float division).\n |      DataFrame.floordiv : Divide DataFrames (integer division).\n |      DataFrame.mod : Calculate modulo (remainder after division).\n |      DataFrame.pow : Calculate exponential power.\n |      \n |      Notes\n |      -----\n |      Mismatched indices will be unioned together.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'angles': [0, 3, 4],\n |      ...                    'degrees': [360, 180, 360]},\n |      ...                   index=['circle', 'triangle', 'rectangle'])\n |      >>> df\n |                 angles  degrees\n |      circle          0      360\n |      triangle        3      180\n |      rectangle       4      360\n |      \n |      Add a scalar with operator version which return the same\n |      results.\n |      \n |      >>> df + 1\n |                 angles  degrees\n |      circle          1      361\n |      triangle        4      181\n |      rectangle       5      361\n |      \n |      >>> df.add(1)\n |                 angles  degrees\n |      circle          1      361\n |      triangle        4      181\n |      rectangle       5      361\n |      \n |      Divide by constant with reverse version.\n |      \n |      >>> df.div(10)\n |                 angles  degrees\n |      circle        0.0     36.0\n |      triangle      0.3     18.0\n |      rectangle     0.4     36.0\n |      \n |      >>> df.rdiv(10)\n |                   angles   degrees\n |      circle          inf  0.027778\n |      triangle   3.333333  0.055556\n |      rectangle  2.500000  0.027778\n |      \n |      Subtract a list and Series by axis with operator version.\n |      \n |      >>> df - [1, 2]\n |                 angles  degrees\n |      circle         -1      358\n |      triangle        2      178\n |      rectangle       3      358\n |      \n |      >>> df.sub([1, 2], axis='columns')\n |                 angles  degrees\n |      circle         -1      358\n |      triangle        2      178\n |      rectangle       3      358\n |      \n |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n |      ...        axis='index')\n |                 angles  degrees\n |      circle         -1      359\n |      triangle        2      179\n |      rectangle       3      359\n |      \n |      Multiply a DataFrame of different shape with operator version.\n |      \n |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},\n |      ...                      index=['circle', 'triangle', 'rectangle'])\n |      >>> other\n |                 angles\n |      circle          0\n |      triangle        3\n |      rectangle       4\n |      \n |      >>> df * other\n |                 angles  degrees\n |      circle          0      NaN\n |      triangle        9      NaN\n |      rectangle      16      NaN\n |      \n |      >>> df.mul(other, fill_value=0)\n |                 angles  degrees\n |      circle          0      0.0\n |      triangle        9      0.0\n |      rectangle      16      0.0\n |      \n |      Divide by a MultiIndex by level.\n |      \n |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n |      ...                                    ['circle', 'triangle', 'rectangle',\n |      ...                                     'square', 'pentagon', 'hexagon']])\n |      >>> df_multindex\n |                   angles  degrees\n |      A circle          0      360\n |        triangle        3      180\n |        rectangle       4      360\n |      B square          4      360\n |        pentagon        5      540\n |        hexagon         6      720\n |      \n |      >>> df.div(df_multindex, level=1, fill_value=0)\n |                   angles  degrees\n |      A circle        NaN      1.0\n |        triangle      1.0      1.0\n |        rectangle     1.0      1.0\n |      B square        0.0      0.0\n |        pentagon      0.0      0.0\n |        hexagon       0.0      0.0\n |  \n |  subtract = sub(self, other, axis='columns', level=None, fill_value=None)\n |  \n |  sum(self, axis=None, skipna=None, level=None, numeric_only=None, min_count=0, **kwargs)\n |      Return the sum of the values for the requested axis.\n |      \n |                  This is equivalent to the method ``numpy.sum``.\n |      \n |      Parameters\n |      ----------\n |      axis : {index (0), columns (1)}\n |          Axis for the function to be applied on.\n |      skipna : bool, default True\n |          Exclude NA/null values when computing the result.\n |      level : int or level name, default None\n |          If the axis is a MultiIndex (hierarchical), count along a\n |          particular level, collapsing into a Series.\n |      numeric_only : bool, default None\n |          Include only float, int, boolean columns. If None, will attempt to use\n |          everything, then use only numeric data. Not implemented for Series.\n |      min_count : int, default 0\n |          The required number of valid values to perform the operation. If fewer than\n |          ``min_count`` non-NA values are present the result will be NA.\n |      \n |          .. versionadded :: 0.22.0\n |      \n |             Added with the default being 0. This means the sum of an all-NA\n |             or empty Series is 0, and the product of an all-NA or empty\n |             Series is 1.\n |      **kwargs\n |          Additional keyword arguments to be passed to the function.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame (if level specified)\n |      \n |      See Also\n |      --------\n |      Series.sum : Return the sum.\n |      Series.min : Return the minimum.\n |      Series.max : Return the maximum.\n |      Series.idxmin : Return the index of the minimum.\n |      Series.idxmax : Return the index of the maximum.\n |      DataFrame.sum : Return the sum over the requested axis.\n |      DataFrame.min : Return the minimum over the requested axis.\n |      DataFrame.max : Return the maximum over the requested axis.\n |      DataFrame.idxmin : Return the index of the minimum over the requested axis.\n |      DataFrame.idxmax : Return the index of the maximum over the requested axis.\n |      \n |      Examples\n |      --------\n |      >>> idx = pd.MultiIndex.from_arrays([\n |      ...     ['warm', 'warm', 'cold', 'cold'],\n |      ...     ['dog', 'falcon', 'fish', 'spider']],\n |      ...     names=['blooded', 'animal'])\n |      >>> s = pd.Series([4, 2, 0, 8], name='legs', index=idx)\n |      >>> s\n |      blooded  animal\n |      warm     dog       4\n |               falcon    2\n |      cold     fish      0\n |               spider    8\n |      Name: legs, dtype: int64\n |      \n |      >>> s.sum()\n |      14\n |      \n |      Sum using level names, as well as indices.\n |      \n |      >>> s.sum(level='blooded')\n |      blooded\n |      warm    6\n |      cold    8\n |      Name: legs, dtype: int64\n |      \n |      >>> s.sum(level=0)\n |      blooded\n |      warm    6\n |      cold    8\n |      Name: legs, dtype: int64\n |      \n |      By default, the sum of an empty or all-NA Series is ``0``.\n |      \n |      >>> pd.Series([]).sum()  # min_count=0 is the default\n |      0.0\n |      \n |      This can be controlled with the ``min_count`` parameter. For example, if\n |      you'd like the sum of an empty series to be NaN, pass ``min_count=1``.\n |      \n |      >>> pd.Series([]).sum(min_count=1)\n |      nan\n |      \n |      Thanks to the ``skipna`` parameter, ``min_count`` handles all-NA and\n |      empty series identically.\n |      \n |      >>> pd.Series([np.nan]).sum()\n |      0.0\n |      \n |      >>> pd.Series([np.nan]).sum(min_count=1)\n |      nan\n |  \n |  swaplevel(self, i=-2, j=-1, axis=0)\n |      Swap levels i and j in a MultiIndex on a particular axis.\n |      \n |      Parameters\n |      ----------\n |      i, j : int, string (can be mixed)\n |          Level of index to be swapped. Can pass level name as string.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |      \n |      .. versionchanged:: 0.18.1\n |      \n |         The indexes ``i`` and ``j`` are now optional, and default to\n |         the two innermost levels of the index.\n |  \n |  to_dict(self, orient='dict', into=<class 'dict'>)\n |      Convert the DataFrame to a dictionary.\n |      \n |      The type of the key-value pairs can be customized with the parameters\n |      (see below).\n |      \n |      Parameters\n |      ----------\n |      orient : str {'dict', 'list', 'series', 'split', 'records', 'index'}\n |          Determines the type of the values of the dictionary.\n |      \n |          - 'dict' (default) : dict like {column -> {index -> value}}\n |          - 'list' : dict like {column -> [values]}\n |          - 'series' : dict like {column -> Series(values)}\n |          - 'split' : dict like\n |            {'index' -> [index], 'columns' -> [columns], 'data' -> [values]}\n |          - 'records' : list like\n |            [{column -> value}, ... , {column -> value}]\n |          - 'index' : dict like {index -> {column -> value}}\n |      \n |          Abbreviations are allowed. `s` indicates `series` and `sp`\n |          indicates `split`.\n |      \n |      into : class, default dict\n |          The collections.abc.Mapping subclass used for all Mappings\n |          in the return value.  Can be the actual class or an empty\n |          instance of the mapping type you want.  If you want a\n |          collections.defaultdict, you must pass it initialized.\n |      \n |          .. versionadded:: 0.21.0\n |      \n |      Returns\n |      -------\n |      dict, list or collections.abc.Mapping\n |          Return a collections.abc.Mapping object representing the DataFrame.\n |          The resulting transformation depends on the `orient` parameter.\n |      \n |      See Also\n |      --------\n |      DataFrame.from_dict: Create a DataFrame from a dictionary.\n |      DataFrame.to_json: Convert a DataFrame to JSON format.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'col1': [1, 2],\n |      ...                    'col2': [0.5, 0.75]},\n |      ...                   index=['row1', 'row2'])\n |      >>> df\n |            col1  col2\n |      row1     1  0.50\n |      row2     2  0.75\n |      >>> df.to_dict()\n |      {'col1': {'row1': 1, 'row2': 2}, 'col2': {'row1': 0.5, 'row2': 0.75}}\n |      \n |      You can specify the return orientation.\n |      \n |      >>> df.to_dict('series')\n |      {'col1': row1    1\n |               row2    2\n |      Name: col1, dtype: int64,\n |      'col2': row1    0.50\n |              row2    0.75\n |      Name: col2, dtype: float64}\n |      \n |      >>> df.to_dict('split')\n |      {'index': ['row1', 'row2'], 'columns': ['col1', 'col2'],\n |       'data': [[1, 0.5], [2, 0.75]]}\n |      \n |      >>> df.to_dict('records')\n |      [{'col1': 1, 'col2': 0.5}, {'col1': 2, 'col2': 0.75}]\n |      \n |      >>> df.to_dict('index')\n |      {'row1': {'col1': 1, 'col2': 0.5}, 'row2': {'col1': 2, 'col2': 0.75}}\n |      \n |      You can also specify the mapping type.\n |      \n |      >>> from collections import OrderedDict, defaultdict\n |      >>> df.to_dict(into=OrderedDict)\n |      OrderedDict([('col1', OrderedDict([('row1', 1), ('row2', 2)])),\n |                   ('col2', OrderedDict([('row1', 0.5), ('row2', 0.75)]))])\n |      \n |      If you want a `defaultdict`, you need to initialize it:\n |      \n |      >>> dd = defaultdict(list)\n |      >>> df.to_dict('records', into=dd)\n |      [defaultdict(<class 'list'>, {'col1': 1, 'col2': 0.5}),\n |       defaultdict(<class 'list'>, {'col1': 2, 'col2': 0.75})]\n |  \n |  to_feather(self, fname)\n |      Write out the binary feather-format for DataFrames.\n |      \n |      .. versionadded:: 0.20.0\n |      \n |      Parameters\n |      ----------\n |      fname : str\n |          string file path\n |  \n |  to_gbq(self, destination_table, project_id=None, chunksize=None, reauth=False, if_exists='fail', auth_local_webserver=False, table_schema=None, location=None, progress_bar=True, credentials=None, verbose=None, private_key=None)\n |      Write a DataFrame to a Google BigQuery table.\n |      \n |      This function requires the `pandas-gbq package\n |      <https://pandas-gbq.readthedocs.io>`__.\n |      \n |      See the `How to authenticate with Google BigQuery\n |      <https://pandas-gbq.readthedocs.io/en/latest/howto/authentication.html>`__\n |      guide for authentication instructions.\n |      \n |      Parameters\n |      ----------\n |      destination_table : str\n |          Name of table to be written, in the form ``dataset.tablename``.\n |      project_id : str, optional\n |          Google BigQuery Account project ID. Optional when available from\n |          the environment.\n |      chunksize : int, optional\n |          Number of rows to be inserted in each chunk from the dataframe.\n |          Set to ``None`` to load the whole dataframe at once.\n |      reauth : bool, default False\n |          Force Google BigQuery to re-authenticate the user. This is useful\n |          if multiple accounts are used.\n |      if_exists : str, default 'fail'\n |          Behavior when the destination table exists. Value can be one of:\n |      \n |          ``'fail'``\n |              If table exists, do nothing.\n |          ``'replace'``\n |              If table exists, drop it, recreate it, and insert data.\n |          ``'append'``\n |              If table exists, insert data. Create if does not exist.\n |      auth_local_webserver : bool, default False\n |          Use the `local webserver flow`_ instead of the `console flow`_\n |          when getting user credentials.\n |      \n |          .. _local webserver flow:\n |              http://google-auth-oauthlib.readthedocs.io/en/latest/reference/google_auth_oauthlib.flow.html#google_auth_oauthlib.flow.InstalledAppFlow.run_local_server\n |          .. _console flow:\n |              http://google-auth-oauthlib.readthedocs.io/en/latest/reference/google_auth_oauthlib.flow.html#google_auth_oauthlib.flow.InstalledAppFlow.run_console\n |      \n |          *New in version 0.2.0 of pandas-gbq*.\n |      table_schema : list of dicts, optional\n |          List of BigQuery table fields to which according DataFrame\n |          columns conform to, e.g. ``[{'name': 'col1', 'type':\n |          'STRING'},...]``. If schema is not provided, it will be\n |          generated according to dtypes of DataFrame columns. See\n |          BigQuery API documentation on available names of a field.\n |      \n |          *New in version 0.3.1 of pandas-gbq*.\n |      location : str, optional\n |          Location where the load job should run. See the `BigQuery locations\n |          documentation\n |          <https://cloud.google.com/bigquery/docs/dataset-locations>`__ for a\n |          list of available locations. The location must match that of the\n |          target dataset.\n |      \n |          *New in version 0.5.0 of pandas-gbq*.\n |      progress_bar : bool, default True\n |          Use the library `tqdm` to show the progress bar for the upload,\n |          chunk by chunk.\n |      \n |          *New in version 0.5.0 of pandas-gbq*.\n |      credentials : google.auth.credentials.Credentials, optional\n |          Credentials for accessing Google APIs. Use this parameter to\n |          override default credentials, such as to use Compute Engine\n |          :class:`google.auth.compute_engine.Credentials` or Service\n |          Account :class:`google.oauth2.service_account.Credentials`\n |          directly.\n |      \n |          *New in version 0.8.0 of pandas-gbq*.\n |      \n |          .. versionadded:: 0.24.0\n |      verbose : bool, deprecated\n |          Deprecated in pandas-gbq version 0.4.0. Use the `logging module\n |          to adjust verbosity instead\n |          <https://pandas-gbq.readthedocs.io/en/latest/intro.html#logging>`__.\n |      private_key : str, deprecated\n |          Deprecated in pandas-gbq version 0.8.0. Use the ``credentials``\n |          parameter and\n |          :func:`google.oauth2.service_account.Credentials.from_service_account_info`\n |          or\n |          :func:`google.oauth2.service_account.Credentials.from_service_account_file`\n |          instead.\n |      \n |          Service account private key in JSON format. Can be file path\n |          or string contents. This is useful for remote server\n |          authentication (eg. Jupyter/IPython notebook on remote host).\n |      \n |      See Also\n |      --------\n |      pandas_gbq.to_gbq : This function in the pandas-gbq library.\n |      read_gbq : Read a DataFrame from Google BigQuery.\n |  \n |  to_html(self, buf=None, columns=None, col_space=None, header=True, index=True, na_rep='NaN', formatters=None, float_format=None, sparsify=None, index_names=True, justify=None, max_rows=None, max_cols=None, show_dimensions=False, decimal='.', bold_rows=True, classes=None, escape=True, notebook=False, border=None, table_id=None, render_links=False)\n |      Render a DataFrame as an HTML table.\n |      \n |      Parameters\n |      ----------\n |      buf : StringIO-like, optional\n |          Buffer to write to.\n |      columns : sequence, optional, default None\n |          The subset of columns to write. Writes all columns by default.\n |      col_space : str or int, optional\n |          The minimum width of each column in CSS length units.  An int is assumed to be px units.\n |      \n |          .. versionadded:: 0.25.0\n |              Ability to use str.\n |      header : bool, optional\n |          Whether to print column labels, default True.\n |      index : bool, optional, default True\n |          Whether to print index (row) labels.\n |      na_rep : str, optional, default 'NaN'\n |          String representation of NAN to use.\n |      formatters : list or dict of one-param. functions, optional\n |          Formatter functions to apply to columns' elements by position or\n |          name.\n |          The result of each function must be a unicode string.\n |          List must be of length equal to the number of columns.\n |      float_format : one-parameter function, optional, default None\n |          Formatter function to apply to columns' elements if they are\n |          floats. The result of this function must be a unicode string.\n |      sparsify : bool, optional, default True\n |          Set to False for a DataFrame with a hierarchical index to print\n |          every multiindex key at each row.\n |      index_names : bool, optional, default True\n |          Prints the names of the indexes.\n |      justify : str, default None\n |          How to justify the column labels. If None uses the option from\n |          the print configuration (controlled by set_option), 'right' out\n |          of the box. Valid values are\n |      \n |          * left\n |          * right\n |          * center\n |          * justify\n |          * justify-all\n |          * start\n |          * end\n |          * inherit\n |          * match-parent\n |          * initial\n |          * unset.\n |      max_rows : int, optional\n |          Maximum number of rows to display in the console.\n |      min_rows : int, optional\n |          The number of rows to display in the console in a truncated repr\n |          (when number of rows is above `max_rows`).\n |      max_cols : int, optional\n |          Maximum number of columns to display in the console.\n |      show_dimensions : bool, default False\n |          Display DataFrame dimensions (number of rows by number of columns).\n |      decimal : str, default '.'\n |          Character recognized as decimal separator, e.g. ',' in Europe.\n |      \n |          .. versionadded:: 0.18.0\n |      \n |      bold_rows : bool, default True\n |          Make the row labels bold in the output.\n |      classes : str or list or tuple, default None\n |          CSS class(es) to apply to the resulting html table.\n |      escape : bool, default True\n |          Convert the characters <, >, and & to HTML-safe sequences.\n |      notebook : {True, False}, default False\n |          Whether the generated HTML is for IPython Notebook.\n |      border : int\n |          A ``border=border`` attribute is included in the opening\n |          `<table>` tag. Default ``pd.options.display.html.border``.\n |      \n |          .. versionadded:: 0.19.0\n |      \n |      table_id : str, optional\n |          A css id is included in the opening `<table>` tag if specified.\n |      \n |          .. versionadded:: 0.23.0\n |      \n |      render_links : bool, default False\n |          Convert URLs to HTML links.\n |      \n |          .. versionadded:: 0.24.0\n |      \n |      Returns\n |      -------\n |      str (or unicode, depending on data and options)\n |          String representation of the dataframe.\n |      \n |      See Also\n |      --------\n |      to_string : Convert DataFrame to a string.\n |  \n |  to_numpy(self, dtype=None, copy=False)\n |      Convert the DataFrame to a NumPy array.\n |      \n |      .. versionadded:: 0.24.0\n |      \n |      By default, the dtype of the returned array will be the common NumPy\n |      dtype of all types in the DataFrame. For example, if the dtypes are\n |      ``float16`` and ``float32``, the results dtype will be ``float32``.\n |      This may require copying data and coercing values, which may be\n |      expensive.\n |      \n |      Parameters\n |      ----------\n |      dtype : str or numpy.dtype, optional\n |          The dtype to pass to :meth:`numpy.asarray`\n |      copy : bool, default False\n |          Whether to ensure that the returned value is a not a view on\n |          another array. Note that ``copy=False`` does not *ensure* that\n |          ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that\n |          a copy is made, even if not strictly necessary.\n |      \n |      Returns\n |      -------\n |      numpy.ndarray\n |      \n |      See Also\n |      --------\n |      Series.to_numpy : Similar method for Series.\n |      \n |      Examples\n |      --------\n |      >>> pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]}).to_numpy()\n |      array([[1, 3],\n |             [2, 4]])\n |      \n |      With heterogenous data, the lowest common type will have to\n |      be used.\n |      \n |      >>> df = pd.DataFrame({\"A\": [1, 2], \"B\": [3.0, 4.5]})\n |      >>> df.to_numpy()\n |      array([[1. , 3. ],\n |             [2. , 4.5]])\n |      \n |      For a mix of numeric and non-numeric types, the output array will\n |      have object dtype.\n |      \n |      >>> df['C'] = pd.date_range('2000', periods=2)\n |      >>> df.to_numpy()\n |      array([[1, 3.0, Timestamp('2000-01-01 00:00:00')],\n |             [2, 4.5, Timestamp('2000-01-02 00:00:00')]], dtype=object)\n |  \n |  to_parquet(self, fname, engine='auto', compression='snappy', index=None, partition_cols=None, **kwargs)\n |      Write a DataFrame to the binary parquet format.\n |      \n |      .. versionadded:: 0.21.0\n |      \n |      This function writes the dataframe as a `parquet file\n |      <https://parquet.apache.org/>`_. You can choose different parquet\n |      backends, and have the option of compression. See\n |      :ref:`the user guide <io.parquet>` for more details.\n |      \n |      Parameters\n |      ----------\n |      fname : str\n |          File path or Root Directory path. Will be used as Root Directory\n |          path while writing a partitioned dataset.\n |      \n |          .. versionchanged:: 0.24.0\n |      \n |      engine : {'auto', 'pyarrow', 'fastparquet'}, default 'auto'\n |          Parquet library to use. If 'auto', then the option\n |          ``io.parquet.engine`` is used. The default ``io.parquet.engine``\n |          behavior is to try 'pyarrow', falling back to 'fastparquet' if\n |          'pyarrow' is unavailable.\n |      compression : {'snappy', 'gzip', 'brotli', None}, default 'snappy'\n |          Name of the compression to use. Use ``None`` for no compression.\n |      index : bool, default None\n |          If ``True``, include the dataframe's index(es) in the file output.\n |          If ``False``, they will not be written to the file. If ``None``,\n |          the behavior depends on the chosen engine.\n |      \n |          .. versionadded:: 0.24.0\n |      \n |      partition_cols : list, optional, default None\n |          Column names by which to partition the dataset\n |          Columns are partitioned in the order they are given\n |      \n |          .. versionadded:: 0.24.0\n |      \n |      **kwargs\n |          Additional arguments passed to the parquet library. See\n |          :ref:`pandas io <io.parquet>` for more details.\n |      \n |      See Also\n |      --------\n |      read_parquet : Read a parquet file.\n |      DataFrame.to_csv : Write a csv file.\n |      DataFrame.to_sql : Write to a sql table.\n |      DataFrame.to_hdf : Write to hdf.\n |      \n |      Notes\n |      -----\n |      This function requires either the `fastparquet\n |      <https://pypi.org/project/fastparquet>`_ or `pyarrow\n |      <https://arrow.apache.org/docs/python/>`_ library.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame(data={'col1': [1, 2], 'col2': [3, 4]})\n |      >>> df.to_parquet('df.parquet.gzip',\n |      ...               compression='gzip')  # doctest: +SKIP\n |      >>> pd.read_parquet('df.parquet.gzip')  # doctest: +SKIP\n |         col1  col2\n |      0     1     3\n |      1     2     4\n |  \n |  to_period(self, freq=None, axis=0, copy=True)\n |      Convert DataFrame from DatetimeIndex to PeriodIndex with desired\n |      frequency (inferred from index if not passed).\n |      \n |      Parameters\n |      ----------\n |      freq : str, default\n |          Frequency of the PeriodIndex.\n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          The axis to convert (the index by default).\n |      copy : bool, default True\n |          If False then underlying input data is not copied.\n |      \n |      Returns\n |      -------\n |      TimeSeries with PeriodIndex\n |  \n |  to_records(self, index=True, convert_datetime64=None, column_dtypes=None, index_dtypes=None)\n |      Convert DataFrame to a NumPy record array.\n |      \n |      Index will be included as the first field of the record array if\n |      requested.\n |      \n |      Parameters\n |      ----------\n |      index : bool, default True\n |          Include index in resulting record array, stored in 'index'\n |          field or using the index label, if set.\n |      convert_datetime64 : bool, default None\n |          .. deprecated:: 0.23.0\n |      \n |          Whether to convert the index to datetime.datetime if it is a\n |          DatetimeIndex.\n |      column_dtypes : str, type, dict, default None\n |          .. versionadded:: 0.24.0\n |      \n |          If a string or type, the data type to store all columns. If\n |          a dictionary, a mapping of column names and indices (zero-indexed)\n |          to specific data types.\n |      index_dtypes : str, type, dict, default None\n |          .. versionadded:: 0.24.0\n |      \n |          If a string or type, the data type to store all index levels. If\n |          a dictionary, a mapping of index level names and indices\n |          (zero-indexed) to specific data types.\n |      \n |          This mapping is applied only if `index=True`.\n |      \n |      Returns\n |      -------\n |      numpy.recarray\n |          NumPy ndarray with the DataFrame labels as fields and each row\n |          of the DataFrame as entries.\n |      \n |      See Also\n |      --------\n |      DataFrame.from_records: Convert structured or record ndarray\n |          to DataFrame.\n |      numpy.recarray: An ndarray that allows field access using\n |          attributes, analogous to typed columns in a\n |          spreadsheet.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'A': [1, 2], 'B': [0.5, 0.75]},\n |      ...                   index=['a', 'b'])\n |      >>> df\n |         A     B\n |      a  1  0.50\n |      b  2  0.75\n |      >>> df.to_records()\n |      rec.array([('a', 1, 0.5 ), ('b', 2, 0.75)],\n |                dtype=[('index', 'O'), ('A', '<i8'), ('B', '<f8')])\n |      \n |      If the DataFrame index has no label then the recarray field name\n |      is set to 'index'. If the index has a label then this is used as the\n |      field name:\n |      \n |      >>> df.index = df.index.rename(\"I\")\n |      >>> df.to_records()\n |      rec.array([('a', 1, 0.5 ), ('b', 2, 0.75)],\n |                dtype=[('I', 'O'), ('A', '<i8'), ('B', '<f8')])\n |      \n |      The index can be excluded from the record array:\n |      \n |      >>> df.to_records(index=False)\n |      rec.array([(1, 0.5 ), (2, 0.75)],\n |                dtype=[('A', '<i8'), ('B', '<f8')])\n |      \n |      Data types can be specified for the columns:\n |      \n |      >>> df.to_records(column_dtypes={\"A\": \"int32\"})\n |      rec.array([('a', 1, 0.5 ), ('b', 2, 0.75)],\n |                dtype=[('I', 'O'), ('A', '<i4'), ('B', '<f8')])\n |      \n |      As well as for the index:\n |      \n |      >>> df.to_records(index_dtypes=\"<S2\")\n |      rec.array([(b'a', 1, 0.5 ), (b'b', 2, 0.75)],\n |                dtype=[('I', 'S2'), ('A', '<i8'), ('B', '<f8')])\n |      \n |      >>> index_dtypes = \"<S{}\".format(df.index.str.len().max())\n |      >>> df.to_records(index_dtypes=index_dtypes)\n |      rec.array([(b'a', 1, 0.5 ), (b'b', 2, 0.75)],\n |                dtype=[('I', 'S1'), ('A', '<i8'), ('B', '<f8')])\n |  \n |  to_sparse(self, fill_value=None, kind='block')\n |      Convert to SparseDataFrame.\n |      \n |      .. deprecated:: 0.25.0\n |      \n |      Implement the sparse version of the DataFrame meaning that any data\n |      matching a specific value it's omitted in the representation.\n |      The sparse DataFrame allows for a more efficient storage.\n |      \n |      Parameters\n |      ----------\n |      fill_value : float, default None\n |          The specific value that should be omitted in the representation.\n |      kind : {'block', 'integer'}, default 'block'\n |          The kind of the SparseIndex tracking where data is not equal to\n |          the fill value:\n |      \n |          - 'block' tracks only the locations and sizes of blocks of data.\n |          - 'integer' keeps an array with all the locations of the data.\n |      \n |          In most cases 'block' is recommended, since it's more memory\n |          efficient.\n |      \n |      Returns\n |      -------\n |      SparseDataFrame\n |          The sparse representation of the DataFrame.\n |      \n |      See Also\n |      --------\n |      DataFrame.to_dense :\n |          Converts the DataFrame back to the its dense form.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame([(np.nan, np.nan),\n |      ...                    (1., np.nan),\n |      ...                    (np.nan, 1.)])\n |      >>> df\n |           0    1\n |      0  NaN  NaN\n |      1  1.0  NaN\n |      2  NaN  1.0\n |      >>> type(df)\n |      <class 'pandas.core.frame.DataFrame'>\n |      \n |      >>> sdf = df.to_sparse()  # doctest: +SKIP\n |      >>> sdf  # doctest: +SKIP\n |           0    1\n |      0  NaN  NaN\n |      1  1.0  NaN\n |      2  NaN  1.0\n |      >>> type(sdf)  # doctest: +SKIP\n |      <class 'pandas.core.sparse.frame.SparseDataFrame'>\n |  \n |  to_stata(self, fname, convert_dates=None, write_index=True, encoding='latin-1', byteorder=None, time_stamp=None, data_label=None, variable_labels=None, version=114, convert_strl=None)\n |      Export DataFrame object to Stata dta format.\n |      \n |      Writes the DataFrame to a Stata dataset file.\n |      \"dta\" files contain a Stata dataset.\n |      \n |      Parameters\n |      ----------\n |      fname : str, buffer or path object\n |          String, path object (pathlib.Path or py._path.local.LocalPath) or\n |          object implementing a binary write() function. If using a buffer\n |          then the buffer will not be automatically closed after the file\n |          data has been written.\n |      convert_dates : dict\n |          Dictionary mapping columns containing datetime types to stata\n |          internal format to use when writing the dates. Options are 'tc',\n |          'td', 'tm', 'tw', 'th', 'tq', 'ty'. Column can be either an integer\n |          or a name. Datetime columns that do not have a conversion type\n |          specified will be converted to 'tc'. Raises NotImplementedError if\n |          a datetime column has timezone information.\n |      write_index : bool\n |          Write the index to Stata dataset.\n |      encoding : str\n |          Default is latin-1. Unicode is not supported.\n |      byteorder : str\n |          Can be \">\", \"<\", \"little\", or \"big\". default is `sys.byteorder`.\n |      time_stamp : datetime\n |          A datetime to use as file creation date.  Default is the current\n |          time.\n |      data_label : str, optional\n |          A label for the data set.  Must be 80 characters or smaller.\n |      variable_labels : dict\n |          Dictionary containing columns as keys and variable labels as\n |          values. Each label must be 80 characters or smaller.\n |      \n |          .. versionadded:: 0.19.0\n |      \n |      version : {114, 117}, default 114\n |          Version to use in the output dta file.  Version 114 can be used\n |          read by Stata 10 and later.  Version 117 can be read by Stata 13\n |          or later. Version 114 limits string variables to 244 characters or\n |          fewer while 117 allows strings with lengths up to 2,000,000\n |          characters.\n |      \n |          .. versionadded:: 0.23.0\n |      \n |      convert_strl : list, optional\n |          List of column names to convert to string columns to Stata StrL\n |          format. Only available if version is 117.  Storing strings in the\n |          StrL format can produce smaller dta files if strings have more than\n |          8 characters and values are repeated.\n |      \n |          .. versionadded:: 0.23.0\n |      \n |      Raises\n |      ------\n |      NotImplementedError\n |          * If datetimes contain timezone information\n |          * Column dtype is not representable in Stata\n |      ValueError\n |          * Columns listed in convert_dates are neither datetime64[ns]\n |            or datetime.datetime\n |          * Column listed in convert_dates is not in DataFrame\n |          * Categorical label contains more than 32,000 characters\n |      \n |          .. versionadded:: 0.19.0\n |      \n |      See Also\n |      --------\n |      read_stata : Import Stata data files.\n |      io.stata.StataWriter : Low-level writer for Stata data files.\n |      io.stata.StataWriter117 : Low-level writer for version 117 files.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'animal': ['falcon', 'parrot', 'falcon',\n |      ...                               'parrot'],\n |      ...                    'speed': [350, 18, 361, 15]})\n |      >>> df.to_stata('animals.dta')  # doctest: +SKIP\n |  \n |  to_string(self, buf=None, columns=None, col_space=None, header=True, index=True, na_rep='NaN', formatters=None, float_format=None, sparsify=None, index_names=True, justify=None, max_rows=None, min_rows=None, max_cols=None, show_dimensions=False, decimal='.', line_width=None)\n |      Render a DataFrame to a console-friendly tabular output.\n |      \n |      Parameters\n |      ----------\n |      buf : StringIO-like, optional\n |          Buffer to write to.\n |      columns : sequence, optional, default None\n |          The subset of columns to write. Writes all columns by default.\n |      col_space : int, optional\n |          The minimum width of each column.\n |      header : bool, optional\n |          Write out the column names. If a list of strings is given, it is assumed to be aliases for the column names.\n |      index : bool, optional, default True\n |          Whether to print index (row) labels.\n |      na_rep : str, optional, default 'NaN'\n |          String representation of NAN to use.\n |      formatters : list or dict of one-param. functions, optional\n |          Formatter functions to apply to columns' elements by position or\n |          name.\n |          The result of each function must be a unicode string.\n |          List must be of length equal to the number of columns.\n |      float_format : one-parameter function, optional, default None\n |          Formatter function to apply to columns' elements if they are\n |          floats. The result of this function must be a unicode string.\n |      sparsify : bool, optional, default True\n |          Set to False for a DataFrame with a hierarchical index to print\n |          every multiindex key at each row.\n |      index_names : bool, optional, default True\n |          Prints the names of the indexes.\n |      justify : str, default None\n |          How to justify the column labels. If None uses the option from\n |          the print configuration (controlled by set_option), 'right' out\n |          of the box. Valid values are\n |      \n |          * left\n |          * right\n |          * center\n |          * justify\n |          * justify-all\n |          * start\n |          * end\n |          * inherit\n |          * match-parent\n |          * initial\n |          * unset.\n |      max_rows : int, optional\n |          Maximum number of rows to display in the console.\n |      min_rows : int, optional\n |          The number of rows to display in the console in a truncated repr\n |          (when number of rows is above `max_rows`).\n |      max_cols : int, optional\n |          Maximum number of columns to display in the console.\n |      show_dimensions : bool, default False\n |          Display DataFrame dimensions (number of rows by number of columns).\n |      decimal : str, default '.'\n |          Character recognized as decimal separator, e.g. ',' in Europe.\n |      \n |          .. versionadded:: 0.18.0\n |      \n |      line_width : int, optional\n |          Width to wrap a line in characters.\n |      \n |      Returns\n |      -------\n |      str (or unicode, depending on data and options)\n |          String representation of the dataframe.\n |      \n |      See Also\n |      --------\n |      to_html : Convert DataFrame to HTML.\n |      \n |      Examples\n |      --------\n |      >>> d = {'col1': [1, 2, 3], 'col2': [4, 5, 6]}\n |      >>> df = pd.DataFrame(d)\n |      >>> print(df.to_string())\n |         col1  col2\n |      0     1     4\n |      1     2     5\n |      2     3     6\n |  \n |  to_timestamp(self, freq=None, how='start', axis=0, copy=True)\n |      Cast to DatetimeIndex of timestamps, at *beginning* of period.\n |      \n |      Parameters\n |      ----------\n |      freq : str, default frequency of PeriodIndex\n |          Desired frequency.\n |      how : {'s', 'e', 'start', 'end'}\n |          Convention for converting period to timestamp; start of period\n |          vs. end.\n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          The axis to convert (the index by default).\n |      copy : bool, default True\n |          If False then underlying input data is not copied.\n |      \n |      Returns\n |      -------\n |      DataFrame with DatetimeIndex\n |  \n |  transform(self, func, axis=0, *args, **kwargs)\n |      Call ``func`` on self producing a DataFrame with transformed values\n |      and that has the same axis length as self.\n |      \n |      .. versionadded:: 0.20.0\n |      \n |      Parameters\n |      ----------\n |      func : function, str, list or dict\n |          Function to use for transforming the data. If a function, must either\n |          work when passed a DataFrame or when passed to DataFrame.apply.\n |      \n |          Accepted combinations are:\n |      \n |          - function\n |          - string function name\n |          - list of functions and/or function names, e.g. ``[np.exp. 'sqrt']``\n |          - dict of axis labels -> functions, function names or list of such.\n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          If 0 or 'index': apply function to each column.\n |          If 1 or 'columns': apply function to each row.\n |      *args\n |          Positional arguments to pass to `func`.\n |      **kwargs\n |          Keyword arguments to pass to `func`.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          A DataFrame that must have the same length as self.\n |      \n |      Raises\n |      ------\n |      ValueError : If the returned DataFrame has a different length than self.\n |      \n |      See Also\n |      --------\n |      DataFrame.agg : Only perform aggregating type operations.\n |      DataFrame.apply : Invoke function on a DataFrame.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'A': range(3), 'B': range(1, 4)})\n |      >>> df\n |         A  B\n |      0  0  1\n |      1  1  2\n |      2  2  3\n |      >>> df.transform(lambda x: x + 1)\n |         A  B\n |      0  1  2\n |      1  2  3\n |      2  3  4\n |      \n |      Even though the resulting DataFrame must have the same length as the\n |      input DataFrame, it is possible to provide several input functions:\n |      \n |      >>> s = pd.Series(range(3))\n |      >>> s\n |      0    0\n |      1    1\n |      2    2\n |      dtype: int64\n |      >>> s.transform([np.sqrt, np.exp])\n |             sqrt        exp\n |      0  0.000000   1.000000\n |      1  1.000000   2.718282\n |      2  1.414214   7.389056\n |  \n |  transpose(self, *args, **kwargs)\n |      Transpose index and columns.\n |      \n |      Reflect the DataFrame over its main diagonal by writing rows as columns\n |      and vice-versa. The property :attr:`.T` is an accessor to the method\n |      :meth:`transpose`.\n |      \n |      Parameters\n |      ----------\n |      copy : bool, default False\n |          If True, the underlying data is copied. Otherwise (default), no\n |          copy is made if possible.\n |      *args, **kwargs\n |          Additional keywords have no effect but might be accepted for\n |          compatibility with numpy.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          The transposed DataFrame.\n |      \n |      See Also\n |      --------\n |      numpy.transpose : Permute the dimensions of a given array.\n |      \n |      Notes\n |      -----\n |      Transposing a DataFrame with mixed dtypes will result in a homogeneous\n |      DataFrame with the `object` dtype. In such a case, a copy of the data\n |      is always made.\n |      \n |      Examples\n |      --------\n |      **Square DataFrame with homogeneous dtype**\n |      \n |      >>> d1 = {'col1': [1, 2], 'col2': [3, 4]}\n |      >>> df1 = pd.DataFrame(data=d1)\n |      >>> df1\n |         col1  col2\n |      0     1     3\n |      1     2     4\n |      \n |      >>> df1_transposed = df1.T # or df1.transpose()\n |      >>> df1_transposed\n |            0  1\n |      col1  1  2\n |      col2  3  4\n |      \n |      When the dtype is homogeneous in the original DataFrame, we get a\n |      transposed DataFrame with the same dtype:\n |      \n |      >>> df1.dtypes\n |      col1    int64\n |      col2    int64\n |      dtype: object\n |      >>> df1_transposed.dtypes\n |      0    int64\n |      1    int64\n |      dtype: object\n |      \n |      **Non-square DataFrame with mixed dtypes**\n |      \n |      >>> d2 = {'name': ['Alice', 'Bob'],\n |      ...       'score': [9.5, 8],\n |      ...       'employed': [False, True],\n |      ...       'kids': [0, 0]}\n |      >>> df2 = pd.DataFrame(data=d2)\n |      >>> df2\n |          name  score  employed  kids\n |      0  Alice    9.5     False     0\n |      1    Bob    8.0      True     0\n |      \n |      >>> df2_transposed = df2.T # or df2.transpose()\n |      >>> df2_transposed\n |                    0     1\n |      name      Alice   Bob\n |      score       9.5     8\n |      employed  False  True\n |      kids          0     0\n |      \n |      When the DataFrame has mixed dtypes, we get a transposed DataFrame with\n |      the `object` dtype:\n |      \n |      >>> df2.dtypes\n |      name         object\n |      score       float64\n |      employed       bool\n |      kids          int64\n |      dtype: object\n |      >>> df2_transposed.dtypes\n |      0    object\n |      1    object\n |      dtype: object\n |  \n |  truediv(self, other, axis='columns', level=None, fill_value=None)\n |      Get Floating division of dataframe and other, element-wise (binary operator `truediv`).\n |      \n |      Equivalent to ``dataframe / other``, but with support to substitute a fill_value\n |      for missing data in one of the inputs. With reverse version, `rtruediv`.\n |      \n |      Among flexible wrappers (`add`, `sub`, `mul`, `div`, `mod`, `pow`) to\n |      arithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n |      \n |      Parameters\n |      ----------\n |      other : scalar, sequence, Series, or DataFrame\n |          Any single or multiple element data structure, or list-like object.\n |      axis :  {0 or 'index', 1 or 'columns'}\n |          Whether to compare by the index (0 or 'index') or columns\n |          (1 or 'columns'). For Series input, axis to match Series index on.\n |      level : int or label\n |          Broadcast across a level, matching Index values on the\n |          passed MultiIndex level.\n |      fill_value : float or None, default None\n |          Fill existing missing (NaN) values, and any new element needed for\n |          successful DataFrame alignment, with this value before computation.\n |          If data in both corresponding DataFrame locations is missing\n |          the result will be missing.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          Result of the arithmetic operation.\n |      \n |      See Also\n |      --------\n |      DataFrame.add : Add DataFrames.\n |      DataFrame.sub : Subtract DataFrames.\n |      DataFrame.mul : Multiply DataFrames.\n |      DataFrame.div : Divide DataFrames (float division).\n |      DataFrame.truediv : Divide DataFrames (float division).\n |      DataFrame.floordiv : Divide DataFrames (integer division).\n |      DataFrame.mod : Calculate modulo (remainder after division).\n |      DataFrame.pow : Calculate exponential power.\n |      \n |      Notes\n |      -----\n |      Mismatched indices will be unioned together.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'angles': [0, 3, 4],\n |      ...                    'degrees': [360, 180, 360]},\n |      ...                   index=['circle', 'triangle', 'rectangle'])\n |      >>> df\n |                 angles  degrees\n |      circle          0      360\n |      triangle        3      180\n |      rectangle       4      360\n |      \n |      Add a scalar with operator version which return the same\n |      results.\n |      \n |      >>> df + 1\n |                 angles  degrees\n |      circle          1      361\n |      triangle        4      181\n |      rectangle       5      361\n |      \n |      >>> df.add(1)\n |                 angles  degrees\n |      circle          1      361\n |      triangle        4      181\n |      rectangle       5      361\n |      \n |      Divide by constant with reverse version.\n |      \n |      >>> df.div(10)\n |                 angles  degrees\n |      circle        0.0     36.0\n |      triangle      0.3     18.0\n |      rectangle     0.4     36.0\n |      \n |      >>> df.rdiv(10)\n |                   angles   degrees\n |      circle          inf  0.027778\n |      triangle   3.333333  0.055556\n |      rectangle  2.500000  0.027778\n |      \n |      Subtract a list and Series by axis with operator version.\n |      \n |      >>> df - [1, 2]\n |                 angles  degrees\n |      circle         -1      358\n |      triangle        2      178\n |      rectangle       3      358\n |      \n |      >>> df.sub([1, 2], axis='columns')\n |                 angles  degrees\n |      circle         -1      358\n |      triangle        2      178\n |      rectangle       3      358\n |      \n |      >>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n |      ...        axis='index')\n |                 angles  degrees\n |      circle         -1      359\n |      triangle        2      179\n |      rectangle       3      359\n |      \n |      Multiply a DataFrame of different shape with operator version.\n |      \n |      >>> other = pd.DataFrame({'angles': [0, 3, 4]},\n |      ...                      index=['circle', 'triangle', 'rectangle'])\n |      >>> other\n |                 angles\n |      circle          0\n |      triangle        3\n |      rectangle       4\n |      \n |      >>> df * other\n |                 angles  degrees\n |      circle          0      NaN\n |      triangle        9      NaN\n |      rectangle      16      NaN\n |      \n |      >>> df.mul(other, fill_value=0)\n |                 angles  degrees\n |      circle          0      0.0\n |      triangle        9      0.0\n |      rectangle      16      0.0\n |      \n |      Divide by a MultiIndex by level.\n |      \n |      >>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n |      ...                              'degrees': [360, 180, 360, 360, 540, 720]},\n |      ...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n |      ...                                    ['circle', 'triangle', 'rectangle',\n |      ...                                     'square', 'pentagon', 'hexagon']])\n |      >>> df_multindex\n |                   angles  degrees\n |      A circle          0      360\n |        triangle        3      180\n |        rectangle       4      360\n |      B square          4      360\n |        pentagon        5      540\n |        hexagon         6      720\n |      \n |      >>> df.div(df_multindex, level=1, fill_value=0)\n |                   angles  degrees\n |      A circle        NaN      1.0\n |        triangle      1.0      1.0\n |        rectangle     1.0      1.0\n |      B square        0.0      0.0\n |        pentagon      0.0      0.0\n |        hexagon       0.0      0.0\n |  \n |  unstack(self, level=-1, fill_value=None)\n |      Pivot a level of the (necessarily hierarchical) index labels, returning\n |      a DataFrame having a new level of column labels whose inner-most level\n |      consists of the pivoted index labels.\n |      \n |      If the index is not a MultiIndex, the output will be a Series\n |      (the analogue of stack when the columns are not a MultiIndex).\n |      \n |      The level involved will automatically get sorted.\n |      \n |      Parameters\n |      ----------\n |      level : int, string, or list of these, default -1 (last level)\n |          Level(s) of index to unstack, can pass level name\n |      fill_value : replace NaN with this value if the unstack produces\n |          missing values\n |      \n |          .. versionadded:: 0.18.0\n |      \n |      Returns\n |      -------\n |      Series or DataFrame\n |      \n |      See Also\n |      --------\n |      DataFrame.pivot : Pivot a table based on column values.\n |      DataFrame.stack : Pivot a level of the column labels (inverse operation\n |          from `unstack`).\n |      \n |      Examples\n |      --------\n |      >>> index = pd.MultiIndex.from_tuples([('one', 'a'), ('one', 'b'),\n |      ...                                    ('two', 'a'), ('two', 'b')])\n |      >>> s = pd.Series(np.arange(1.0, 5.0), index=index)\n |      >>> s\n |      one  a   1.0\n |           b   2.0\n |      two  a   3.0\n |           b   4.0\n |      dtype: float64\n |      \n |      >>> s.unstack(level=-1)\n |           a   b\n |      one  1.0  2.0\n |      two  3.0  4.0\n |      \n |      >>> s.unstack(level=0)\n |         one  two\n |      a  1.0   3.0\n |      b  2.0   4.0\n |      \n |      >>> df = s.unstack(level=0)\n |      >>> df.unstack()\n |      one  a  1.0\n |           b  2.0\n |      two  a  3.0\n |           b  4.0\n |      dtype: float64\n |  \n |  update(self, other, join='left', overwrite=True, filter_func=None, errors='ignore')\n |      Modify in place using non-NA values from another DataFrame.\n |      \n |      Aligns on indices. There is no return value.\n |      \n |      Parameters\n |      ----------\n |      other : DataFrame, or object coercible into a DataFrame\n |          Should have at least one matching index/column label\n |          with the original DataFrame. If a Series is passed,\n |          its name attribute must be set, and that will be\n |          used as the column name to align with the original DataFrame.\n |      join : {'left'}, default 'left'\n |          Only left join is implemented, keeping the index and columns of the\n |          original object.\n |      overwrite : bool, default True\n |          How to handle non-NA values for overlapping keys:\n |      \n |          * True: overwrite original DataFrame's values\n |            with values from `other`.\n |          * False: only update values that are NA in\n |            the original DataFrame.\n |      \n |      filter_func : callable(1d-array) -> bool 1d-array, optional\n |          Can choose to replace values other than NA. Return True for values\n |          that should be updated.\n |      errors : {'raise', 'ignore'}, default 'ignore'\n |          If 'raise', will raise a ValueError if the DataFrame and `other`\n |          both contain non-NA data in the same place.\n |      \n |          .. versionchanged :: 0.24.0\n |             Changed from `raise_conflict=False|True`\n |             to `errors='ignore'|'raise'`.\n |      \n |      Returns\n |      -------\n |      None : method directly changes calling object\n |      \n |      Raises\n |      ------\n |      ValueError\n |          * When `errors='raise'` and there's overlapping non-NA data.\n |          * When `errors` is not either `'ignore'` or `'raise'`\n |      NotImplementedError\n |          * If `join != 'left'`\n |      \n |      See Also\n |      --------\n |      dict.update : Similar method for dictionaries.\n |      DataFrame.merge : For column(s)-on-columns(s) operations.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'A': [1, 2, 3],\n |      ...                    'B': [400, 500, 600]})\n |      >>> new_df = pd.DataFrame({'B': [4, 5, 6],\n |      ...                        'C': [7, 8, 9]})\n |      >>> df.update(new_df)\n |      >>> df\n |         A  B\n |      0  1  4\n |      1  2  5\n |      2  3  6\n |      \n |      The DataFrame's length does not increase as a result of the update,\n |      only values at matching index/column labels are updated.\n |      \n |      >>> df = pd.DataFrame({'A': ['a', 'b', 'c'],\n |      ...                    'B': ['x', 'y', 'z']})\n |      >>> new_df = pd.DataFrame({'B': ['d', 'e', 'f', 'g', 'h', 'i']})\n |      >>> df.update(new_df)\n |      >>> df\n |         A  B\n |      0  a  d\n |      1  b  e\n |      2  c  f\n |      \n |      For Series, it's name attribute must be set.\n |      \n |      >>> df = pd.DataFrame({'A': ['a', 'b', 'c'],\n |      ...                    'B': ['x', 'y', 'z']})\n |      >>> new_column = pd.Series(['d', 'e'], name='B', index=[0, 2])\n |      >>> df.update(new_column)\n |      >>> df\n |         A  B\n |      0  a  d\n |      1  b  y\n |      2  c  e\n |      >>> df = pd.DataFrame({'A': ['a', 'b', 'c'],\n |      ...                    'B': ['x', 'y', 'z']})\n |      >>> new_df = pd.DataFrame({'B': ['d', 'e']}, index=[1, 2])\n |      >>> df.update(new_df)\n |      >>> df\n |         A  B\n |      0  a  x\n |      1  b  d\n |      2  c  e\n |      \n |      If `other` contains NaNs the corresponding values are not updated\n |      in the original dataframe.\n |      \n |      >>> df = pd.DataFrame({'A': [1, 2, 3],\n |      ...                    'B': [400, 500, 600]})\n |      >>> new_df = pd.DataFrame({'B': [4, np.nan, 6]})\n |      >>> df.update(new_df)\n |      >>> df\n |         A      B\n |      0  1    4.0\n |      1  2  500.0\n |      2  3    6.0\n |  \n |  var(self, axis=None, skipna=None, level=None, ddof=1, numeric_only=None, **kwargs)\n |      Return unbiased variance over requested axis.\n |      \n |      Normalized by N-1 by default. This can be changed using the ddof argument\n |      \n |      Parameters\n |      ----------\n |      axis : {index (0), columns (1)}\n |      skipna : bool, default True\n |          Exclude NA/null values. If an entire row/column is NA, the result\n |          will be NA\n |      level : int or level name, default None\n |          If the axis is a MultiIndex (hierarchical), count along a\n |          particular level, collapsing into a Series\n |      ddof : int, default 1\n |          Delta Degrees of Freedom. The divisor used in calculations is N - ddof,\n |          where N represents the number of elements.\n |      numeric_only : bool, default None\n |          Include only float, int, boolean columns. If None, will attempt to use\n |          everything, then use only numeric data. Not implemented for Series.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame (if level specified)\n |  \n |  ----------------------------------------------------------------------\n |  Class methods defined here:\n |  \n |  from_dict(data, orient='columns', dtype=None, columns=None) from builtins.type\n |      Construct DataFrame from dict of array-like or dicts.\n |      \n |      Creates DataFrame object from dictionary by columns or by index\n |      allowing dtype specification.\n |      \n |      Parameters\n |      ----------\n |      data : dict\n |          Of the form {field : array-like} or {field : dict}.\n |      orient : {'columns', 'index'}, default 'columns'\n |          The \"orientation\" of the data. If the keys of the passed dict\n |          should be the columns of the resulting DataFrame, pass 'columns'\n |          (default). Otherwise if the keys should be rows, pass 'index'.\n |      dtype : dtype, default None\n |          Data type to force, otherwise infer.\n |      columns : list, default None\n |          Column labels to use when ``orient='index'``. Raises a ValueError\n |          if used with ``orient='columns'``.\n |      \n |          .. versionadded:: 0.23.0\n |      \n |      Returns\n |      -------\n |      DataFrame\n |      \n |      See Also\n |      --------\n |      DataFrame.from_records : DataFrame from ndarray (structured\n |          dtype), list of tuples, dict, or DataFrame.\n |      DataFrame : DataFrame object creation using constructor.\n |      \n |      Examples\n |      --------\n |      By default the keys of the dict become the DataFrame columns:\n |      \n |      >>> data = {'col_1': [3, 2, 1, 0], 'col_2': ['a', 'b', 'c', 'd']}\n |      >>> pd.DataFrame.from_dict(data)\n |         col_1 col_2\n |      0      3     a\n |      1      2     b\n |      2      1     c\n |      3      0     d\n |      \n |      Specify ``orient='index'`` to create the DataFrame using dictionary\n |      keys as rows:\n |      \n |      >>> data = {'row_1': [3, 2, 1, 0], 'row_2': ['a', 'b', 'c', 'd']}\n |      >>> pd.DataFrame.from_dict(data, orient='index')\n |             0  1  2  3\n |      row_1  3  2  1  0\n |      row_2  a  b  c  d\n |      \n |      When using the 'index' orientation, the column names can be\n |      specified manually:\n |      \n |      >>> pd.DataFrame.from_dict(data, orient='index',\n |      ...                        columns=['A', 'B', 'C', 'D'])\n |             A  B  C  D\n |      row_1  3  2  1  0\n |      row_2  a  b  c  d\n |  \n |  from_items(items, columns=None, orient='columns') from builtins.type\n |      Construct a DataFrame from a list of tuples.\n |      \n |      .. deprecated:: 0.23.0\n |        `from_items` is deprecated and will be removed in a future version.\n |        Use :meth:`DataFrame.from_dict(dict(items)) <DataFrame.from_dict>`\n |        instead.\n |        :meth:`DataFrame.from_dict(OrderedDict(items)) <DataFrame.from_dict>`\n |        may be used to preserve the key order.\n |      \n |      Convert (key, value) pairs to DataFrame. The keys will be the axis\n |      index (usually the columns, but depends on the specified\n |      orientation). The values should be arrays or Series.\n |      \n |      Parameters\n |      ----------\n |      items : sequence of (key, value) pairs\n |          Values should be arrays or Series.\n |      columns : sequence of column labels, optional\n |          Must be passed if orient='index'.\n |      orient : {'columns', 'index'}, default 'columns'\n |          The \"orientation\" of the data. If the keys of the\n |          input correspond to column labels, pass 'columns'\n |          (default). Otherwise if the keys correspond to the index,\n |          pass 'index'.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |  \n |  from_records(data, index=None, exclude=None, columns=None, coerce_float=False, nrows=None) from builtins.type\n |      Convert structured or record ndarray to DataFrame.\n |      \n |      Parameters\n |      ----------\n |      data : ndarray (structured dtype), list of tuples, dict, or DataFrame\n |      index : string, list of fields, array-like\n |          Field of array to use as the index, alternately a specific set of\n |          input labels to use\n |      exclude : sequence, default None\n |          Columns or fields to exclude\n |      columns : sequence, default None\n |          Column names to use. If the passed data do not have names\n |          associated with them, this argument provides names for the\n |          columns. Otherwise this argument indicates the order of the columns\n |          in the result (any names not found in the data will become all-NA\n |          columns)\n |      coerce_float : boolean, default False\n |          Attempt to convert values of non-string, non-numeric objects (like\n |          decimal.Decimal) to floating point, useful for SQL result sets\n |      nrows : int, default None\n |          Number of rows to read if data is an iterator\n |      \n |      Returns\n |      -------\n |      DataFrame\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  T\n |      Transpose index and columns.\n |      \n |      Reflect the DataFrame over its main diagonal by writing rows as columns\n |      and vice-versa. The property :attr:`.T` is an accessor to the method\n |      :meth:`transpose`.\n |      \n |      Parameters\n |      ----------\n |      copy : bool, default False\n |          If True, the underlying data is copied. Otherwise (default), no\n |          copy is made if possible.\n |      *args, **kwargs\n |          Additional keywords have no effect but might be accepted for\n |          compatibility with numpy.\n |      \n |      Returns\n |      -------\n |      DataFrame\n |          The transposed DataFrame.\n |      \n |      See Also\n |      --------\n |      numpy.transpose : Permute the dimensions of a given array.\n |      \n |      Notes\n |      -----\n |      Transposing a DataFrame with mixed dtypes will result in a homogeneous\n |      DataFrame with the `object` dtype. In such a case, a copy of the data\n |      is always made.\n |      \n |      Examples\n |      --------\n |      **Square DataFrame with homogeneous dtype**\n |      \n |      >>> d1 = {'col1': [1, 2], 'col2': [3, 4]}\n |      >>> df1 = pd.DataFrame(data=d1)\n |      >>> df1\n |         col1  col2\n |      0     1     3\n |      1     2     4\n |      \n |      >>> df1_transposed = df1.T # or df1.transpose()\n |      >>> df1_transposed\n |            0  1\n |      col1  1  2\n |      col2  3  4\n |      \n |      When the dtype is homogeneous in the original DataFrame, we get a\n |      transposed DataFrame with the same dtype:\n |      \n |      >>> df1.dtypes\n |      col1    int64\n |      col2    int64\n |      dtype: object\n |      >>> df1_transposed.dtypes\n |      0    int64\n |      1    int64\n |      dtype: object\n |      \n |      **Non-square DataFrame with mixed dtypes**\n |      \n |      >>> d2 = {'name': ['Alice', 'Bob'],\n |      ...       'score': [9.5, 8],\n |      ...       'employed': [False, True],\n |      ...       'kids': [0, 0]}\n |      >>> df2 = pd.DataFrame(data=d2)\n |      >>> df2\n |          name  score  employed  kids\n |      0  Alice    9.5     False     0\n |      1    Bob    8.0      True     0\n |      \n |      >>> df2_transposed = df2.T # or df2.transpose()\n |      >>> df2_transposed\n |                    0     1\n |      name      Alice   Bob\n |      score       9.5     8\n |      employed  False  True\n |      kids          0     0\n |      \n |      When the DataFrame has mixed dtypes, we get a transposed DataFrame with\n |      the `object` dtype:\n |      \n |      >>> df2.dtypes\n |      name         object\n |      score       float64\n |      employed       bool\n |      kids          int64\n |      dtype: object\n |      >>> df2_transposed.dtypes\n |      0    object\n |      1    object\n |      dtype: object\n |  \n |  axes\n |      Return a list representing the axes of the DataFrame.\n |      \n |      It has the row axis labels and column axis labels as the only members.\n |      They are returned in that order.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n |      >>> df.axes\n |      [RangeIndex(start=0, stop=2, step=1), Index(['col1', 'col2'],\n |      dtype='object')]\n |  \n |  columns\n |      The column labels of the DataFrame.\n |  \n |  index\n |      The index (row labels) of the DataFrame.\n |  \n |  shape\n |      Return a tuple representing the dimensionality of the DataFrame.\n |      \n |      See Also\n |      --------\n |      ndarray.shape\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n |      >>> df.shape\n |      (2, 2)\n |      \n |      >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4],\n |      ...                    'col3': [5, 6]})\n |      >>> df.shape\n |      (2, 3)\n |  \n |  style\n |      Property returning a Styler object containing methods for\n |      building a styled HTML representation fo the DataFrame.\n |      \n |      See Also\n |      --------\n |      io.formats.style.Styler\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  plot = <class 'pandas.plotting._core.PlotAccessor'>\n |      Make plots of Series or DataFrame using the backend specified by the\n |      option ``plotting.backend``. By default, matplotlib is used.\n |      \n |      Parameters\n |      ----------\n |      data : Series or DataFrame\n |          The object for which the method is called\n |      x : label or position, default None\n |          Only used if data is a DataFrame.\n |      y : label, position or list of label, positions, default None\n |          Allows plotting of one column versus another. Only used if data is a\n |          DataFrame.\n |      kind : str\n |          - 'line' : line plot (default)\n |          - 'bar' : vertical bar plot\n |          - 'barh' : horizontal bar plot\n |          - 'hist' : histogram\n |          - 'box' : boxplot\n |          - 'kde' : Kernel Density Estimation plot\n |          - 'density' : same as 'kde'\n |          - 'area' : area plot\n |          - 'pie' : pie plot\n |          - 'scatter' : scatter plot\n |          - 'hexbin' : hexbin plot\n |      figsize : a tuple (width, height) in inches\n |      use_index : bool, default True\n |          Use index as ticks for x axis\n |      title : string or list\n |          Title to use for the plot. If a string is passed, print the string\n |          at the top of the figure. If a list is passed and `subplots` is\n |          True, print each item in the list above the corresponding subplot.\n |      grid : bool, default None (matlab style default)\n |          Axis grid lines\n |      legend : False/True/'reverse'\n |          Place legend on axis subplots\n |      style : list or dict\n |          matplotlib line style per column\n |      logx : bool or 'sym', default False\n |          Use log scaling or symlog scaling on x axis\n |          .. versionchanged:: 0.25.0\n |      \n |      logy : bool or 'sym' default False\n |          Use log scaling or symlog scaling on y axis\n |          .. versionchanged:: 0.25.0\n |      \n |      loglog : bool or 'sym', default False\n |          Use log scaling or symlog scaling on both x and y axes\n |          .. versionchanged:: 0.25.0\n |      \n |      xticks : sequence\n |          Values to use for the xticks\n |      yticks : sequence\n |          Values to use for the yticks\n |      xlim : 2-tuple/list\n |      ylim : 2-tuple/list\n |      rot : int, default None\n |          Rotation for ticks (xticks for vertical, yticks for horizontal\n |          plots)\n |      fontsize : int, default None\n |          Font size for xticks and yticks\n |      colormap : str or matplotlib colormap object, default None\n |          Colormap to select colors from. If string, load colormap with that\n |          name from matplotlib.\n |      colorbar : bool, optional\n |          If True, plot colorbar (only relevant for 'scatter' and 'hexbin'\n |          plots)\n |      position : float\n |          Specify relative alignments for bar plot layout.\n |          From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5\n |          (center)\n |      table : bool, Series or DataFrame, default False\n |          If True, draw a table using the data in the DataFrame and the data\n |          will be transposed to meet matplotlib's default layout.\n |          If a Series or DataFrame is passed, use passed data to draw a\n |          table.\n |      yerr : DataFrame, Series, array-like, dict and str\n |          See :ref:`Plotting with Error Bars <visualization.errorbars>` for\n |          detail.\n |      xerr : DataFrame, Series, array-like, dict and str\n |          Equivalent to yerr.\n |      mark_right : bool, default True\n |          When using a secondary_y axis, automatically mark the column\n |          labels with \"(right)\" in the legend\n |      `**kwds` : keywords\n |          Options to pass to matplotlib plotting method\n |      \n |      Returns\n |      -------\n |      :class:`matplotlib.axes.Axes` or numpy.ndarray of them\n |          If the backend is not the default matplotlib one, the return value\n |          will be the object returned by the backend.\n |      \n |      Notes\n |      -----\n |      - See matplotlib documentation online for more on this subject\n |      - If `kind` = 'bar' or 'barh', you can specify relative alignments\n |        for bar plot layout by `position` keyword.\n |        From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5\n |        (center)\n |  \n |  sparse = <class 'pandas.core.arrays.sparse.SparseFrameAccessor'>\n |      DataFrame accessor for sparse data.\n |      \n |      .. versionadded :: 0.25.0\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pandas.core.generic.NDFrame:\n |  \n |  __abs__(self)\n |  \n |  __array__(self, dtype=None)\n |  \n |  __array_wrap__(self, result, context=None)\n |  \n |  __bool__ = __nonzero__(self)\n |  \n |  __contains__(self, key)\n |      True if the key is in the info axis\n |  \n |  __copy__(self, deep=True)\n |  \n |  __deepcopy__(self, memo=None)\n |      Parameters\n |      ----------\n |      memo, default None\n |          Standard signature. Unused\n |  \n |  __delitem__(self, key)\n |      Delete item\n |  \n |  __finalize__(self, other, method=None, **kwargs)\n |      Propagate metadata from other to self.\n |      \n |      Parameters\n |      ----------\n |      other : the object from which to get the attributes that we are going\n |          to propagate\n |      method : optional, a passed method name ; possibly to take different\n |          types of propagation actions based on this\n |  \n |  __getattr__(self, name)\n |      After regular attribute access, try looking up the name\n |      This allows simpler access to columns for interactive use.\n |  \n |  __getstate__(self)\n |  \n |  __hash__(self)\n |      Return hash(self).\n |  \n |  __invert__(self)\n |  \n |  __iter__(self)\n |      Iterate over info axis.\n |      \n |      Returns\n |      -------\n |      iterator\n |          Info axis as iterator.\n |  \n |  __neg__(self)\n |  \n |  __nonzero__(self)\n |  \n |  __pos__(self)\n |  \n |  __round__(self, decimals=0)\n |  \n |  __setattr__(self, name, value)\n |      After regular attribute access, try setting the name\n |      This allows simpler access to columns for interactive use.\n |  \n |  __setstate__(self, state)\n |  \n |  abs(self)\n |      Return a Series/DataFrame with absolute numeric value of each element.\n |      \n |      This function only applies to elements that are all numeric.\n |      \n |      Returns\n |      -------\n |      abs\n |          Series/DataFrame containing the absolute value of each element.\n |      \n |      See Also\n |      --------\n |      numpy.absolute : Calculate the absolute value element-wise.\n |      \n |      Notes\n |      -----\n |      For ``complex`` inputs, ``1.2 + 1j``, the absolute value is\n |      :math:`\\sqrt{ a^2 + b^2 }`.\n |      \n |      Examples\n |      --------\n |      Absolute numeric values in a Series.\n |      \n |      >>> s = pd.Series([-1.10, 2, -3.33, 4])\n |      >>> s.abs()\n |      0    1.10\n |      1    2.00\n |      2    3.33\n |      3    4.00\n |      dtype: float64\n |      \n |      Absolute numeric values in a Series with complex numbers.\n |      \n |      >>> s = pd.Series([1.2 + 1j])\n |      >>> s.abs()\n |      0    1.56205\n |      dtype: float64\n |      \n |      Absolute numeric values in a Series with a Timedelta element.\n |      \n |      >>> s = pd.Series([pd.Timedelta('1 days')])\n |      >>> s.abs()\n |      0   1 days\n |      dtype: timedelta64[ns]\n |      \n |      Select rows with data closest to certain value using argsort (from\n |      `StackOverflow <https://stackoverflow.com/a/17758115>`__).\n |      \n |      >>> df = pd.DataFrame({\n |      ...     'a': [4, 5, 6, 7],\n |      ...     'b': [10, 20, 30, 40],\n |      ...     'c': [100, 50, -30, -50]\n |      ... })\n |      >>> df\n |           a    b    c\n |      0    4   10  100\n |      1    5   20   50\n |      2    6   30  -30\n |      3    7   40  -50\n |      >>> df.loc[(df.c - 43).abs().argsort()]\n |           a    b    c\n |      1    5   20   50\n |      0    4   10  100\n |      2    6   30  -30\n |      3    7   40  -50\n |  \n |  add_prefix(self, prefix)\n |      Prefix labels with string `prefix`.\n |      \n |      For Series, the row labels are prefixed.\n |      For DataFrame, the column labels are prefixed.\n |      \n |      Parameters\n |      ----------\n |      prefix : str\n |          The string to add before each label.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame\n |          New Series or DataFrame with updated labels.\n |      \n |      See Also\n |      --------\n |      Series.add_suffix: Suffix row labels with string `suffix`.\n |      DataFrame.add_suffix: Suffix column labels with string `suffix`.\n |      \n |      Examples\n |      --------\n |      >>> s = pd.Series([1, 2, 3, 4])\n |      >>> s\n |      0    1\n |      1    2\n |      2    3\n |      3    4\n |      dtype: int64\n |      \n |      >>> s.add_prefix('item_')\n |      item_0    1\n |      item_1    2\n |      item_2    3\n |      item_3    4\n |      dtype: int64\n |      \n |      >>> df = pd.DataFrame({'A': [1, 2, 3, 4],  'B': [3, 4, 5, 6]})\n |      >>> df\n |         A  B\n |      0  1  3\n |      1  2  4\n |      2  3  5\n |      3  4  6\n |      \n |      >>> df.add_prefix('col_')\n |           col_A  col_B\n |      0       1       3\n |      1       2       4\n |      2       3       5\n |      3       4       6\n |  \n |  add_suffix(self, suffix)\n |      Suffix labels with string `suffix`.\n |      \n |      For Series, the row labels are suffixed.\n |      For DataFrame, the column labels are suffixed.\n |      \n |      Parameters\n |      ----------\n |      suffix : str\n |          The string to add after each label.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame\n |          New Series or DataFrame with updated labels.\n |      \n |      See Also\n |      --------\n |      Series.add_prefix: Prefix row labels with string `prefix`.\n |      DataFrame.add_prefix: Prefix column labels with string `prefix`.\n |      \n |      Examples\n |      --------\n |      >>> s = pd.Series([1, 2, 3, 4])\n |      >>> s\n |      0    1\n |      1    2\n |      2    3\n |      3    4\n |      dtype: int64\n |      \n |      >>> s.add_suffix('_item')\n |      0_item    1\n |      1_item    2\n |      2_item    3\n |      3_item    4\n |      dtype: int64\n |      \n |      >>> df = pd.DataFrame({'A': [1, 2, 3, 4],  'B': [3, 4, 5, 6]})\n |      >>> df\n |         A  B\n |      0  1  3\n |      1  2  4\n |      2  3  5\n |      3  4  6\n |      \n |      >>> df.add_suffix('_col')\n |           A_col  B_col\n |      0       1       3\n |      1       2       4\n |      2       3       5\n |      3       4       6\n |  \n |  as_blocks(self, copy=True)\n |      Convert the frame to a dict of dtype -> Constructor Types that each has\n |      a homogeneous dtype.\n |      \n |      .. deprecated:: 0.21.0\n |      \n |      NOTE: the dtypes of the blocks WILL BE PRESERVED HERE (unlike in\n |            as_matrix)\n |      \n |      Parameters\n |      ----------\n |      copy : boolean, default True\n |      \n |      Returns\n |      -------\n |      values : a dict of dtype -> Constructor Types\n |  \n |  as_matrix(self, columns=None)\n |      Convert the frame to its Numpy-array representation.\n |      \n |      .. deprecated:: 0.23.0\n |          Use :meth:`DataFrame.values` instead.\n |      \n |      Parameters\n |      ----------\n |      columns : list, optional, default:None\n |          If None, return all columns, otherwise, returns specified columns.\n |      \n |      Returns\n |      -------\n |      values : ndarray\n |          If the caller is heterogeneous and contains booleans or objects,\n |          the result will be of dtype=object. See Notes.\n |      \n |      See Also\n |      --------\n |      DataFrame.values\n |      \n |      Notes\n |      -----\n |      Return is NOT a Numpy-matrix, rather, a Numpy-array.\n |      \n |      The dtype will be a lower-common-denominator dtype (implicit\n |      upcasting); that is to say if the dtypes (even of numeric types)\n |      are mixed, the one that accommodates all will be chosen. Use this\n |      with care if you are not dealing with the blocks.\n |      \n |      e.g. If the dtypes are float16 and float32, dtype will be upcast to\n |      float32.  If dtypes are int32 and uint8, dtype will be upcase to\n |      int32. By numpy.find_common_type convention, mixing int64 and uint64\n |      will result in a float64 dtype.\n |      \n |      This method is provided for backwards compatibility. Generally,\n |      it is recommended to use '.values'.\n |  \n |  asfreq(self, freq, method=None, how=None, normalize=False, fill_value=None)\n |      Convert TimeSeries to specified frequency.\n |      \n |      Optionally provide filling method to pad/backfill missing values.\n |      \n |      Returns the original data conformed to a new index with the specified\n |      frequency. ``resample`` is more appropriate if an operation, such as\n |      summarization, is necessary to represent the data at the new frequency.\n |      \n |      Parameters\n |      ----------\n |      freq : DateOffset object, or string\n |      method : {'backfill'/'bfill', 'pad'/'ffill'}, default None\n |          Method to use for filling holes in reindexed Series (note this\n |          does not fill NaNs that already were present):\n |      \n |          * 'pad' / 'ffill': propagate last valid observation forward to next\n |            valid\n |          * 'backfill' / 'bfill': use NEXT valid observation to fill\n |      how : {'start', 'end'}, default end\n |          For PeriodIndex only, see PeriodIndex.asfreq\n |      normalize : bool, default False\n |          Whether to reset output index to midnight\n |      fill_value : scalar, optional\n |          Value to use for missing values, applied during upsampling (note\n |          this does not fill NaNs that already were present).\n |      \n |          .. versionadded:: 0.20.0\n |      \n |      Returns\n |      -------\n |      converted : same type as caller\n |      \n |      See Also\n |      --------\n |      reindex\n |      \n |      Notes\n |      -----\n |      To learn more about the frequency strings, please see `this link\n |      <http://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__.\n |      \n |      Examples\n |      --------\n |      \n |      Start by creating a series with 4 one minute timestamps.\n |      \n |      >>> index = pd.date_range('1/1/2000', periods=4, freq='T')\n |      >>> series = pd.Series([0.0, None, 2.0, 3.0], index=index)\n |      >>> df = pd.DataFrame({'s':series})\n |      >>> df\n |                             s\n |      2000-01-01 00:00:00    0.0\n |      2000-01-01 00:01:00    NaN\n |      2000-01-01 00:02:00    2.0\n |      2000-01-01 00:03:00    3.0\n |      \n |      Upsample the series into 30 second bins.\n |      \n |      >>> df.asfreq(freq='30S')\n |                             s\n |      2000-01-01 00:00:00    0.0\n |      2000-01-01 00:00:30    NaN\n |      2000-01-01 00:01:00    NaN\n |      2000-01-01 00:01:30    NaN\n |      2000-01-01 00:02:00    2.0\n |      2000-01-01 00:02:30    NaN\n |      2000-01-01 00:03:00    3.0\n |      \n |      Upsample again, providing a ``fill value``.\n |      \n |      >>> df.asfreq(freq='30S', fill_value=9.0)\n |                             s\n |      2000-01-01 00:00:00    0.0\n |      2000-01-01 00:00:30    9.0\n |      2000-01-01 00:01:00    NaN\n |      2000-01-01 00:01:30    9.0\n |      2000-01-01 00:02:00    2.0\n |      2000-01-01 00:02:30    9.0\n |      2000-01-01 00:03:00    3.0\n |      \n |      Upsample again, providing a ``method``.\n |      \n |      >>> df.asfreq(freq='30S', method='bfill')\n |                             s\n |      2000-01-01 00:00:00    0.0\n |      2000-01-01 00:00:30    NaN\n |      2000-01-01 00:01:00    NaN\n |      2000-01-01 00:01:30    2.0\n |      2000-01-01 00:02:00    2.0\n |      2000-01-01 00:02:30    3.0\n |      2000-01-01 00:03:00    3.0\n |  \n |  asof(self, where, subset=None)\n |      Return the last row(s) without any NaNs before `where`.\n |      \n |      The last row (for each element in `where`, if list) without any\n |      NaN is taken.\n |      In case of a :class:`~pandas.DataFrame`, the last row without NaN\n |      considering only the subset of columns (if not `None`)\n |      \n |      .. versionadded:: 0.19.0 For DataFrame\n |      \n |      If there is no good value, NaN is returned for a Series or\n |      a Series of NaN values for a DataFrame\n |      \n |      Parameters\n |      ----------\n |      where : date or array-like of dates\n |          Date(s) before which the last row(s) are returned.\n |      subset : str or array-like of str, default `None`\n |          For DataFrame, if not `None`, only use these columns to\n |          check for NaNs.\n |      \n |      Returns\n |      -------\n |      scalar, Series, or DataFrame\n |      \n |          The return can be:\n |      \n |          * scalar : when `self` is a Series and `where` is a scalar\n |          * Series: when `self` is a Series and `where` is an array-like,\n |            or when `self` is a DataFrame and `where` is a scalar\n |          * DataFrame : when `self` is a DataFrame and `where` is an\n |            array-like\n |      \n |          Return scalar, Series, or DataFrame.\n |      \n |      See Also\n |      --------\n |      merge_asof : Perform an asof merge. Similar to left join.\n |      \n |      Notes\n |      -----\n |      Dates are assumed to be sorted. Raises if this is not the case.\n |      \n |      Examples\n |      --------\n |      A Series and a scalar `where`.\n |      \n |      >>> s = pd.Series([1, 2, np.nan, 4], index=[10, 20, 30, 40])\n |      >>> s\n |      10    1.0\n |      20    2.0\n |      30    NaN\n |      40    4.0\n |      dtype: float64\n |      \n |      >>> s.asof(20)\n |      2.0\n |      \n |      For a sequence `where`, a Series is returned. The first value is\n |      NaN, because the first element of `where` is before the first\n |      index value.\n |      \n |      >>> s.asof([5, 20])\n |      5     NaN\n |      20    2.0\n |      dtype: float64\n |      \n |      Missing values are not considered. The following is ``2.0``, not\n |      NaN, even though NaN is at the index location for ``30``.\n |      \n |      >>> s.asof(30)\n |      2.0\n |      \n |      Take all columns into consideration\n |      \n |      >>> df = pd.DataFrame({'a': [10, 20, 30, 40, 50],\n |      ...                    'b': [None, None, None, None, 500]},\n |      ...                   index=pd.DatetimeIndex(['2018-02-27 09:01:00',\n |      ...                                           '2018-02-27 09:02:00',\n |      ...                                           '2018-02-27 09:03:00',\n |      ...                                           '2018-02-27 09:04:00',\n |      ...                                           '2018-02-27 09:05:00']))\n |      >>> df.asof(pd.DatetimeIndex(['2018-02-27 09:03:30',\n |      ...                           '2018-02-27 09:04:30']))\n |                            a   b\n |      2018-02-27 09:03:30 NaN NaN\n |      2018-02-27 09:04:30 NaN NaN\n |      \n |      Take a single column into consideration\n |      \n |      >>> df.asof(pd.DatetimeIndex(['2018-02-27 09:03:30',\n |      ...                           '2018-02-27 09:04:30']),\n |      ...         subset=['a'])\n |                               a   b\n |      2018-02-27 09:03:30   30.0 NaN\n |      2018-02-27 09:04:30   40.0 NaN\n |  \n |  astype(self, dtype, copy=True, errors='raise', **kwargs)\n |      Cast a pandas object to a specified dtype ``dtype``.\n |      \n |      Parameters\n |      ----------\n |      dtype : data type, or dict of column name -> data type\n |          Use a numpy.dtype or Python type to cast entire pandas object to\n |          the same type. Alternatively, use {col: dtype, ...}, where col is a\n |          column label and dtype is a numpy.dtype or Python type to cast one\n |          or more of the DataFrame's columns to column-specific types.\n |      copy : bool, default True\n |          Return a copy when ``copy=True`` (be very careful setting\n |          ``copy=False`` as changes to values then may propagate to other\n |          pandas objects).\n |      errors : {'raise', 'ignore'}, default 'raise'\n |          Control raising of exceptions on invalid data for provided dtype.\n |      \n |          - ``raise`` : allow exceptions to be raised\n |          - ``ignore`` : suppress exceptions. On error return original object\n |      \n |          .. versionadded:: 0.20.0\n |      \n |      kwargs : keyword arguments to pass on to the constructor\n |      \n |      Returns\n |      -------\n |      casted : same type as caller\n |      \n |      See Also\n |      --------\n |      to_datetime : Convert argument to datetime.\n |      to_timedelta : Convert argument to timedelta.\n |      to_numeric : Convert argument to a numeric type.\n |      numpy.ndarray.astype : Cast a numpy array to a specified type.\n |      \n |      Examples\n |      --------\n |      Create a DataFrame:\n |      \n |      >>> d = {'col1': [1, 2], 'col2': [3, 4]}\n |      >>> df = pd.DataFrame(data=d)\n |      >>> df.dtypes\n |      col1    int64\n |      col2    int64\n |      dtype: object\n |      \n |      Cast all columns to int32:\n |      \n |      >>> df.astype('int32').dtypes\n |      col1    int32\n |      col2    int32\n |      dtype: object\n |      \n |      Cast col1 to int32 using a dictionary:\n |      \n |      >>> df.astype({'col1': 'int32'}).dtypes\n |      col1    int32\n |      col2    int64\n |      dtype: object\n |      \n |      Create a series:\n |      \n |      >>> ser = pd.Series([1, 2], dtype='int32')\n |      >>> ser\n |      0    1\n |      1    2\n |      dtype: int32\n |      >>> ser.astype('int64')\n |      0    1\n |      1    2\n |      dtype: int64\n |      \n |      Convert to categorical type:\n |      \n |      >>> ser.astype('category')\n |      0    1\n |      1    2\n |      dtype: category\n |      Categories (2, int64): [1, 2]\n |      \n |      Convert to ordered categorical type with custom ordering:\n |      \n |      >>> cat_dtype = pd.api.types.CategoricalDtype(\n |      ...                     categories=[2, 1], ordered=True)\n |      >>> ser.astype(cat_dtype)\n |      0    1\n |      1    2\n |      dtype: category\n |      Categories (2, int64): [2 < 1]\n |      \n |      Note that using ``copy=False`` and changing data on a new\n |      pandas object may propagate changes:\n |      \n |      >>> s1 = pd.Series([1,2])\n |      >>> s2 = s1.astype('int64', copy=False)\n |      >>> s2[0] = 10\n |      >>> s1  # note that s1[0] has changed too\n |      0    10\n |      1     2\n |      dtype: int64\n |  \n |  at_time(self, time, asof=False, axis=None)\n |      Select values at particular time of day (e.g. 9:30AM).\n |      \n |      Parameters\n |      ----------\n |      time : datetime.time or str\n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |      \n |          .. versionadded:: 0.24.0\n |      \n |      Returns\n |      -------\n |      Series or DataFrame\n |      \n |      Raises\n |      ------\n |      TypeError\n |          If the index is not  a :class:`DatetimeIndex`\n |      \n |      See Also\n |      --------\n |      between_time : Select values between particular times of the day.\n |      first : Select initial periods of time series based on a date offset.\n |      last : Select final periods of time series based on a date offset.\n |      DatetimeIndex.indexer_at_time : Get just the index locations for\n |          values at particular time of the day.\n |      \n |      Examples\n |      --------\n |      >>> i = pd.date_range('2018-04-09', periods=4, freq='12H')\n |      >>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)\n |      >>> ts\n |                           A\n |      2018-04-09 00:00:00  1\n |      2018-04-09 12:00:00  2\n |      2018-04-10 00:00:00  3\n |      2018-04-10 12:00:00  4\n |      \n |      >>> ts.at_time('12:00')\n |                           A\n |      2018-04-09 12:00:00  2\n |      2018-04-10 12:00:00  4\n |  \n |  between_time(self, start_time, end_time, include_start=True, include_end=True, axis=None)\n |      Select values between particular times of the day (e.g., 9:00-9:30 AM).\n |      \n |      By setting ``start_time`` to be later than ``end_time``,\n |      you can get the times that are *not* between the two times.\n |      \n |      Parameters\n |      ----------\n |      start_time : datetime.time or str\n |      end_time : datetime.time or str\n |      include_start : bool, default True\n |      include_end : bool, default True\n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |      \n |          .. versionadded:: 0.24.0\n |      \n |      Returns\n |      -------\n |      Series or DataFrame\n |      \n |      Raises\n |      ------\n |      TypeError\n |          If the index is not  a :class:`DatetimeIndex`\n |      \n |      See Also\n |      --------\n |      at_time : Select values at a particular time of the day.\n |      first : Select initial periods of time series based on a date offset.\n |      last : Select final periods of time series based on a date offset.\n |      DatetimeIndex.indexer_between_time : Get just the index locations for\n |          values between particular times of the day.\n |      \n |      Examples\n |      --------\n |      >>> i = pd.date_range('2018-04-09', periods=4, freq='1D20min')\n |      >>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)\n |      >>> ts\n |                           A\n |      2018-04-09 00:00:00  1\n |      2018-04-10 00:20:00  2\n |      2018-04-11 00:40:00  3\n |      2018-04-12 01:00:00  4\n |      \n |      >>> ts.between_time('0:15', '0:45')\n |                           A\n |      2018-04-10 00:20:00  2\n |      2018-04-11 00:40:00  3\n |      \n |      You get the times that are *not* between two times by setting\n |      ``start_time`` later than ``end_time``:\n |      \n |      >>> ts.between_time('0:45', '0:15')\n |                           A\n |      2018-04-09 00:00:00  1\n |      2018-04-12 01:00:00  4\n |  \n |  bfill(self, axis=None, inplace=False, limit=None, downcast=None)\n |      Synonym for :meth:`DataFrame.fillna` with ``method='bfill'``.\n |      \n |      Returns\n |      -------\n |      %(klass)s\n |          Object with missing values filled.\n |  \n |  bool(self)\n |      Return the bool of a single element PandasObject.\n |      \n |      This must be a boolean scalar value, either True or False.  Raise a\n |      ValueError if the PandasObject does not have exactly 1 element, or that\n |      element is not boolean\n |      \n |      Returns\n |      -------\n |      bool\n |          Same single boolean value converted to bool type.\n |  \n |  clip(self, lower=None, upper=None, axis=None, inplace=False, *args, **kwargs)\n |      Trim values at input threshold(s).\n |      \n |      Assigns values outside boundary to boundary values. Thresholds\n |      can be singular values or array like, and in the latter case\n |      the clipping is performed element-wise in the specified axis.\n |      \n |      Parameters\n |      ----------\n |      lower : float or array_like, default None\n |          Minimum threshold value. All values below this\n |          threshold will be set to it.\n |      upper : float or array_like, default None\n |          Maximum threshold value. All values above this\n |          threshold will be set to it.\n |      axis : int or str axis name, optional\n |          Align object with lower and upper along the given axis.\n |      inplace : bool, default False\n |          Whether to perform the operation in place on the data.\n |      \n |          .. versionadded:: 0.21.0\n |      *args, **kwargs\n |          Additional keywords have no effect but might be accepted\n |          for compatibility with numpy.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame\n |          Same type as calling object with the values outside the\n |          clip boundaries replaced.\n |      \n |      Examples\n |      --------\n |      >>> data = {'col_0': [9, -3, 0, -1, 5], 'col_1': [-2, -7, 6, 8, -5]}\n |      >>> df = pd.DataFrame(data)\n |      >>> df\n |         col_0  col_1\n |      0      9     -2\n |      1     -3     -7\n |      2      0      6\n |      3     -1      8\n |      4      5     -5\n |      \n |      Clips per column using lower and upper thresholds:\n |      \n |      >>> df.clip(-4, 6)\n |         col_0  col_1\n |      0      6     -2\n |      1     -3     -4\n |      2      0      6\n |      3     -1      6\n |      4      5     -4\n |      \n |      Clips using specific lower and upper thresholds per column element:\n |      \n |      >>> t = pd.Series([2, -4, -1, 6, 3])\n |      >>> t\n |      0    2\n |      1   -4\n |      2   -1\n |      3    6\n |      4    3\n |      dtype: int64\n |      \n |      >>> df.clip(t, t + 4, axis=0)\n |         col_0  col_1\n |      0      6      2\n |      1     -3     -4\n |      2      0      3\n |      3      6      8\n |      4      5      3\n |  \n |  clip_lower(self, threshold, axis=None, inplace=False)\n |      Trim values below a given threshold.\n |      \n |      .. deprecated:: 0.24.0\n |          Use clip(lower=threshold) instead.\n |      \n |      Elements below the `threshold` will be changed to match the\n |      `threshold` value(s). Threshold can be a single value or an array,\n |      in the latter case it performs the truncation element-wise.\n |      \n |      Parameters\n |      ----------\n |      threshold : numeric or array-like\n |          Minimum value allowed. All values below threshold will be set to\n |          this value.\n |      \n |          * float : every value is compared to `threshold`.\n |          * array-like : The shape of `threshold` should match the object\n |            it's compared to. When `self` is a Series, `threshold` should be\n |            the length. When `self` is a DataFrame, `threshold` should 2-D\n |            and the same shape as `self` for ``axis=None``, or 1-D and the\n |            same length as the axis being compared.\n |      \n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          Align `self` with `threshold` along the given axis.\n |      \n |      inplace : bool, default False\n |          Whether to perform the operation in place on the data.\n |      \n |          .. versionadded:: 0.21.0\n |      \n |      Returns\n |      -------\n |      Series or DataFrame\n |          Original data with values trimmed.\n |      \n |      See Also\n |      --------\n |      Series.clip : General purpose method to trim Series values to given\n |          threshold(s).\n |      DataFrame.clip : General purpose method to trim DataFrame values to\n |          given threshold(s).\n |      \n |      Examples\n |      --------\n |      \n |      Series single threshold clipping:\n |      \n |      >>> s = pd.Series([5, 6, 7, 8, 9])\n |      >>> s.clip(lower=8)\n |      0    8\n |      1    8\n |      2    8\n |      3    8\n |      4    9\n |      dtype: int64\n |      \n |      Series clipping element-wise using an array of thresholds. `threshold`\n |      should be the same length as the Series.\n |      \n |      >>> elemwise_thresholds = [4, 8, 7, 2, 5]\n |      >>> s.clip(lower=elemwise_thresholds)\n |      0    5\n |      1    8\n |      2    7\n |      3    8\n |      4    9\n |      dtype: int64\n |      \n |      DataFrames can be compared to a scalar.\n |      \n |      >>> df = pd.DataFrame({\"A\": [1, 3, 5], \"B\": [2, 4, 6]})\n |      >>> df\n |         A  B\n |      0  1  2\n |      1  3  4\n |      2  5  6\n |      \n |      >>> df.clip(lower=3)\n |         A  B\n |      0  3  3\n |      1  3  4\n |      2  5  6\n |      \n |      Or to an array of values. By default, `threshold` should be the same\n |      shape as the DataFrame.\n |      \n |      >>> df.clip(lower=np.array([[3, 4], [2, 2], [6, 2]]))\n |         A  B\n |      0  3  4\n |      1  3  4\n |      2  6  6\n |      \n |      Control how `threshold` is broadcast with `axis`. In this case\n |      `threshold` should be the same length as the axis specified by\n |      `axis`.\n |      \n |      >>> df.clip(lower=[3, 3, 5], axis='index')\n |         A  B\n |      0  3  3\n |      1  3  4\n |      2  5  6\n |      \n |      >>> df.clip(lower=[4, 5], axis='columns')\n |         A  B\n |      0  4  5\n |      1  4  5\n |      2  5  6\n |  \n |  clip_upper(self, threshold, axis=None, inplace=False)\n |      Trim values above a given threshold.\n |      \n |      .. deprecated:: 0.24.0\n |          Use clip(upper=threshold) instead.\n |      \n |      Elements above the `threshold` will be changed to match the\n |      `threshold` value(s). Threshold can be a single value or an array,\n |      in the latter case it performs the truncation element-wise.\n |      \n |      Parameters\n |      ----------\n |      threshold : numeric or array-like\n |          Maximum value allowed. All values above threshold will be set to\n |          this value.\n |      \n |          * float : every value is compared to `threshold`.\n |          * array-like : The shape of `threshold` should match the object\n |            it's compared to. When `self` is a Series, `threshold` should be\n |            the length. When `self` is a DataFrame, `threshold` should 2-D\n |            and the same shape as `self` for ``axis=None``, or 1-D and the\n |            same length as the axis being compared.\n |      \n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          Align object with `threshold` along the given axis.\n |      inplace : bool, default False\n |          Whether to perform the operation in place on the data.\n |      \n |          .. versionadded:: 0.21.0\n |      \n |      Returns\n |      -------\n |      Series or DataFrame\n |          Original data with values trimmed.\n |      \n |      See Also\n |      --------\n |      Series.clip : General purpose method to trim Series values to given\n |          threshold(s).\n |      DataFrame.clip : General purpose method to trim DataFrame values to\n |          given threshold(s).\n |      \n |      Examples\n |      --------\n |      >>> s = pd.Series([1, 2, 3, 4, 5])\n |      >>> s\n |      0    1\n |      1    2\n |      2    3\n |      3    4\n |      4    5\n |      dtype: int64\n |      \n |      >>> s.clip(upper=3)\n |      0    1\n |      1    2\n |      2    3\n |      3    3\n |      4    3\n |      dtype: int64\n |      \n |      >>> elemwise_thresholds = [5, 4, 3, 2, 1]\n |      >>> elemwise_thresholds\n |      [5, 4, 3, 2, 1]\n |      \n |      >>> s.clip(upper=elemwise_thresholds)\n |      0    1\n |      1    2\n |      2    3\n |      3    2\n |      4    1\n |      dtype: int64\n |  \n |  copy(self, deep=True)\n |      Make a copy of this object's indices and data.\n |      \n |      When ``deep=True`` (default), a new object will be created with a\n |      copy of the calling object's data and indices. Modifications to\n |      the data or indices of the copy will not be reflected in the\n |      original object (see notes below).\n |      \n |      When ``deep=False``, a new object will be created without copying\n |      the calling object's data or index (only references to the data\n |      and index are copied). Any changes to the data of the original\n |      will be reflected in the shallow copy (and vice versa).\n |      \n |      Parameters\n |      ----------\n |      deep : bool, default True\n |          Make a deep copy, including a copy of the data and the indices.\n |          With ``deep=False`` neither the indices nor the data are copied.\n |      \n |      Returns\n |      -------\n |      copy : Series or DataFrame\n |          Object type matches caller.\n |      \n |      Notes\n |      -----\n |      When ``deep=True``, data is copied but actual Python objects\n |      will not be copied recursively, only the reference to the object.\n |      This is in contrast to `copy.deepcopy` in the Standard Library,\n |      which recursively copies object data (see examples below).\n |      \n |      While ``Index`` objects are copied when ``deep=True``, the underlying\n |      numpy array is not copied for performance reasons. Since ``Index`` is\n |      immutable, the underlying data can be safely shared and a copy\n |      is not needed.\n |      \n |      Examples\n |      --------\n |      >>> s = pd.Series([1, 2], index=[\"a\", \"b\"])\n |      >>> s\n |      a    1\n |      b    2\n |      dtype: int64\n |      \n |      >>> s_copy = s.copy()\n |      >>> s_copy\n |      a    1\n |      b    2\n |      dtype: int64\n |      \n |      **Shallow copy versus default (deep) copy:**\n |      \n |      >>> s = pd.Series([1, 2], index=[\"a\", \"b\"])\n |      >>> deep = s.copy()\n |      >>> shallow = s.copy(deep=False)\n |      \n |      Shallow copy shares data and index with original.\n |      \n |      >>> s is shallow\n |      False\n |      >>> s.values is shallow.values and s.index is shallow.index\n |      True\n |      \n |      Deep copy has own copy of data and index.\n |      \n |      >>> s is deep\n |      False\n |      >>> s.values is deep.values or s.index is deep.index\n |      False\n |      \n |      Updates to the data shared by shallow copy and original is reflected\n |      in both; deep copy remains unchanged.\n |      \n |      >>> s[0] = 3\n |      >>> shallow[1] = 4\n |      >>> s\n |      a    3\n |      b    4\n |      dtype: int64\n |      >>> shallow\n |      a    3\n |      b    4\n |      dtype: int64\n |      >>> deep\n |      a    1\n |      b    2\n |      dtype: int64\n |      \n |      Note that when copying an object containing Python objects, a deep copy\n |      will copy the data, but will not do so recursively. Updating a nested\n |      data object will be reflected in the deep copy.\n |      \n |      >>> s = pd.Series([[1, 2], [3, 4]])\n |      >>> deep = s.copy()\n |      >>> s[0][0] = 10\n |      >>> s\n |      0    [10, 2]\n |      1     [3, 4]\n |      dtype: object\n |      >>> deep\n |      0    [10, 2]\n |      1     [3, 4]\n |      dtype: object\n |  \n |  describe(self, percentiles=None, include=None, exclude=None)\n |      Generate descriptive statistics that summarize the central tendency,\n |      dispersion and shape of a dataset's distribution, excluding\n |      ``NaN`` values.\n |      \n |      Analyzes both numeric and object series, as well\n |      as ``DataFrame`` column sets of mixed data types. The output\n |      will vary depending on what is provided. Refer to the notes\n |      below for more detail.\n |      \n |      Parameters\n |      ----------\n |      percentiles : list-like of numbers, optional\n |          The percentiles to include in the output. All should\n |          fall between 0 and 1. The default is\n |          ``[.25, .5, .75]``, which returns the 25th, 50th, and\n |          75th percentiles.\n |      include : 'all', list-like of dtypes or None (default), optional\n |          A white list of data types to include in the result. Ignored\n |          for ``Series``. Here are the options:\n |      \n |          - 'all' : All columns of the input will be included in the output.\n |          - A list-like of dtypes : Limits the results to the\n |            provided data types.\n |            To limit the result to numeric types submit\n |            ``numpy.number``. To limit it instead to object columns submit\n |            the ``numpy.object`` data type. Strings\n |            can also be used in the style of\n |            ``select_dtypes`` (e.g. ``df.describe(include=['O'])``). To\n |            select pandas categorical columns, use ``'category'``\n |          - None (default) : The result will include all numeric columns.\n |      exclude : list-like of dtypes or None (default), optional,\n |          A black list of data types to omit from the result. Ignored\n |          for ``Series``. Here are the options:\n |      \n |          - A list-like of dtypes : Excludes the provided data types\n |            from the result. To exclude numeric types submit\n |            ``numpy.number``. To exclude object columns submit the data\n |            type ``numpy.object``. Strings can also be used in the style of\n |            ``select_dtypes`` (e.g. ``df.describe(include=['O'])``). To\n |            exclude pandas categorical columns, use ``'category'``\n |          - None (default) : The result will exclude nothing.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame\n |          Summary statistics of the Series or Dataframe provided.\n |      \n |      See Also\n |      --------\n |      DataFrame.count: Count number of non-NA/null observations.\n |      DataFrame.max: Maximum of the values in the object.\n |      DataFrame.min: Minimum of the values in the object.\n |      DataFrame.mean: Mean of the values.\n |      DataFrame.std: Standard deviation of the observations.\n |      DataFrame.select_dtypes: Subset of a DataFrame including/excluding\n |          columns based on their dtype.\n |      \n |      Notes\n |      -----\n |      For numeric data, the result's index will include ``count``,\n |      ``mean``, ``std``, ``min``, ``max`` as well as lower, ``50`` and\n |      upper percentiles. By default the lower percentile is ``25`` and the\n |      upper percentile is ``75``. The ``50`` percentile is the\n |      same as the median.\n |      \n |      For object data (e.g. strings or timestamps), the result's index\n |      will include ``count``, ``unique``, ``top``, and ``freq``. The ``top``\n |      is the most common value. The ``freq`` is the most common value's\n |      frequency. Timestamps also include the ``first`` and ``last`` items.\n |      \n |      If multiple object values have the highest count, then the\n |      ``count`` and ``top`` results will be arbitrarily chosen from\n |      among those with the highest count.\n |      \n |      For mixed data types provided via a ``DataFrame``, the default is to\n |      return only an analysis of numeric columns. If the dataframe consists\n |      only of object and categorical data without any numeric columns, the\n |      default is to return an analysis of both the object and categorical\n |      columns. If ``include='all'`` is provided as an option, the result\n |      will include a union of attributes of each type.\n |      \n |      The `include` and `exclude` parameters can be used to limit\n |      which columns in a ``DataFrame`` are analyzed for the output.\n |      The parameters are ignored when analyzing a ``Series``.\n |      \n |      Examples\n |      --------\n |      Describing a numeric ``Series``.\n |      \n |      >>> s = pd.Series([1, 2, 3])\n |      >>> s.describe()\n |      count    3.0\n |      mean     2.0\n |      std      1.0\n |      min      1.0\n |      25%      1.5\n |      50%      2.0\n |      75%      2.5\n |      max      3.0\n |      dtype: float64\n |      \n |      Describing a categorical ``Series``.\n |      \n |      >>> s = pd.Series(['a', 'a', 'b', 'c'])\n |      >>> s.describe()\n |      count     4\n |      unique    3\n |      top       a\n |      freq      2\n |      dtype: object\n |      \n |      Describing a timestamp ``Series``.\n |      \n |      >>> s = pd.Series([\n |      ...   np.datetime64(\"2000-01-01\"),\n |      ...   np.datetime64(\"2010-01-01\"),\n |      ...   np.datetime64(\"2010-01-01\")\n |      ... ])\n |      >>> s.describe()\n |      count                       3\n |      unique                      2\n |      top       2010-01-01 00:00:00\n |      freq                        2\n |      first     2000-01-01 00:00:00\n |      last      2010-01-01 00:00:00\n |      dtype: object\n |      \n |      Describing a ``DataFrame``. By default only numeric fields\n |      are returned.\n |      \n |      >>> df = pd.DataFrame({'categorical': pd.Categorical(['d','e','f']),\n |      ...                    'numeric': [1, 2, 3],\n |      ...                    'object': ['a', 'b', 'c']\n |      ...                   })\n |      >>> df.describe()\n |             numeric\n |      count      3.0\n |      mean       2.0\n |      std        1.0\n |      min        1.0\n |      25%        1.5\n |      50%        2.0\n |      75%        2.5\n |      max        3.0\n |      \n |      Describing all columns of a ``DataFrame`` regardless of data type.\n |      \n |      >>> df.describe(include='all')\n |              categorical  numeric object\n |      count            3      3.0      3\n |      unique           3      NaN      3\n |      top              f      NaN      c\n |      freq             1      NaN      1\n |      mean           NaN      2.0    NaN\n |      std            NaN      1.0    NaN\n |      min            NaN      1.0    NaN\n |      25%            NaN      1.5    NaN\n |      50%            NaN      2.0    NaN\n |      75%            NaN      2.5    NaN\n |      max            NaN      3.0    NaN\n |      \n |      Describing a column from a ``DataFrame`` by accessing it as\n |      an attribute.\n |      \n |      >>> df.numeric.describe()\n |      count    3.0\n |      mean     2.0\n |      std      1.0\n |      min      1.0\n |      25%      1.5\n |      50%      2.0\n |      75%      2.5\n |      max      3.0\n |      Name: numeric, dtype: float64\n |      \n |      Including only numeric columns in a ``DataFrame`` description.\n |      \n |      >>> df.describe(include=[np.number])\n |             numeric\n |      count      3.0\n |      mean       2.0\n |      std        1.0\n |      min        1.0\n |      25%        1.5\n |      50%        2.0\n |      75%        2.5\n |      max        3.0\n |      \n |      Including only string columns in a ``DataFrame`` description.\n |      \n |      >>> df.describe(include=[np.object])\n |             object\n |      count       3\n |      unique      3\n |      top         c\n |      freq        1\n |      \n |      Including only categorical columns from a ``DataFrame`` description.\n |      \n |      >>> df.describe(include=['category'])\n |             categorical\n |      count            3\n |      unique           3\n |      top              f\n |      freq             1\n |      \n |      Excluding numeric columns from a ``DataFrame`` description.\n |      \n |      >>> df.describe(exclude=[np.number])\n |             categorical object\n |      count            3      3\n |      unique           3      3\n |      top              f      c\n |      freq             1      1\n |      \n |      Excluding object columns from a ``DataFrame`` description.\n |      \n |      >>> df.describe(exclude=[np.object])\n |             categorical  numeric\n |      count            3      3.0\n |      unique           3      NaN\n |      top              f      NaN\n |      freq             1      NaN\n |      mean           NaN      2.0\n |      std            NaN      1.0\n |      min            NaN      1.0\n |      25%            NaN      1.5\n |      50%            NaN      2.0\n |      75%            NaN      2.5\n |      max            NaN      3.0\n |  \n |  droplevel(self, level, axis=0)\n |      Return DataFrame with requested index / column level(s) removed.\n |      \n |      .. versionadded:: 0.24.0\n |      \n |      Parameters\n |      ----------\n |      level : int, str, or list-like\n |          If a string is given, must be the name of a level\n |          If list-like, elements must be names or positional indexes\n |          of levels.\n |      \n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |      \n |      Returns\n |      -------\n |      DataFrame.droplevel()\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame([\n |      ...     [1, 2, 3, 4],\n |      ...     [5, 6, 7, 8],\n |      ...     [9, 10, 11, 12]\n |      ... ]).set_index([0, 1]).rename_axis(['a', 'b'])\n |      \n |      >>> df.columns = pd.MultiIndex.from_tuples([\n |      ...    ('c', 'e'), ('d', 'f')\n |      ... ], names=['level_1', 'level_2'])\n |      \n |      >>> df\n |      level_1   c   d\n |      level_2   e   f\n |      a b\n |      1 2      3   4\n |      5 6      7   8\n |      9 10    11  12\n |      \n |      >>> df.droplevel('a')\n |      level_1   c   d\n |      level_2   e   f\n |      b\n |      2        3   4\n |      6        7   8\n |      10      11  12\n |      \n |      >>> df.droplevel('level2', axis=1)\n |      level_1   c   d\n |      a b\n |      1 2      3   4\n |      5 6      7   8\n |      9 10    11  12\n |  \n |  equals(self, other)\n |      Test whether two objects contain the same elements.\n |      \n |      This function allows two Series or DataFrames to be compared against\n |      each other to see if they have the same shape and elements. NaNs in\n |      the same location are considered equal. The column headers do not\n |      need to have the same type, but the elements within the columns must\n |      be the same dtype.\n |      \n |      Parameters\n |      ----------\n |      other : Series or DataFrame\n |          The other Series or DataFrame to be compared with the first.\n |      \n |      Returns\n |      -------\n |      bool\n |          True if all elements are the same in both objects, False\n |          otherwise.\n |      \n |      See Also\n |      --------\n |      Series.eq : Compare two Series objects of the same length\n |          and return a Series where each element is True if the element\n |          in each Series is equal, False otherwise.\n |      DataFrame.eq : Compare two DataFrame objects of the same shape and\n |          return a DataFrame where each element is True if the respective\n |          element in each DataFrame is equal, False otherwise.\n |      assert_series_equal : Return True if left and right Series are equal,\n |          False otherwise.\n |      assert_frame_equal : Return True if left and right DataFrames are\n |          equal, False otherwise.\n |      numpy.array_equal : Return True if two arrays have the same shape\n |          and elements, False otherwise.\n |      \n |      Notes\n |      -----\n |      This function requires that the elements have the same dtype as their\n |      respective elements in the other Series or DataFrame. However, the\n |      column labels do not need to have the same type, as long as they are\n |      still considered equal.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({1: [10], 2: [20]})\n |      >>> df\n |          1   2\n |      0  10  20\n |      \n |      DataFrames df and exactly_equal have the same types and values for\n |      their elements and column labels, which will return True.\n |      \n |      >>> exactly_equal = pd.DataFrame({1: [10], 2: [20]})\n |      >>> exactly_equal\n |          1   2\n |      0  10  20\n |      >>> df.equals(exactly_equal)\n |      True\n |      \n |      DataFrames df and different_column_type have the same element\n |      types and values, but have different types for the column labels,\n |      which will still return True.\n |      \n |      >>> different_column_type = pd.DataFrame({1.0: [10], 2.0: [20]})\n |      >>> different_column_type\n |         1.0  2.0\n |      0   10   20\n |      >>> df.equals(different_column_type)\n |      True\n |      \n |      DataFrames df and different_data_type have different types for the\n |      same values for their elements, and will return False even though\n |      their column labels are the same values and types.\n |      \n |      >>> different_data_type = pd.DataFrame({1: [10.0], 2: [20.0]})\n |      >>> different_data_type\n |            1     2\n |      0  10.0  20.0\n |      >>> df.equals(different_data_type)\n |      False\n |  \n |  ffill(self, axis=None, inplace=False, limit=None, downcast=None)\n |      Synonym for :meth:`DataFrame.fillna` with ``method='ffill'``.\n |      \n |      Returns\n |      -------\n |      %(klass)s\n |          Object with missing values filled.\n |  \n |  filter(self, items=None, like=None, regex=None, axis=None)\n |      Subset rows or columns of dataframe according to labels in\n |      the specified index.\n |      \n |      Note that this routine does not filter a dataframe on its\n |      contents. The filter is applied to the labels of the index.\n |      \n |      Parameters\n |      ----------\n |      items : list-like\n |          Keep labels from axis which are in items.\n |      like : string\n |          Keep labels from axis for which \"like in label == True\".\n |      regex : string (regular expression)\n |          Keep labels from axis for which re.search(regex, label) == True.\n |      axis : int or string axis name\n |          The axis to filter on.  By default this is the info axis,\n |          'index' for Series, 'columns' for DataFrame.\n |      \n |      Returns\n |      -------\n |      same type as input object\n |      \n |      See Also\n |      --------\n |      DataFrame.loc\n |      \n |      Notes\n |      -----\n |      The ``items``, ``like``, and ``regex`` parameters are\n |      enforced to be mutually exclusive.\n |      \n |      ``axis`` defaults to the info axis that is used when indexing\n |      with ``[]``.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame(np.array(([1, 2, 3], [4, 5, 6])),\n |      ...                   index=['mouse', 'rabbit'],\n |      ...                   columns=['one', 'two', 'three'])\n |      \n |      >>> # select columns by name\n |      >>> df.filter(items=['one', 'three'])\n |               one  three\n |      mouse     1      3\n |      rabbit    4      6\n |      \n |      >>> # select columns by regular expression\n |      >>> df.filter(regex='e$', axis=1)\n |               one  three\n |      mouse     1      3\n |      rabbit    4      6\n |      \n |      >>> # select rows containing 'bbi'\n |      >>> df.filter(like='bbi', axis=0)\n |               one  two  three\n |      rabbit    4    5      6\n |  \n |  first(self, offset)\n |      Convenience method for subsetting initial periods of time series data\n |      based on a date offset.\n |      \n |      Parameters\n |      ----------\n |      offset : string, DateOffset, dateutil.relativedelta\n |      \n |      Returns\n |      -------\n |      subset : same type as caller\n |      \n |      Raises\n |      ------\n |      TypeError\n |          If the index is not  a :class:`DatetimeIndex`\n |      \n |      See Also\n |      --------\n |      last : Select final periods of time series based on a date offset.\n |      at_time : Select values at a particular time of the day.\n |      between_time : Select values between particular times of the day.\n |      \n |      Examples\n |      --------\n |      >>> i = pd.date_range('2018-04-09', periods=4, freq='2D')\n |      >>> ts = pd.DataFrame({'A': [1,2,3,4]}, index=i)\n |      >>> ts\n |                  A\n |      2018-04-09  1\n |      2018-04-11  2\n |      2018-04-13  3\n |      2018-04-15  4\n |      \n |      Get the rows for the first 3 days:\n |      \n |      >>> ts.first('3D')\n |                  A\n |      2018-04-09  1\n |      2018-04-11  2\n |      \n |      Notice the data for 3 first calender days were returned, not the first\n |      3 days observed in the dataset, and therefore data for 2018-04-13 was\n |      not returned.\n |  \n |  first_valid_index(self)\n |      Return index for first non-NA/null value.\n |      \n |      Returns\n |      -------\n |      scalar : type of index\n |      \n |      Notes\n |      -----\n |      If all elements are non-NA/null, returns None.\n |      Also returns None for empty Series/DataFrame.\n |  \n |  get(self, key, default=None)\n |      Get item from object for given key (ex: DataFrame column).\n |      \n |      Returns default value if not found.\n |      \n |      Parameters\n |      ----------\n |      key : object\n |      \n |      Returns\n |      -------\n |      value : same type as items contained in object\n |  \n |  get_dtype_counts(self)\n |      Return counts of unique dtypes in this object.\n |      \n |      .. deprecated:: 0.25.0\n |      \n |      Use `.dtypes.value_counts()` instead.\n |      \n |      Returns\n |      -------\n |      dtype : Series\n |          Series with the count of columns with each dtype.\n |      \n |      See Also\n |      --------\n |      dtypes : Return the dtypes in this object.\n |      \n |      Examples\n |      --------\n |      >>> a = [['a', 1, 1.0], ['b', 2, 2.0], ['c', 3, 3.0]]\n |      >>> df = pd.DataFrame(a, columns=['str', 'int', 'float'])\n |      >>> df\n |        str  int  float\n |      0   a    1    1.0\n |      1   b    2    2.0\n |      2   c    3    3.0\n |      \n |      >>> df.get_dtype_counts()\n |      float64    1\n |      int64      1\n |      object     1\n |      dtype: int64\n |  \n |  get_ftype_counts(self)\n |      Return counts of unique ftypes in this object.\n |      \n |      .. deprecated:: 0.23.0\n |      \n |      This is useful for SparseDataFrame or for DataFrames containing\n |      sparse arrays.\n |      \n |      Returns\n |      -------\n |      dtype : Series\n |          Series with the count of columns with each type and\n |          sparsity (dense/sparse).\n |      \n |      See Also\n |      --------\n |      ftypes : Return ftypes (indication of sparse/dense and dtype) in\n |          this object.\n |      \n |      Examples\n |      --------\n |      >>> a = [['a', 1, 1.0], ['b', 2, 2.0], ['c', 3, 3.0]]\n |      >>> df = pd.DataFrame(a, columns=['str', 'int', 'float'])\n |      >>> df\n |        str  int  float\n |      0   a    1    1.0\n |      1   b    2    2.0\n |      2   c    3    3.0\n |      \n |      >>> df.get_ftype_counts()  # doctest: +SKIP\n |      float64:dense    1\n |      int64:dense      1\n |      object:dense     1\n |      dtype: int64\n |  \n |  get_values(self)\n |      Return an ndarray after converting sparse values to dense.\n |      \n |      .. deprecated:: 0.25.0\n |          Use ``np.asarray(..)`` or :meth:`DataFrame.values` instead.\n |      \n |      This is the same as ``.values`` for non-sparse data. For sparse\n |      data contained in a `SparseArray`, the data are first\n |      converted to a dense representation.\n |      \n |      Returns\n |      -------\n |      numpy.ndarray\n |          Numpy representation of DataFrame.\n |      \n |      See Also\n |      --------\n |      values : Numpy representation of DataFrame.\n |      SparseArray : Container for sparse data.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'a': [1, 2], 'b': [True, False],\n |      ...                    'c': [1.0, 2.0]})\n |      >>> df\n |         a      b    c\n |      0  1   True  1.0\n |      1  2  False  2.0\n |      \n |      >>> df.get_values()\n |      array([[1, True, 1.0], [2, False, 2.0]], dtype=object)\n |      \n |      >>> df = pd.DataFrame({\"a\": pd.SparseArray([1, None, None]),\n |      ...                    \"c\": [1.0, 2.0, 3.0]})\n |      >>> df\n |           a    c\n |      0  1.0  1.0\n |      1  NaN  2.0\n |      2  NaN  3.0\n |      \n |      >>> df.get_values()\n |      array([[ 1.,  1.],\n |             [nan,  2.],\n |             [nan,  3.]])\n |  \n |  groupby(self, by=None, axis=0, level=None, as_index=True, sort=True, group_keys=True, squeeze=False, observed=False, **kwargs)\n |      Group DataFrame or Series using a mapper or by a Series of columns.\n |      \n |      A groupby operation involves some combination of splitting the\n |      object, applying a function, and combining the results. This can be\n |      used to group large amounts of data and compute operations on these\n |      groups.\n |      \n |      Parameters\n |      ----------\n |      by : mapping, function, label, or list of labels\n |          Used to determine the groups for the groupby.\n |          If ``by`` is a function, it's called on each value of the object's\n |          index. If a dict or Series is passed, the Series or dict VALUES\n |          will be used to determine the groups (the Series' values are first\n |          aligned; see ``.align()`` method). If an ndarray is passed, the\n |          values are used as-is determine the groups. A label or list of\n |          labels may be passed to group by the columns in ``self``. Notice\n |          that a tuple is interpreted as a (single) key.\n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          Split along rows (0) or columns (1).\n |      level : int, level name, or sequence of such, default None\n |          If the axis is a MultiIndex (hierarchical), group by a particular\n |          level or levels.\n |      as_index : bool, default True\n |          For aggregated output, return object with group labels as the\n |          index. Only relevant for DataFrame input. as_index=False is\n |          effectively \"SQL-style\" grouped output.\n |      sort : bool, default True\n |          Sort group keys. Get better performance by turning this off.\n |          Note this does not influence the order of observations within each\n |          group. Groupby preserves the order of rows within each group.\n |      group_keys : bool, default True\n |          When calling apply, add group keys to index to identify pieces.\n |      squeeze : bool, default False\n |          Reduce the dimensionality of the return type if possible,\n |          otherwise return a consistent type.\n |      observed : bool, default False\n |          This only applies if any of the groupers are Categoricals.\n |          If True: only show observed values for categorical groupers.\n |          If False: show all values for categorical groupers.\n |      \n |          .. versionadded:: 0.23.0\n |      \n |      **kwargs\n |          Optional, only accepts keyword argument 'mutated' and is passed\n |          to groupby.\n |      \n |      Returns\n |      -------\n |      DataFrameGroupBy or SeriesGroupBy\n |          Depends on the calling object and returns groupby object that\n |          contains information about the groups.\n |      \n |      See Also\n |      --------\n |      resample : Convenience method for frequency conversion and resampling\n |          of time series.\n |      \n |      Notes\n |      -----\n |      See the `user guide\n |      <http://pandas.pydata.org/pandas-docs/stable/groupby.html>`_ for more.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'Animal': ['Falcon', 'Falcon',\n |      ...                               'Parrot', 'Parrot'],\n |      ...                    'Max Speed': [380., 370., 24., 26.]})\n |      >>> df\n |         Animal  Max Speed\n |      0  Falcon      380.0\n |      1  Falcon      370.0\n |      2  Parrot       24.0\n |      3  Parrot       26.0\n |      >>> df.groupby(['Animal']).mean()\n |              Max Speed\n |      Animal\n |      Falcon      375.0\n |      Parrot       25.0\n |      \n |      **Hierarchical Indexes**\n |      \n |      We can groupby different levels of a hierarchical index\n |      using the `level` parameter:\n |      \n |      >>> arrays = [['Falcon', 'Falcon', 'Parrot', 'Parrot'],\n |      ...           ['Captive', 'Wild', 'Captive', 'Wild']]\n |      >>> index = pd.MultiIndex.from_arrays(arrays, names=('Animal', 'Type'))\n |      >>> df = pd.DataFrame({'Max Speed': [390., 350., 30., 20.]},\n |      ...                   index=index)\n |      >>> df\n |                      Max Speed\n |      Animal Type\n |      Falcon Captive      390.0\n |             Wild         350.0\n |      Parrot Captive       30.0\n |             Wild          20.0\n |      >>> df.groupby(level=0).mean()\n |              Max Speed\n |      Animal\n |      Falcon      370.0\n |      Parrot       25.0\n |      >>> df.groupby(level=1).mean()\n |               Max Speed\n |      Type\n |      Captive      210.0\n |      Wild         185.0\n |  \n |  head(self, n=5)\n |      Return the first `n` rows.\n |      \n |      This function returns the first `n` rows for the object based\n |      on position. It is useful for quickly testing if your object\n |      has the right type of data in it.\n |      \n |      Parameters\n |      ----------\n |      n : int, default 5\n |          Number of rows to select.\n |      \n |      Returns\n |      -------\n |      obj_head : same type as caller\n |          The first `n` rows of the caller object.\n |      \n |      See Also\n |      --------\n |      DataFrame.tail: Returns the last `n` rows.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'animal':['alligator', 'bee', 'falcon', 'lion',\n |      ...                    'monkey', 'parrot', 'shark', 'whale', 'zebra']})\n |      >>> df\n |            animal\n |      0  alligator\n |      1        bee\n |      2     falcon\n |      3       lion\n |      4     monkey\n |      5     parrot\n |      6      shark\n |      7      whale\n |      8      zebra\n |      \n |      Viewing the first 5 lines\n |      \n |      >>> df.head()\n |            animal\n |      0  alligator\n |      1        bee\n |      2     falcon\n |      3       lion\n |      4     monkey\n |      \n |      Viewing the first `n` lines (three in this case)\n |      \n |      >>> df.head(3)\n |            animal\n |      0  alligator\n |      1        bee\n |      2     falcon\n |  \n |  infer_objects(self)\n |      Attempt to infer better dtypes for object columns.\n |      \n |      Attempts soft conversion of object-dtyped\n |      columns, leaving non-object and unconvertible\n |      columns unchanged. The inference rules are the\n |      same as during normal Series/DataFrame construction.\n |      \n |      .. versionadded:: 0.21.0\n |      \n |      Returns\n |      -------\n |      converted : same type as input object\n |      \n |      See Also\n |      --------\n |      to_datetime : Convert argument to datetime.\n |      to_timedelta : Convert argument to timedelta.\n |      to_numeric : Convert argument to numeric type.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({\"A\": [\"a\", 1, 2, 3]})\n |      >>> df = df.iloc[1:]\n |      >>> df\n |         A\n |      1  1\n |      2  2\n |      3  3\n |      \n |      >>> df.dtypes\n |      A    object\n |      dtype: object\n |      \n |      >>> df.infer_objects().dtypes\n |      A    int64\n |      dtype: object\n |  \n |  interpolate(self, method='linear', axis=0, limit=None, inplace=False, limit_direction='forward', limit_area=None, downcast=None, **kwargs)\n |      Interpolate values according to different methods.\n |      \n |      Please note that only ``method='linear'`` is supported for\n |      DataFrame/Series with a MultiIndex.\n |      \n |      Parameters\n |      ----------\n |      method : str, default 'linear'\n |          Interpolation technique to use. One of:\n |      \n |          * 'linear': Ignore the index and treat the values as equally\n |            spaced. This is the only method supported on MultiIndexes.\n |          * 'time': Works on daily and higher resolution data to interpolate\n |            given length of interval.\n |          * 'index', 'values': use the actual numerical values of the index.\n |          * 'pad': Fill in NaNs using existing values.\n |          * 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'spline',\n |            'barycentric', 'polynomial': Passed to\n |            `scipy.interpolate.interp1d`. These methods use the numerical\n |            values of the index.  Both 'polynomial' and 'spline' require that\n |            you also specify an `order` (int), e.g.\n |            ``df.interpolate(method='polynomial', order=5)``.\n |          * 'krogh', 'piecewise_polynomial', 'spline', 'pchip', 'akima':\n |            Wrappers around the SciPy interpolation methods of similar\n |            names. See `Notes`.\n |          * 'from_derivatives': Refers to\n |            `scipy.interpolate.BPoly.from_derivatives` which\n |            replaces 'piecewise_polynomial' interpolation method in\n |            scipy 0.18.\n |      \n |          .. versionadded:: 0.18.1\n |      \n |             Added support for the 'akima' method.\n |             Added interpolate method 'from_derivatives' which replaces\n |             'piecewise_polynomial' in SciPy 0.18; backwards-compatible with\n |             SciPy < 0.18\n |      \n |      axis : {0 or 'index', 1 or 'columns', None}, default None\n |          Axis to interpolate along.\n |      limit : int, optional\n |          Maximum number of consecutive NaNs to fill. Must be greater than\n |          0.\n |      inplace : bool, default False\n |          Update the data in place if possible.\n |      limit_direction : {'forward', 'backward', 'both'}, default 'forward'\n |          If limit is specified, consecutive NaNs will be filled in this\n |          direction.\n |      limit_area : {`None`, 'inside', 'outside'}, default None\n |          If limit is specified, consecutive NaNs will be filled with this\n |          restriction.\n |      \n |          * ``None``: No fill restriction.\n |          * 'inside': Only fill NaNs surrounded by valid values\n |            (interpolate).\n |          * 'outside': Only fill NaNs outside valid values (extrapolate).\n |      \n |          .. versionadded:: 0.23.0\n |      \n |      downcast : optional, 'infer' or None, defaults to None\n |          Downcast dtypes if possible.\n |      **kwargs\n |          Keyword arguments to pass on to the interpolating function.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame\n |          Returns the same object type as the caller, interpolated at\n |          some or all ``NaN`` values.\n |      \n |      See Also\n |      --------\n |      fillna : Fill missing values using different methods.\n |      scipy.interpolate.Akima1DInterpolator : Piecewise cubic polynomials\n |          (Akima interpolator).\n |      scipy.interpolate.BPoly.from_derivatives : Piecewise polynomial in the\n |          Bernstein basis.\n |      scipy.interpolate.interp1d : Interpolate a 1-D function.\n |      scipy.interpolate.KroghInterpolator : Interpolate polynomial (Krogh\n |          interpolator).\n |      scipy.interpolate.PchipInterpolator : PCHIP 1-d monotonic cubic\n |          interpolation.\n |      scipy.interpolate.CubicSpline : Cubic spline data interpolator.\n |      \n |      Notes\n |      -----\n |      The 'krogh', 'piecewise_polynomial', 'spline', 'pchip' and 'akima'\n |      methods are wrappers around the respective SciPy implementations of\n |      similar names. These use the actual numerical values of the index.\n |      For more information on their behavior, see the\n |      `SciPy documentation\n |      <http://docs.scipy.org/doc/scipy/reference/interpolate.html#univariate-interpolation>`__\n |      and `SciPy tutorial\n |      <http://docs.scipy.org/doc/scipy/reference/tutorial/interpolate.html>`__.\n |      \n |      Examples\n |      --------\n |      Filling in ``NaN`` in a :class:`~pandas.Series` via linear\n |      interpolation.\n |      \n |      >>> s = pd.Series([0, 1, np.nan, 3])\n |      >>> s\n |      0    0.0\n |      1    1.0\n |      2    NaN\n |      3    3.0\n |      dtype: float64\n |      >>> s.interpolate()\n |      0    0.0\n |      1    1.0\n |      2    2.0\n |      3    3.0\n |      dtype: float64\n |      \n |      Filling in ``NaN`` in a Series by padding, but filling at most two\n |      consecutive ``NaN`` at a time.\n |      \n |      >>> s = pd.Series([np.nan, \"single_one\", np.nan,\n |      ...                \"fill_two_more\", np.nan, np.nan, np.nan,\n |      ...                4.71, np.nan])\n |      >>> s\n |      0              NaN\n |      1       single_one\n |      2              NaN\n |      3    fill_two_more\n |      4              NaN\n |      5              NaN\n |      6              NaN\n |      7             4.71\n |      8              NaN\n |      dtype: object\n |      >>> s.interpolate(method='pad', limit=2)\n |      0              NaN\n |      1       single_one\n |      2       single_one\n |      3    fill_two_more\n |      4    fill_two_more\n |      5    fill_two_more\n |      6              NaN\n |      7             4.71\n |      8             4.71\n |      dtype: object\n |      \n |      Filling in ``NaN`` in a Series via polynomial interpolation or splines:\n |      Both 'polynomial' and 'spline' methods require that you also specify\n |      an ``order`` (int).\n |      \n |      >>> s = pd.Series([0, 2, np.nan, 8])\n |      >>> s.interpolate(method='polynomial', order=2)\n |      0    0.000000\n |      1    2.000000\n |      2    4.666667\n |      3    8.000000\n |      dtype: float64\n |      \n |      Fill the DataFrame forward (that is, going down) along each column\n |      using linear interpolation.\n |      \n |      Note how the last entry in column 'a' is interpolated differently,\n |      because there is no entry after it to use for interpolation.\n |      Note how the first entry in column 'b' remains ``NaN``, because there\n |      is no entry before it to use for interpolation.\n |      \n |      >>> df = pd.DataFrame([(0.0, np.nan, -1.0, 1.0),\n |      ...                    (np.nan, 2.0, np.nan, np.nan),\n |      ...                    (2.0, 3.0, np.nan, 9.0),\n |      ...                    (np.nan, 4.0, -4.0, 16.0)],\n |      ...                   columns=list('abcd'))\n |      >>> df\n |           a    b    c     d\n |      0  0.0  NaN -1.0   1.0\n |      1  NaN  2.0  NaN   NaN\n |      2  2.0  3.0  NaN   9.0\n |      3  NaN  4.0 -4.0  16.0\n |      >>> df.interpolate(method='linear', limit_direction='forward', axis=0)\n |           a    b    c     d\n |      0  0.0  NaN -1.0   1.0\n |      1  1.0  2.0 -2.0   5.0\n |      2  2.0  3.0 -3.0   9.0\n |      3  2.0  4.0 -4.0  16.0\n |      \n |      Using polynomial interpolation.\n |      \n |      >>> df['d'].interpolate(method='polynomial', order=2)\n |      0     1.0\n |      1     4.0\n |      2     9.0\n |      3    16.0\n |      Name: d, dtype: float64\n |  \n |  keys(self)\n |      Get the 'info axis' (see Indexing for more)\n |      \n |      This is index for Series, columns for DataFrame.\n |      \n |      Returns\n |      -------\n |      Index\n |          Info axis.\n |  \n |  last(self, offset)\n |      Convenience method for subsetting final periods of time series data\n |      based on a date offset.\n |      \n |      Parameters\n |      ----------\n |      offset : string, DateOffset, dateutil.relativedelta\n |      \n |      Returns\n |      -------\n |      subset : same type as caller\n |      \n |      Raises\n |      ------\n |      TypeError\n |          If the index is not  a :class:`DatetimeIndex`\n |      \n |      See Also\n |      --------\n |      first : Select initial periods of time series based on a date offset.\n |      at_time : Select values at a particular time of the day.\n |      between_time : Select values between particular times of the day.\n |      \n |      Examples\n |      --------\n |      >>> i = pd.date_range('2018-04-09', periods=4, freq='2D')\n |      >>> ts = pd.DataFrame({'A': [1,2,3,4]}, index=i)\n |      >>> ts\n |                  A\n |      2018-04-09  1\n |      2018-04-11  2\n |      2018-04-13  3\n |      2018-04-15  4\n |      \n |      Get the rows for the last 3 days:\n |      \n |      >>> ts.last('3D')\n |                  A\n |      2018-04-13  3\n |      2018-04-15  4\n |      \n |      Notice the data for 3 last calender days were returned, not the last\n |      3 observed days in the dataset, and therefore data for 2018-04-11 was\n |      not returned.\n |  \n |  last_valid_index(self)\n |      Return index for last non-NA/null value.\n |      \n |      Returns\n |      -------\n |      scalar : type of index\n |      \n |      Notes\n |      -----\n |      If all elements are non-NA/null, returns None.\n |      Also returns None for empty Series/DataFrame.\n |  \n |  mask(self, cond, other=nan, inplace=False, axis=None, level=None, errors='raise', try_cast=False)\n |      Replace values where the condition is True.\n |      \n |      Parameters\n |      ----------\n |      cond : boolean Series/DataFrame, array-like, or callable\n |          Where `cond` is False, keep the original value. Where\n |          True, replace with corresponding value from `other`.\n |          If `cond` is callable, it is computed on the Series/DataFrame and\n |          should return boolean Series/DataFrame or array. The callable must\n |          not change input Series/DataFrame (though pandas doesn't check it).\n |      \n |          .. versionadded:: 0.18.1\n |              A callable can be used as cond.\n |      \n |      other : scalar, Series/DataFrame, or callable\n |          Entries where `cond` is True are replaced with\n |          corresponding value from `other`.\n |          If other is callable, it is computed on the Series/DataFrame and\n |          should return scalar or Series/DataFrame. The callable must not\n |          change input Series/DataFrame (though pandas doesn't check it).\n |      \n |          .. versionadded:: 0.18.1\n |              A callable can be used as other.\n |      \n |      inplace : bool, default False\n |          Whether to perform the operation in place on the data.\n |      axis : int, default None\n |          Alignment axis if needed.\n |      level : int, default None\n |          Alignment level if needed.\n |      errors : str, {'raise', 'ignore'}, default 'raise'\n |          Note that currently this parameter won't affect\n |          the results and will always coerce to a suitable dtype.\n |      \n |          - 'raise' : allow exceptions to be raised.\n |          - 'ignore' : suppress exceptions. On error return original object.\n |      \n |      try_cast : bool, default False\n |          Try to cast the result back to the input type (if possible).\n |      \n |      Returns\n |      -------\n |      Same type as caller\n |      \n |      See Also\n |      --------\n |      :func:`DataFrame.where` : Return an object of same shape as\n |          self.\n |      \n |      Notes\n |      -----\n |      The mask method is an application of the if-then idiom. For each\n |      element in the calling DataFrame, if ``cond`` is ``False`` the\n |      element is used; otherwise the corresponding element from the DataFrame\n |      ``other`` is used.\n |      \n |      The signature for :func:`DataFrame.where` differs from\n |      :func:`numpy.where`. Roughly ``df1.where(m, df2)`` is equivalent to\n |      ``np.where(m, df1, df2)``.\n |      \n |      For further details and examples see the ``mask`` documentation in\n |      :ref:`indexing <indexing.where_mask>`.\n |      \n |      Examples\n |      --------\n |      >>> s = pd.Series(range(5))\n |      >>> s.where(s > 0)\n |      0    NaN\n |      1    1.0\n |      2    2.0\n |      3    3.0\n |      4    4.0\n |      dtype: float64\n |      \n |      >>> s.mask(s > 0)\n |      0    0.0\n |      1    NaN\n |      2    NaN\n |      3    NaN\n |      4    NaN\n |      dtype: float64\n |      \n |      >>> s.where(s > 1, 10)\n |      0    10\n |      1    10\n |      2    2\n |      3    3\n |      4    4\n |      dtype: int64\n |      \n |      >>> df = pd.DataFrame(np.arange(10).reshape(-1, 2), columns=['A', 'B'])\n |      >>> df\n |         A  B\n |      0  0  1\n |      1  2  3\n |      2  4  5\n |      3  6  7\n |      4  8  9\n |      >>> m = df % 3 == 0\n |      >>> df.where(m, -df)\n |         A  B\n |      0  0 -1\n |      1 -2  3\n |      2 -4 -5\n |      3  6 -7\n |      4 -8  9\n |      >>> df.where(m, -df) == np.where(m, df, -df)\n |            A     B\n |      0  True  True\n |      1  True  True\n |      2  True  True\n |      3  True  True\n |      4  True  True\n |      >>> df.where(m, -df) == df.mask(~m, -df)\n |            A     B\n |      0  True  True\n |      1  True  True\n |      2  True  True\n |      3  True  True\n |      4  True  True\n |  \n |  pct_change(self, periods=1, fill_method='pad', limit=None, freq=None, **kwargs)\n |      Percentage change between the current and a prior element.\n |      \n |      Computes the percentage change from the immediately previous row by\n |      default. This is useful in comparing the percentage of change in a time\n |      series of elements.\n |      \n |      Parameters\n |      ----------\n |      periods : int, default 1\n |          Periods to shift for forming percent change.\n |      fill_method : str, default 'pad'\n |          How to handle NAs before computing percent changes.\n |      limit : int, default None\n |          The number of consecutive NAs to fill before stopping.\n |      freq : DateOffset, timedelta, or offset alias string, optional\n |          Increment to use from time series API (e.g. 'M' or BDay()).\n |      **kwargs\n |          Additional keyword arguments are passed into\n |          `DataFrame.shift` or `Series.shift`.\n |      \n |      Returns\n |      -------\n |      chg : Series or DataFrame\n |          The same type as the calling object.\n |      \n |      See Also\n |      --------\n |      Series.diff : Compute the difference of two elements in a Series.\n |      DataFrame.diff : Compute the difference of two elements in a DataFrame.\n |      Series.shift : Shift the index by some number of periods.\n |      DataFrame.shift : Shift the index by some number of periods.\n |      \n |      Examples\n |      --------\n |      **Series**\n |      \n |      >>> s = pd.Series([90, 91, 85])\n |      >>> s\n |      0    90\n |      1    91\n |      2    85\n |      dtype: int64\n |      \n |      >>> s.pct_change()\n |      0         NaN\n |      1    0.011111\n |      2   -0.065934\n |      dtype: float64\n |      \n |      >>> s.pct_change(periods=2)\n |      0         NaN\n |      1         NaN\n |      2   -0.055556\n |      dtype: float64\n |      \n |      See the percentage change in a Series where filling NAs with last\n |      valid observation forward to next valid.\n |      \n |      >>> s = pd.Series([90, 91, None, 85])\n |      >>> s\n |      0    90.0\n |      1    91.0\n |      2     NaN\n |      3    85.0\n |      dtype: float64\n |      \n |      >>> s.pct_change(fill_method='ffill')\n |      0         NaN\n |      1    0.011111\n |      2    0.000000\n |      3   -0.065934\n |      dtype: float64\n |      \n |      **DataFrame**\n |      \n |      Percentage change in French franc, Deutsche Mark, and Italian lira from\n |      1980-01-01 to 1980-03-01.\n |      \n |      >>> df = pd.DataFrame({\n |      ...     'FR': [4.0405, 4.0963, 4.3149],\n |      ...     'GR': [1.7246, 1.7482, 1.8519],\n |      ...     'IT': [804.74, 810.01, 860.13]},\n |      ...     index=['1980-01-01', '1980-02-01', '1980-03-01'])\n |      >>> df\n |                      FR      GR      IT\n |      1980-01-01  4.0405  1.7246  804.74\n |      1980-02-01  4.0963  1.7482  810.01\n |      1980-03-01  4.3149  1.8519  860.13\n |      \n |      >>> df.pct_change()\n |                        FR        GR        IT\n |      1980-01-01       NaN       NaN       NaN\n |      1980-02-01  0.013810  0.013684  0.006549\n |      1980-03-01  0.053365  0.059318  0.061876\n |      \n |      Percentage of change in GOOG and APPL stock volume. Shows computing\n |      the percentage change between columns.\n |      \n |      >>> df = pd.DataFrame({\n |      ...     '2016': [1769950, 30586265],\n |      ...     '2015': [1500923, 40912316],\n |      ...     '2014': [1371819, 41403351]},\n |      ...     index=['GOOG', 'APPL'])\n |      >>> df\n |                2016      2015      2014\n |      GOOG   1769950   1500923   1371819\n |      APPL  30586265  40912316  41403351\n |      \n |      >>> df.pct_change(axis='columns')\n |            2016      2015      2014\n |      GOOG   NaN -0.151997 -0.086016\n |      APPL   NaN  0.337604  0.012002\n |  \n |  pipe(self, func, *args, **kwargs)\n |      Apply func(self, \\*args, \\*\\*kwargs).\n |      \n |      Parameters\n |      ----------\n |      func : function\n |          function to apply to the Series/DataFrame.\n |          ``args``, and ``kwargs`` are passed into ``func``.\n |          Alternatively a ``(callable, data_keyword)`` tuple where\n |          ``data_keyword`` is a string indicating the keyword of\n |          ``callable`` that expects the Series/DataFrame.\n |      args : iterable, optional\n |          positional arguments passed into ``func``.\n |      kwargs : mapping, optional\n |          a dictionary of keyword arguments passed into ``func``.\n |      \n |      Returns\n |      -------\n |      object : the return type of ``func``.\n |      \n |      See Also\n |      --------\n |      DataFrame.apply\n |      DataFrame.applymap\n |      Series.map\n |      \n |      Notes\n |      -----\n |      \n |      Use ``.pipe`` when chaining together functions that expect\n |      Series, DataFrames or GroupBy objects. Instead of writing\n |      \n |      >>> f(g(h(df), arg1=a), arg2=b, arg3=c)\n |      \n |      You can write\n |      \n |      >>> (df.pipe(h)\n |      ...    .pipe(g, arg1=a)\n |      ...    .pipe(f, arg2=b, arg3=c)\n |      ... )\n |      \n |      If you have a function that takes the data as (say) the second\n |      argument, pass a tuple indicating which keyword expects the\n |      data. For example, suppose ``f`` takes its data as ``arg2``:\n |      \n |      >>> (df.pipe(h)\n |      ...    .pipe(g, arg1=a)\n |      ...    .pipe((f, 'arg2'), arg1=a, arg3=c)\n |      ...  )\n |  \n |  pop(self, item)\n |      Return item and drop from frame. Raise KeyError if not found.\n |      \n |      Parameters\n |      ----------\n |      item : str\n |          Label of column to be popped.\n |      \n |      Returns\n |      -------\n |      Series\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame([('falcon', 'bird', 389.0),\n |      ...                    ('parrot', 'bird', 24.0),\n |      ...                    ('lion', 'mammal', 80.5),\n |      ...                    ('monkey','mammal', np.nan)],\n |      ...                   columns=('name', 'class', 'max_speed'))\n |      >>> df\n |           name   class  max_speed\n |      0  falcon    bird      389.0\n |      1  parrot    bird       24.0\n |      2    lion  mammal       80.5\n |      3  monkey  mammal        NaN\n |      \n |      >>> df.pop('class')\n |      0      bird\n |      1      bird\n |      2    mammal\n |      3    mammal\n |      Name: class, dtype: object\n |      \n |      >>> df\n |           name  max_speed\n |      0  falcon      389.0\n |      1  parrot       24.0\n |      2    lion       80.5\n |      3  monkey        NaN\n |  \n |  rank(self, axis=0, method='average', numeric_only=None, na_option='keep', ascending=True, pct=False)\n |      Compute numerical data ranks (1 through n) along axis.\n |      \n |      By default, equal values are assigned a rank that is the average of the\n |      ranks of those values.\n |      \n |      Parameters\n |      ----------\n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          Index to direct ranking.\n |      method : {'average', 'min', 'max', 'first', 'dense'}, default 'average'\n |          How to rank the group of records that have the same value\n |          (i.e. ties):\n |      \n |          * average: average rank of the group\n |          * min: lowest rank in the group\n |          * max: highest rank in the group\n |          * first: ranks assigned in order they appear in the array\n |          * dense: like 'min', but rank always increases by 1 between groups\n |      numeric_only : bool, optional\n |          For DataFrame objects, rank only numeric columns if set to True.\n |      na_option : {'keep', 'top', 'bottom'}, default 'keep'\n |          How to rank NaN values:\n |      \n |          * keep: assign NaN rank to NaN values\n |          * top: assign smallest rank to NaN values if ascending\n |          * bottom: assign highest rank to NaN values if ascending\n |      ascending : bool, default True\n |          Whether or not the elements should be ranked in ascending order.\n |      pct : bool, default False\n |          Whether or not to display the returned rankings in percentile\n |          form.\n |      \n |      Returns\n |      -------\n |      same type as caller\n |          Return a Series or DataFrame with data ranks as values.\n |      \n |      See Also\n |      --------\n |      core.groupby.GroupBy.rank : Rank of values within each group.\n |      \n |      Examples\n |      --------\n |      \n |      >>> df = pd.DataFrame(data={'Animal': ['cat', 'penguin', 'dog',\n |      ...                                    'spider', 'snake'],\n |      ...                         'Number_legs': [4, 2, 4, 8, np.nan]})\n |      >>> df\n |          Animal  Number_legs\n |      0      cat          4.0\n |      1  penguin          2.0\n |      2      dog          4.0\n |      3   spider          8.0\n |      4    snake          NaN\n |      \n |      The following example shows how the method behaves with the above\n |      parameters:\n |      \n |      * default_rank: this is the default behaviour obtained without using\n |        any parameter.\n |      * max_rank: setting ``method = 'max'`` the records that have the\n |        same values are ranked using the highest rank (e.g.: since 'cat'\n |        and 'dog' are both in the 2nd and 3rd position, rank 3 is assigned.)\n |      * NA_bottom: choosing ``na_option = 'bottom'``, if there are records\n |        with NaN values they are placed at the bottom of the ranking.\n |      * pct_rank: when setting ``pct = True``, the ranking is expressed as\n |        percentile rank.\n |      \n |      >>> df['default_rank'] = df['Number_legs'].rank()\n |      >>> df['max_rank'] = df['Number_legs'].rank(method='max')\n |      >>> df['NA_bottom'] = df['Number_legs'].rank(na_option='bottom')\n |      >>> df['pct_rank'] = df['Number_legs'].rank(pct=True)\n |      >>> df\n |          Animal  Number_legs  default_rank  max_rank  NA_bottom  pct_rank\n |      0      cat          4.0           2.5       3.0        2.5     0.625\n |      1  penguin          2.0           1.0       1.0        1.0     0.250\n |      2      dog          4.0           2.5       3.0        2.5     0.625\n |      3   spider          8.0           4.0       4.0        4.0     1.000\n |      4    snake          NaN           NaN       NaN        5.0       NaN\n |  \n |  reindex_like(self, other, method=None, copy=True, limit=None, tolerance=None)\n |      Return an object with matching indices as other object.\n |      \n |      Conform the object to the same index on all axes. Optional\n |      filling logic, placing NaN in locations having no value\n |      in the previous index. A new object is produced unless the\n |      new index is equivalent to the current one and copy=False.\n |      \n |      Parameters\n |      ----------\n |      other : Object of the same data type\n |          Its row and column indices are used to define the new indices\n |          of this object.\n |      method : {None, 'backfill'/'bfill', 'pad'/'ffill', 'nearest'}\n |          Method to use for filling holes in reindexed DataFrame.\n |          Please note: this is only applicable to DataFrames/Series with a\n |          monotonically increasing/decreasing index.\n |      \n |          * None (default): don't fill gaps\n |          * pad / ffill: propagate last valid observation forward to next\n |            valid\n |          * backfill / bfill: use next valid observation to fill gap\n |          * nearest: use nearest valid observations to fill gap\n |      \n |      copy : bool, default True\n |          Return a new object, even if the passed indexes are the same.\n |      limit : int, default None\n |          Maximum number of consecutive labels to fill for inexact matches.\n |      tolerance : optional\n |          Maximum distance between original and new labels for inexact\n |          matches. The values of the index at the matching locations most\n |          satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n |      \n |          Tolerance may be a scalar value, which applies the same tolerance\n |          to all values, or list-like, which applies variable tolerance per\n |          element. List-like includes list, tuple, array, Series, and must be\n |          the same size as the index and its dtype must exactly match the\n |          index's type.\n |      \n |          .. versionadded:: 0.21.0 (list-like tolerance)\n |      \n |      Returns\n |      -------\n |      Series or DataFrame\n |          Same type as caller, but with changed indices on each axis.\n |      \n |      See Also\n |      --------\n |      DataFrame.set_index : Set row labels.\n |      DataFrame.reset_index : Remove row labels or move them to new columns.\n |      DataFrame.reindex : Change to new indices or expand indices.\n |      \n |      Notes\n |      -----\n |      Same as calling\n |      ``.reindex(index=other.index, columns=other.columns,...)``.\n |      \n |      Examples\n |      --------\n |      >>> df1 = pd.DataFrame([[24.3, 75.7, 'high'],\n |      ...                     [31, 87.8, 'high'],\n |      ...                     [22, 71.6, 'medium'],\n |      ...                     [35, 95, 'medium']],\n |      ...     columns=['temp_celsius', 'temp_fahrenheit', 'windspeed'],\n |      ...     index=pd.date_range(start='2014-02-12',\n |      ...                         end='2014-02-15', freq='D'))\n |      \n |      >>> df1\n |                  temp_celsius  temp_fahrenheit windspeed\n |      2014-02-12          24.3             75.7      high\n |      2014-02-13          31.0             87.8      high\n |      2014-02-14          22.0             71.6    medium\n |      2014-02-15          35.0             95.0    medium\n |      \n |      >>> df2 = pd.DataFrame([[28, 'low'],\n |      ...                     [30, 'low'],\n |      ...                     [35.1, 'medium']],\n |      ...     columns=['temp_celsius', 'windspeed'],\n |      ...     index=pd.DatetimeIndex(['2014-02-12', '2014-02-13',\n |      ...                             '2014-02-15']))\n |      \n |      >>> df2\n |                  temp_celsius windspeed\n |      2014-02-12          28.0       low\n |      2014-02-13          30.0       low\n |      2014-02-15          35.1    medium\n |      \n |      >>> df2.reindex_like(df1)\n |                  temp_celsius  temp_fahrenheit windspeed\n |      2014-02-12          28.0              NaN       low\n |      2014-02-13          30.0              NaN       low\n |      2014-02-14           NaN              NaN       NaN\n |      2014-02-15          35.1              NaN    medium\n |  \n |  rename_axis(self, mapper=None, index=None, columns=None, axis=None, copy=True, inplace=False)\n |      Set the name of the axis for the index or columns.\n |      \n |      Parameters\n |      ----------\n |      mapper : scalar, list-like, optional\n |          Value to set the axis name attribute.\n |      index, columns : scalar, list-like, dict-like or function, optional\n |          A scalar, list-like, dict-like or functions transformations to\n |          apply to that axis' values.\n |      \n |          Use either ``mapper`` and ``axis`` to\n |          specify the axis to target with ``mapper``, or ``index``\n |          and/or ``columns``.\n |      \n |          .. versionchanged:: 0.24.0\n |      \n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          The axis to rename.\n |      copy : bool, default True\n |          Also copy underlying data.\n |      inplace : bool, default False\n |          Modifies the object directly, instead of creating a new Series\n |          or DataFrame.\n |      \n |      Returns\n |      -------\n |      Series, DataFrame, or None\n |          The same type as the caller or None if `inplace` is True.\n |      \n |      See Also\n |      --------\n |      Series.rename : Alter Series index labels or name.\n |      DataFrame.rename : Alter DataFrame index labels or name.\n |      Index.rename : Set new names on index.\n |      \n |      Notes\n |      -----\n |      ``DataFrame.rename_axis`` supports two calling conventions\n |      \n |      * ``(index=index_mapper, columns=columns_mapper, ...)``\n |      * ``(mapper, axis={'index', 'columns'}, ...)``\n |      \n |      The first calling convention will only modify the names of\n |      the index and/or the names of the Index object that is the columns.\n |      In this case, the parameter ``copy`` is ignored.\n |      \n |      The second calling convention will modify the names of the\n |      the corresponding index if mapper is a list or a scalar.\n |      However, if mapper is dict-like or a function, it will use the\n |      deprecated behavior of modifying the axis *labels*.\n |      \n |      We *highly* recommend using keyword arguments to clarify your\n |      intent.\n |      \n |      Examples\n |      --------\n |      **Series**\n |      \n |      >>> s = pd.Series([\"dog\", \"cat\", \"monkey\"])\n |      >>> s\n |      0       dog\n |      1       cat\n |      2    monkey\n |      dtype: object\n |      >>> s.rename_axis(\"animal\")\n |      animal\n |      0    dog\n |      1    cat\n |      2    monkey\n |      dtype: object\n |      \n |      **DataFrame**\n |      \n |      >>> df = pd.DataFrame({\"num_legs\": [4, 4, 2],\n |      ...                    \"num_arms\": [0, 0, 2]},\n |      ...                   [\"dog\", \"cat\", \"monkey\"])\n |      >>> df\n |              num_legs  num_arms\n |      dog            4         0\n |      cat            4         0\n |      monkey         2         2\n |      >>> df = df.rename_axis(\"animal\")\n |      >>> df\n |              num_legs  num_arms\n |      animal\n |      dog            4         0\n |      cat            4         0\n |      monkey         2         2\n |      >>> df = df.rename_axis(\"limbs\", axis=\"columns\")\n |      >>> df\n |      limbs   num_legs  num_arms\n |      animal\n |      dog            4         0\n |      cat            4         0\n |      monkey         2         2\n |      \n |      **MultiIndex**\n |      \n |      >>> df.index = pd.MultiIndex.from_product([['mammal'],\n |      ...                                        ['dog', 'cat', 'monkey']],\n |      ...                                       names=['type', 'name'])\n |      >>> df\n |      limbs          num_legs  num_arms\n |      type   name\n |      mammal dog            4         0\n |             cat            4         0\n |             monkey         2         2\n |      \n |      >>> df.rename_axis(index={'type': 'class'})\n |      limbs          num_legs  num_arms\n |      class  name\n |      mammal dog            4         0\n |             cat            4         0\n |             monkey         2         2\n |      \n |      >>> df.rename_axis(columns=str.upper)\n |      LIMBS          num_legs  num_arms\n |      type   name\n |      mammal dog            4         0\n |             cat            4         0\n |             monkey         2         2\n |  \n |  resample(self, rule, how=None, axis=0, fill_method=None, closed=None, label=None, convention='start', kind=None, loffset=None, limit=None, base=0, on=None, level=None)\n |      Resample time-series data.\n |      \n |      Convenience method for frequency conversion and resampling of time\n |      series. Object must have a datetime-like index (`DatetimeIndex`,\n |      `PeriodIndex`, or `TimedeltaIndex`), or pass datetime-like values\n |      to the `on` or `level` keyword.\n |      \n |      Parameters\n |      ----------\n |      rule : DateOffset, Timedelta or str\n |          The offset string or object representing target conversion.\n |      how : str\n |          Method for down/re-sampling, default to 'mean' for downsampling.\n |      \n |          .. deprecated:: 0.18.0\n |             The new syntax is ``.resample(...).mean()``, or\n |             ``.resample(...).apply(<func>)``\n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          Which axis to use for up- or down-sampling. For `Series` this\n |          will default to 0, i.e. along the rows. Must be\n |          `DatetimeIndex`, `TimedeltaIndex` or `PeriodIndex`.\n |      fill_method : str, default None\n |          Filling method for upsampling.\n |      \n |          .. deprecated:: 0.18.0\n |             The new syntax is ``.resample(...).<func>()``,\n |             e.g. ``.resample(...).pad()``\n |      closed : {'right', 'left'}, default None\n |          Which side of bin interval is closed. The default is 'left'\n |          for all frequency offsets except for 'M', 'A', 'Q', 'BM',\n |          'BA', 'BQ', and 'W' which all have a default of 'right'.\n |      label : {'right', 'left'}, default None\n |          Which bin edge label to label bucket with. The default is 'left'\n |          for all frequency offsets except for 'M', 'A', 'Q', 'BM',\n |          'BA', 'BQ', and 'W' which all have a default of 'right'.\n |      convention : {'start', 'end', 's', 'e'}, default 'start'\n |          For `PeriodIndex` only, controls whether to use the start or\n |          end of `rule`.\n |      kind : {'timestamp', 'period'}, optional, default None\n |          Pass 'timestamp' to convert the resulting index to a\n |          `DateTimeIndex` or 'period' to convert it to a `PeriodIndex`.\n |          By default the input representation is retained.\n |      loffset : timedelta, default None\n |          Adjust the resampled time labels.\n |      limit : int, default None\n |          Maximum size gap when reindexing with `fill_method`.\n |      \n |          .. deprecated:: 0.18.0\n |      base : int, default 0\n |          For frequencies that evenly subdivide 1 day, the \"origin\" of the\n |          aggregated intervals. For example, for '5min' frequency, base could\n |          range from 0 through 4. Defaults to 0.\n |      on : str, optional\n |          For a DataFrame, column to use instead of index for resampling.\n |          Column must be datetime-like.\n |      \n |          .. versionadded:: 0.19.0\n |      \n |      level : str or int, optional\n |          For a MultiIndex, level (name or number) to use for\n |          resampling. `level` must be datetime-like.\n |      \n |          .. versionadded:: 0.19.0\n |      \n |      Returns\n |      -------\n |      Resampler object\n |      \n |      See Also\n |      --------\n |      groupby : Group by mapping, function, label, or list of labels.\n |      Series.resample : Resample a Series.\n |      DataFrame.resample: Resample a DataFrame.\n |      \n |      Notes\n |      -----\n |      See the `user guide\n |      <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#resampling>`_\n |      for more.\n |      \n |      To learn more about the offset strings, please see `this link\n |      <http://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects>`__.\n |      \n |      Examples\n |      --------\n |      \n |      Start by creating a series with 9 one minute timestamps.\n |      \n |      >>> index = pd.date_range('1/1/2000', periods=9, freq='T')\n |      >>> series = pd.Series(range(9), index=index)\n |      >>> series\n |      2000-01-01 00:00:00    0\n |      2000-01-01 00:01:00    1\n |      2000-01-01 00:02:00    2\n |      2000-01-01 00:03:00    3\n |      2000-01-01 00:04:00    4\n |      2000-01-01 00:05:00    5\n |      2000-01-01 00:06:00    6\n |      2000-01-01 00:07:00    7\n |      2000-01-01 00:08:00    8\n |      Freq: T, dtype: int64\n |      \n |      Downsample the series into 3 minute bins and sum the values\n |      of the timestamps falling into a bin.\n |      \n |      >>> series.resample('3T').sum()\n |      2000-01-01 00:00:00     3\n |      2000-01-01 00:03:00    12\n |      2000-01-01 00:06:00    21\n |      Freq: 3T, dtype: int64\n |      \n |      Downsample the series into 3 minute bins as above, but label each\n |      bin using the right edge instead of the left. Please note that the\n |      value in the bucket used as the label is not included in the bucket,\n |      which it labels. For example, in the original series the\n |      bucket ``2000-01-01 00:03:00`` contains the value 3, but the summed\n |      value in the resampled bucket with the label ``2000-01-01 00:03:00``\n |      does not include 3 (if it did, the summed value would be 6, not 3).\n |      To include this value close the right side of the bin interval as\n |      illustrated in the example below this one.\n |      \n |      >>> series.resample('3T', label='right').sum()\n |      2000-01-01 00:03:00     3\n |      2000-01-01 00:06:00    12\n |      2000-01-01 00:09:00    21\n |      Freq: 3T, dtype: int64\n |      \n |      Downsample the series into 3 minute bins as above, but close the right\n |      side of the bin interval.\n |      \n |      >>> series.resample('3T', label='right', closed='right').sum()\n |      2000-01-01 00:00:00     0\n |      2000-01-01 00:03:00     6\n |      2000-01-01 00:06:00    15\n |      2000-01-01 00:09:00    15\n |      Freq: 3T, dtype: int64\n |      \n |      Upsample the series into 30 second bins.\n |      \n |      >>> series.resample('30S').asfreq()[0:5]   # Select first 5 rows\n |      2000-01-01 00:00:00   0.0\n |      2000-01-01 00:00:30   NaN\n |      2000-01-01 00:01:00   1.0\n |      2000-01-01 00:01:30   NaN\n |      2000-01-01 00:02:00   2.0\n |      Freq: 30S, dtype: float64\n |      \n |      Upsample the series into 30 second bins and fill the ``NaN``\n |      values using the ``pad`` method.\n |      \n |      >>> series.resample('30S').pad()[0:5]\n |      2000-01-01 00:00:00    0\n |      2000-01-01 00:00:30    0\n |      2000-01-01 00:01:00    1\n |      2000-01-01 00:01:30    1\n |      2000-01-01 00:02:00    2\n |      Freq: 30S, dtype: int64\n |      \n |      Upsample the series into 30 second bins and fill the\n |      ``NaN`` values using the ``bfill`` method.\n |      \n |      >>> series.resample('30S').bfill()[0:5]\n |      2000-01-01 00:00:00    0\n |      2000-01-01 00:00:30    1\n |      2000-01-01 00:01:00    1\n |      2000-01-01 00:01:30    2\n |      2000-01-01 00:02:00    2\n |      Freq: 30S, dtype: int64\n |      \n |      Pass a custom function via ``apply``\n |      \n |      >>> def custom_resampler(array_like):\n |      ...     return np.sum(array_like) + 5\n |      ...\n |      >>> series.resample('3T').apply(custom_resampler)\n |      2000-01-01 00:00:00     8\n |      2000-01-01 00:03:00    17\n |      2000-01-01 00:06:00    26\n |      Freq: 3T, dtype: int64\n |      \n |      For a Series with a PeriodIndex, the keyword `convention` can be\n |      used to control whether to use the start or end of `rule`.\n |      \n |      Resample a year by quarter using 'start' `convention`. Values are\n |      assigned to the first quarter of the period.\n |      \n |      >>> s = pd.Series([1, 2], index=pd.period_range('2012-01-01',\n |      ...                                             freq='A',\n |      ...                                             periods=2))\n |      >>> s\n |      2012    1\n |      2013    2\n |      Freq: A-DEC, dtype: int64\n |      >>> s.resample('Q', convention='start').asfreq()\n |      2012Q1    1.0\n |      2012Q2    NaN\n |      2012Q3    NaN\n |      2012Q4    NaN\n |      2013Q1    2.0\n |      2013Q2    NaN\n |      2013Q3    NaN\n |      2013Q4    NaN\n |      Freq: Q-DEC, dtype: float64\n |      \n |      Resample quarters by month using 'end' `convention`. Values are\n |      assigned to the last month of the period.\n |      \n |      >>> q = pd.Series([1, 2, 3, 4], index=pd.period_range('2018-01-01',\n |      ...                                                   freq='Q',\n |      ...                                                   periods=4))\n |      >>> q\n |      2018Q1    1\n |      2018Q2    2\n |      2018Q3    3\n |      2018Q4    4\n |      Freq: Q-DEC, dtype: int64\n |      >>> q.resample('M', convention='end').asfreq()\n |      2018-03    1.0\n |      2018-04    NaN\n |      2018-05    NaN\n |      2018-06    2.0\n |      2018-07    NaN\n |      2018-08    NaN\n |      2018-09    3.0\n |      2018-10    NaN\n |      2018-11    NaN\n |      2018-12    4.0\n |      Freq: M, dtype: float64\n |      \n |      For DataFrame objects, the keyword `on` can be used to specify the\n |      column instead of the index for resampling.\n |      \n |      >>> d = dict({'price': [10, 11, 9, 13, 14, 18, 17, 19],\n |      ...           'volume': [50, 60, 40, 100, 50, 100, 40, 50]})\n |      >>> df = pd.DataFrame(d)\n |      >>> df['week_starting'] = pd.date_range('01/01/2018',\n |      ...                                     periods=8,\n |      ...                                     freq='W')\n |      >>> df\n |         price  volume week_starting\n |      0     10      50    2018-01-07\n |      1     11      60    2018-01-14\n |      2      9      40    2018-01-21\n |      3     13     100    2018-01-28\n |      4     14      50    2018-02-04\n |      5     18     100    2018-02-11\n |      6     17      40    2018-02-18\n |      7     19      50    2018-02-25\n |      >>> df.resample('M', on='week_starting').mean()\n |                     price  volume\n |      week_starting\n |      2018-01-31     10.75    62.5\n |      2018-02-28     17.00    60.0\n |      \n |      For a DataFrame with MultiIndex, the keyword `level` can be used to\n |      specify on which level the resampling needs to take place.\n |      \n |      >>> days = pd.date_range('1/1/2000', periods=4, freq='D')\n |      >>> d2 = dict({'price': [10, 11, 9, 13, 14, 18, 17, 19],\n |      ...            'volume': [50, 60, 40, 100, 50, 100, 40, 50]})\n |      >>> df2 = pd.DataFrame(d2,\n |      ...                    index=pd.MultiIndex.from_product([days,\n |      ...                                                     ['morning',\n |      ...                                                      'afternoon']]\n |      ...                                                     ))\n |      >>> df2\n |                            price  volume\n |      2000-01-01 morning       10      50\n |                 afternoon     11      60\n |      2000-01-02 morning        9      40\n |                 afternoon     13     100\n |      2000-01-03 morning       14      50\n |                 afternoon     18     100\n |      2000-01-04 morning       17      40\n |                 afternoon     19      50\n |      >>> df2.resample('D', level=0).sum()\n |                  price  volume\n |      2000-01-01     21     110\n |      2000-01-02     22     140\n |      2000-01-03     32     150\n |      2000-01-04     36      90\n |  \n |  sample(self, n=None, frac=None, replace=False, weights=None, random_state=None, axis=None)\n |      Return a random sample of items from an axis of object.\n |      \n |      You can use `random_state` for reproducibility.\n |      \n |      Parameters\n |      ----------\n |      n : int, optional\n |          Number of items from axis to return. Cannot be used with `frac`.\n |          Default = 1 if `frac` = None.\n |      frac : float, optional\n |          Fraction of axis items to return. Cannot be used with `n`.\n |      replace : bool, default False\n |          Sample with or without replacement.\n |      weights : str or ndarray-like, optional\n |          Default 'None' results in equal probability weighting.\n |          If passed a Series, will align with target object on index. Index\n |          values in weights not found in sampled object will be ignored and\n |          index values in sampled object not in weights will be assigned\n |          weights of zero.\n |          If called on a DataFrame, will accept the name of a column\n |          when axis = 0.\n |          Unless weights are a Series, weights must be same length as axis\n |          being sampled.\n |          If weights do not sum to 1, they will be normalized to sum to 1.\n |          Missing values in the weights column will be treated as zero.\n |          Infinite values not allowed.\n |      random_state : int or numpy.random.RandomState, optional\n |          Seed for the random number generator (if int), or numpy RandomState\n |          object.\n |      axis : int or string, optional\n |          Axis to sample. Accepts axis number or name. Default is stat axis\n |          for given data type (0 for Series and DataFrames).\n |      \n |      Returns\n |      -------\n |      Series or DataFrame\n |          A new object of same type as caller containing `n` items randomly\n |          sampled from the caller object.\n |      \n |      See Also\n |      --------\n |      numpy.random.choice: Generates a random sample from a given 1-D numpy\n |          array.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'num_legs': [2, 4, 8, 0],\n |      ...                    'num_wings': [2, 0, 0, 0],\n |      ...                    'num_specimen_seen': [10, 2, 1, 8]},\n |      ...                   index=['falcon', 'dog', 'spider', 'fish'])\n |      >>> df\n |              num_legs  num_wings  num_specimen_seen\n |      falcon         2          2                 10\n |      dog            4          0                  2\n |      spider         8          0                  1\n |      fish           0          0                  8\n |      \n |      Extract 3 random elements from the ``Series`` ``df['num_legs']``:\n |      Note that we use `random_state` to ensure the reproducibility of\n |      the examples.\n |      \n |      >>> df['num_legs'].sample(n=3, random_state=1)\n |      fish      0\n |      spider    8\n |      falcon    2\n |      Name: num_legs, dtype: int64\n |      \n |      A random 50% sample of the ``DataFrame`` with replacement:\n |      \n |      >>> df.sample(frac=0.5, replace=True, random_state=1)\n |            num_legs  num_wings  num_specimen_seen\n |      dog          4          0                  2\n |      fish         0          0                  8\n |      \n |      Using a DataFrame column as weights. Rows with larger value in the\n |      `num_specimen_seen` column are more likely to be sampled.\n |      \n |      >>> df.sample(n=2, weights='num_specimen_seen', random_state=1)\n |              num_legs  num_wings  num_specimen_seen\n |      falcon         2          2                 10\n |      fish           0          0                  8\n |  \n |  set_axis(self, labels, axis=0, inplace=None)\n |      Assign desired index to given axis.\n |      \n |      Indexes for column or row labels can be changed by assigning\n |      a list-like or Index.\n |      \n |      .. versionchanged:: 0.21.0\n |      \n |         The signature is now `labels` and `axis`, consistent with\n |         the rest of pandas API. Previously, the `axis` and `labels`\n |         arguments were respectively the first and second positional\n |         arguments.\n |      \n |      Parameters\n |      ----------\n |      labels : list-like, Index\n |          The values for the new index.\n |      \n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          The axis to update. The value 0 identifies the rows, and 1\n |          identifies the columns.\n |      \n |      inplace : bool, default None\n |          Whether to return a new %(klass)s instance.\n |      \n |          .. warning::\n |      \n |             ``inplace=None`` currently falls back to to True, but in a\n |             future version, will default to False. Use inplace=True\n |             explicitly rather than relying on the default.\n |      \n |      Returns\n |      -------\n |      renamed : %(klass)s or None\n |          An object of same type as caller if inplace=False, None otherwise.\n |      \n |      See Also\n |      --------\n |      DataFrame.rename_axis : Alter the name of the index or columns.\n |      \n |      Examples\n |      --------\n |      **Series**\n |      \n |      >>> s = pd.Series([1, 2, 3])\n |      >>> s\n |      0    1\n |      1    2\n |      2    3\n |      dtype: int64\n |      \n |      >>> s.set_axis(['a', 'b', 'c'], axis=0, inplace=False)\n |      a    1\n |      b    2\n |      c    3\n |      dtype: int64\n |      \n |      The original object is not modified.\n |      \n |      >>> s\n |      0    1\n |      1    2\n |      2    3\n |      dtype: int64\n |      \n |      **DataFrame**\n |      \n |      >>> df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n |      \n |      Change the row labels.\n |      \n |      >>> df.set_axis(['a', 'b', 'c'], axis='index', inplace=False)\n |         A  B\n |      a  1  4\n |      b  2  5\n |      c  3  6\n |      \n |      Change the column labels.\n |      \n |      >>> df.set_axis(['I', 'II'], axis='columns', inplace=False)\n |         I  II\n |      0  1   4\n |      1  2   5\n |      2  3   6\n |      \n |      Now, update the labels inplace.\n |      \n |      >>> df.set_axis(['i', 'ii'], axis='columns', inplace=True)\n |      >>> df\n |         i  ii\n |      0  1   4\n |      1  2   5\n |      2  3   6\n |  \n |  slice_shift(self, periods=1, axis=0)\n |      Equivalent to `shift` without copying data. The shifted data will\n |      not include the dropped periods and the shifted axis will be smaller\n |      than the original.\n |      \n |      Parameters\n |      ----------\n |      periods : int\n |          Number of periods to move, can be positive or negative\n |      \n |      Returns\n |      -------\n |      shifted : same type as caller\n |      \n |      Notes\n |      -----\n |      While the `slice_shift` is faster than `shift`, you may pay for it\n |      later during alignment.\n |  \n |  squeeze(self, axis=None)\n |      Squeeze 1 dimensional axis objects into scalars.\n |      \n |      Series or DataFrames with a single element are squeezed to a scalar.\n |      DataFrames with a single column or a single row are squeezed to a\n |      Series. Otherwise the object is unchanged.\n |      \n |      This method is most useful when you don't know if your\n |      object is a Series or DataFrame, but you do know it has just a single\n |      column. In that case you can safely call `squeeze` to ensure you have a\n |      Series.\n |      \n |      Parameters\n |      ----------\n |      axis : {0 or 'index', 1 or 'columns', None}, default None\n |          A specific axis to squeeze. By default, all length-1 axes are\n |          squeezed.\n |      \n |          .. versionadded:: 0.20.0\n |      \n |      Returns\n |      -------\n |      DataFrame, Series, or scalar\n |          The projection after squeezing `axis` or all the axes.\n |      \n |      See Also\n |      --------\n |      Series.iloc : Integer-location based indexing for selecting scalars.\n |      DataFrame.iloc : Integer-location based indexing for selecting Series.\n |      Series.to_frame : Inverse of DataFrame.squeeze for a\n |          single-column DataFrame.\n |      \n |      Examples\n |      --------\n |      >>> primes = pd.Series([2, 3, 5, 7])\n |      \n |      Slicing might produce a Series with a single value:\n |      \n |      >>> even_primes = primes[primes % 2 == 0]\n |      >>> even_primes\n |      0    2\n |      dtype: int64\n |      \n |      >>> even_primes.squeeze()\n |      2\n |      \n |      Squeezing objects with more than one value in every axis does nothing:\n |      \n |      >>> odd_primes = primes[primes % 2 == 1]\n |      >>> odd_primes\n |      1    3\n |      2    5\n |      3    7\n |      dtype: int64\n |      \n |      >>> odd_primes.squeeze()\n |      1    3\n |      2    5\n |      3    7\n |      dtype: int64\n |      \n |      Squeezing is even more effective when used with DataFrames.\n |      \n |      >>> df = pd.DataFrame([[1, 2], [3, 4]], columns=['a', 'b'])\n |      >>> df\n |         a  b\n |      0  1  2\n |      1  3  4\n |      \n |      Slicing a single column will produce a DataFrame with the columns\n |      having only one value:\n |      \n |      >>> df_a = df[['a']]\n |      >>> df_a\n |         a\n |      0  1\n |      1  3\n |      \n |      So the columns can be squeezed down, resulting in a Series:\n |      \n |      >>> df_a.squeeze('columns')\n |      0    1\n |      1    3\n |      Name: a, dtype: int64\n |      \n |      Slicing a single row from a single column will produce a single\n |      scalar DataFrame:\n |      \n |      >>> df_0a = df.loc[df.index < 1, ['a']]\n |      >>> df_0a\n |         a\n |      0  1\n |      \n |      Squeezing the rows produces a single scalar Series:\n |      \n |      >>> df_0a.squeeze('rows')\n |      a    1\n |      Name: 0, dtype: int64\n |      \n |      Squeezing all axes will project directly into a scalar:\n |      \n |      >>> df_0a.squeeze()\n |      1\n |  \n |  swapaxes(self, axis1, axis2, copy=True)\n |      Interchange axes and swap values axes appropriately.\n |      \n |      Returns\n |      -------\n |      y : same as input\n |  \n |  tail(self, n=5)\n |      Return the last `n` rows.\n |      \n |      This function returns last `n` rows from the object based on\n |      position. It is useful for quickly verifying data, for example,\n |      after sorting or appending rows.\n |      \n |      Parameters\n |      ----------\n |      n : int, default 5\n |          Number of rows to select.\n |      \n |      Returns\n |      -------\n |      type of caller\n |          The last `n` rows of the caller object.\n |      \n |      See Also\n |      --------\n |      DataFrame.head : The first `n` rows of the caller object.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'animal':['alligator', 'bee', 'falcon', 'lion',\n |      ...                    'monkey', 'parrot', 'shark', 'whale', 'zebra']})\n |      >>> df\n |            animal\n |      0  alligator\n |      1        bee\n |      2     falcon\n |      3       lion\n |      4     monkey\n |      5     parrot\n |      6      shark\n |      7      whale\n |      8      zebra\n |      \n |      Viewing the last 5 lines\n |      \n |      >>> df.tail()\n |         animal\n |      4  monkey\n |      5  parrot\n |      6   shark\n |      7   whale\n |      8   zebra\n |      \n |      Viewing the last `n` lines (three in this case)\n |      \n |      >>> df.tail(3)\n |        animal\n |      6  shark\n |      7  whale\n |      8  zebra\n |  \n |  take(self, indices, axis=0, is_copy=True, **kwargs)\n |      Return the elements in the given *positional* indices along an axis.\n |      \n |      This means that we are not indexing according to actual values in\n |      the index attribute of the object. We are indexing according to the\n |      actual position of the element in the object.\n |      \n |      Parameters\n |      ----------\n |      indices : array-like\n |          An array of ints indicating which positions to take.\n |      axis : {0 or 'index', 1 or 'columns', None}, default 0\n |          The axis on which to select elements. ``0`` means that we are\n |          selecting rows, ``1`` means that we are selecting columns.\n |      is_copy : bool, default True\n |          Whether to return a copy of the original object or not.\n |      **kwargs\n |          For compatibility with :meth:`numpy.take`. Has no effect on the\n |          output.\n |      \n |      Returns\n |      -------\n |      taken : same type as caller\n |          An array-like containing the elements taken from the object.\n |      \n |      See Also\n |      --------\n |      DataFrame.loc : Select a subset of a DataFrame by labels.\n |      DataFrame.iloc : Select a subset of a DataFrame by positions.\n |      numpy.take : Take elements from an array along an axis.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame([('falcon', 'bird',    389.0),\n |      ...                    ('parrot', 'bird',     24.0),\n |      ...                    ('lion',   'mammal',   80.5),\n |      ...                    ('monkey', 'mammal', np.nan)],\n |      ...                    columns=['name', 'class', 'max_speed'],\n |      ...                    index=[0, 2, 3, 1])\n |      >>> df\n |           name   class  max_speed\n |      0  falcon    bird      389.0\n |      2  parrot    bird       24.0\n |      3    lion  mammal       80.5\n |      1  monkey  mammal        NaN\n |      \n |      Take elements at positions 0 and 3 along the axis 0 (default).\n |      \n |      Note how the actual indices selected (0 and 1) do not correspond to\n |      our selected indices 0 and 3. That's because we are selecting the 0th\n |      and 3rd rows, not rows whose indices equal 0 and 3.\n |      \n |      >>> df.take([0, 3])\n |           name   class  max_speed\n |      0  falcon    bird      389.0\n |      1  monkey  mammal        NaN\n |      \n |      Take elements at indices 1 and 2 along the axis 1 (column selection).\n |      \n |      >>> df.take([1, 2], axis=1)\n |          class  max_speed\n |      0    bird      389.0\n |      2    bird       24.0\n |      3  mammal       80.5\n |      1  mammal        NaN\n |      \n |      We may take elements using negative integers for positive indices,\n |      starting from the end of the object, just like with Python lists.\n |      \n |      >>> df.take([-1, -2])\n |           name   class  max_speed\n |      1  monkey  mammal        NaN\n |      3    lion  mammal       80.5\n |  \n |  to_clipboard(self, excel=True, sep=None, **kwargs)\n |      Copy object to the system clipboard.\n |      \n |      Write a text representation of object to the system clipboard.\n |      This can be pasted into Excel, for example.\n |      \n |      Parameters\n |      ----------\n |      excel : bool, default True\n |          - True, use the provided separator, writing in a csv format for\n |            allowing easy pasting into excel.\n |          - False, write a string representation of the object to the\n |            clipboard.\n |      \n |      sep : str, default ``'\\t'``\n |          Field delimiter.\n |      **kwargs\n |          These parameters will be passed to DataFrame.to_csv.\n |      \n |      See Also\n |      --------\n |      DataFrame.to_csv : Write a DataFrame to a comma-separated values\n |          (csv) file.\n |      read_clipboard : Read text from clipboard and pass to read_table.\n |      \n |      Notes\n |      -----\n |      Requirements for your platform.\n |      \n |        - Linux : `xclip`, or `xsel` (with `PyQt4` modules)\n |        - Windows : none\n |        - OS X : none\n |      \n |      Examples\n |      --------\n |      Copy the contents of a DataFrame to the clipboard.\n |      \n |      >>> df = pd.DataFrame([[1, 2, 3], [4, 5, 6]], columns=['A', 'B', 'C'])\n |      >>> df.to_clipboard(sep=',')\n |      ... # Wrote the following to the system clipboard:\n |      ... # ,A,B,C\n |      ... # 0,1,2,3\n |      ... # 1,4,5,6\n |      \n |      We can omit the the index by passing the keyword `index` and setting\n |      it to false.\n |      \n |      >>> df.to_clipboard(sep=',', index=False)\n |      ... # Wrote the following to the system clipboard:\n |      ... # A,B,C\n |      ... # 1,2,3\n |      ... # 4,5,6\n |  \n |  to_csv(self, path_or_buf=None, sep=',', na_rep='', float_format=None, columns=None, header=True, index=True, index_label=None, mode='w', encoding=None, compression='infer', quoting=None, quotechar='\"', line_terminator=None, chunksize=None, date_format=None, doublequote=True, escapechar=None, decimal='.')\n |      Write object to a comma-separated values (csv) file.\n |      \n |      .. versionchanged:: 0.24.0\n |          The order of arguments for Series was changed.\n |      \n |      Parameters\n |      ----------\n |      path_or_buf : str or file handle, default None\n |          File path or object, if None is provided the result is returned as\n |          a string.  If a file object is passed it should be opened with\n |          `newline=''`, disabling universal newlines.\n |      \n |          .. versionchanged:: 0.24.0\n |      \n |             Was previously named \"path\" for Series.\n |      \n |      sep : str, default ','\n |          String of length 1. Field delimiter for the output file.\n |      na_rep : str, default ''\n |          Missing data representation.\n |      float_format : str, default None\n |          Format string for floating point numbers.\n |      columns : sequence, optional\n |          Columns to write.\n |      header : bool or list of str, default True\n |          Write out the column names. If a list of strings is given it is\n |          assumed to be aliases for the column names.\n |      \n |          .. versionchanged:: 0.24.0\n |      \n |             Previously defaulted to False for Series.\n |      \n |      index : bool, default True\n |          Write row names (index).\n |      index_label : str or sequence, or False, default None\n |          Column label for index column(s) if desired. If None is given, and\n |          `header` and `index` are True, then the index names are used. A\n |          sequence should be given if the object uses MultiIndex. If\n |          False do not print fields for index names. Use index_label=False\n |          for easier importing in R.\n |      mode : str\n |          Python write mode, default 'w'.\n |      encoding : str, optional\n |          A string representing the encoding to use in the output file,\n |          defaults to 'utf-8'.\n |      compression : str, default 'infer'\n |          Compression mode among the following possible values: {'infer',\n |          'gzip', 'bz2', 'zip', 'xz', None}. If 'infer' and `path_or_buf`\n |          is path-like, then detect compression from the following\n |          extensions: '.gz', '.bz2', '.zip' or '.xz'. (otherwise no\n |          compression).\n |      \n |          .. versionchanged:: 0.24.0\n |      \n |             'infer' option added and set to default.\n |      \n |      quoting : optional constant from csv module\n |          Defaults to csv.QUOTE_MINIMAL. If you have set a `float_format`\n |          then floats are converted to strings and thus csv.QUOTE_NONNUMERIC\n |          will treat them as non-numeric.\n |      quotechar : str, default '\\\"'\n |          String of length 1. Character used to quote fields.\n |      line_terminator : str, optional\n |          The newline character or character sequence to use in the output\n |          file. Defaults to `os.linesep`, which depends on the OS in which\n |          this method is called ('\\n' for linux, '\\r\\n' for Windows, i.e.).\n |      \n |          .. versionchanged:: 0.24.0\n |      chunksize : int or None\n |          Rows to write at a time.\n |      date_format : str, default None\n |          Format string for datetime objects.\n |      doublequote : bool, default True\n |          Control quoting of `quotechar` inside a field.\n |      escapechar : str, default None\n |          String of length 1. Character used to escape `sep` and `quotechar`\n |          when appropriate.\n |      decimal : str, default '.'\n |          Character recognized as decimal separator. E.g. use ',' for\n |          European data.\n |      \n |      Returns\n |      -------\n |      None or str\n |          If path_or_buf is None, returns the resulting csv format as a\n |          string. Otherwise returns None.\n |      \n |      See Also\n |      --------\n |      read_csv : Load a CSV file into a DataFrame.\n |      to_excel : Write DataFrame to an Excel file.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'name': ['Raphael', 'Donatello'],\n |      ...                    'mask': ['red', 'purple'],\n |      ...                    'weapon': ['sai', 'bo staff']})\n |      >>> df.to_csv(index=False)\n |      'name,mask,weapon\\nRaphael,red,sai\\nDonatello,purple,bo staff\\n'\n |  \n |  to_dense(self)\n |      Return dense representation of Series/DataFrame (as opposed to sparse).\n |      \n |      .. deprecated:: 0.25.0\n |      \n |      Returns\n |      -------\n |      %(klass)s\n |          Dense %(klass)s.\n |  \n |  to_excel(self, excel_writer, sheet_name='Sheet1', na_rep='', float_format=None, columns=None, header=True, index=True, index_label=None, startrow=0, startcol=0, engine=None, merge_cells=True, encoding=None, inf_rep='inf', verbose=True, freeze_panes=None)\n |      Write object to an Excel sheet.\n |      \n |      To write a single object to an Excel .xlsx file it is only necessary to\n |      specify a target file name. To write to multiple sheets it is necessary to\n |      create an `ExcelWriter` object with a target file name, and specify a sheet\n |      in the file to write to.\n |      \n |      Multiple sheets may be written to by specifying unique `sheet_name`.\n |      With all data written to the file it is necessary to save the changes.\n |      Note that creating an `ExcelWriter` object with a file name that already\n |      exists will result in the contents of the existing file being erased.\n |      \n |      Parameters\n |      ----------\n |      excel_writer : str or ExcelWriter object\n |          File path or existing ExcelWriter.\n |      sheet_name : str, default 'Sheet1'\n |          Name of sheet which will contain DataFrame.\n |      na_rep : str, default ''\n |          Missing data representation.\n |      float_format : str, optional\n |          Format string for floating point numbers. For example\n |          ``float_format=\"%.2f\"`` will format 0.1234 to 0.12.\n |      columns : sequence or list of str, optional\n |          Columns to write.\n |      header : bool or list of str, default True\n |          Write out the column names. If a list of string is given it is\n |          assumed to be aliases for the column names.\n |      index : bool, default True\n |          Write row names (index).\n |      index_label : str or sequence, optional\n |          Column label for index column(s) if desired. If not specified, and\n |          `header` and `index` are True, then the index names are used. A\n |          sequence should be given if the DataFrame uses MultiIndex.\n |      startrow : int, default 0\n |          Upper left cell row to dump data frame.\n |      startcol : int, default 0\n |          Upper left cell column to dump data frame.\n |      engine : str, optional\n |          Write engine to use, 'openpyxl' or 'xlsxwriter'. You can also set this\n |          via the options ``io.excel.xlsx.writer``, ``io.excel.xls.writer``, and\n |          ``io.excel.xlsm.writer``.\n |      merge_cells : bool, default True\n |          Write MultiIndex and Hierarchical Rows as merged cells.\n |      encoding : str, optional\n |          Encoding of the resulting excel file. Only necessary for xlwt,\n |          other writers support unicode natively.\n |      inf_rep : str, default 'inf'\n |          Representation for infinity (there is no native representation for\n |          infinity in Excel).\n |      verbose : bool, default True\n |          Display more information in the error logs.\n |      freeze_panes : tuple of int (length 2), optional\n |          Specifies the one-based bottommost row and rightmost column that\n |          is to be frozen.\n |      \n |          .. versionadded:: 0.20.0.\n |      \n |      See Also\n |      --------\n |      to_csv : Write DataFrame to a comma-separated values (csv) file.\n |      ExcelWriter : Class for writing DataFrame objects into excel sheets.\n |      read_excel : Read an Excel file into a pandas DataFrame.\n |      read_csv : Read a comma-separated values (csv) file into DataFrame.\n |      \n |      Notes\n |      -----\n |      For compatibility with :meth:`~DataFrame.to_csv`,\n |      to_excel serializes lists and dicts to strings before writing.\n |      \n |      Once a workbook has been saved it is not possible write further data\n |      without rewriting the whole workbook.\n |      \n |      Examples\n |      --------\n |      \n |      Create, write to and save a workbook:\n |      \n |      >>> df1 = pd.DataFrame([['a', 'b'], ['c', 'd']],\n |      ...                    index=['row 1', 'row 2'],\n |      ...                    columns=['col 1', 'col 2'])\n |      >>> df1.to_excel(\"output.xlsx\")  # doctest: +SKIP\n |      \n |      To specify the sheet name:\n |      \n |      >>> df1.to_excel(\"output.xlsx\",\n |      ...              sheet_name='Sheet_name_1')  # doctest: +SKIP\n |      \n |      If you wish to write to more than one sheet in the workbook, it is\n |      necessary to specify an ExcelWriter object:\n |      \n |      >>> df2 = df1.copy()\n |      >>> with pd.ExcelWriter('output.xlsx') as writer:  # doctest: +SKIP\n |      ...     df1.to_excel(writer, sheet_name='Sheet_name_1')\n |      ...     df2.to_excel(writer, sheet_name='Sheet_name_2')\n |      \n |      To set the library that is used to write the Excel file,\n |      you can pass the `engine` keyword (the default engine is\n |      automatically chosen depending on the file extension):\n |      \n |      >>> df1.to_excel('output1.xlsx', engine='xlsxwriter')  # doctest: +SKIP\n |  \n |  to_hdf(self, path_or_buf, key, **kwargs)\n |      Write the contained data to an HDF5 file using HDFStore.\n |      \n |      Hierarchical Data Format (HDF) is self-describing, allowing an\n |      application to interpret the structure and contents of a file with\n |      no outside information. One HDF file can hold a mix of related objects\n |      which can be accessed as a group or as individual objects.\n |      \n |      In order to add another DataFrame or Series to an existing HDF file\n |      please use append mode and a different a key.\n |      \n |      For more information see the :ref:`user guide <io.hdf5>`.\n |      \n |      Parameters\n |      ----------\n |      path_or_buf : str or pandas.HDFStore\n |          File path or HDFStore object.\n |      key : str\n |          Identifier for the group in the store.\n |      mode : {'a', 'w', 'r+'}, default 'a'\n |          Mode to open file:\n |      \n |          - 'w': write, a new file is created (an existing file with\n |            the same name would be deleted).\n |          - 'a': append, an existing file is opened for reading and\n |            writing, and if the file does not exist it is created.\n |          - 'r+': similar to 'a', but the file must already exist.\n |      format : {'fixed', 'table'}, default 'fixed'\n |          Possible values:\n |      \n |          - 'fixed': Fixed format. Fast writing/reading. Not-appendable,\n |            nor searchable.\n |          - 'table': Table format. Write as a PyTables Table structure\n |            which may perform worse but allow more flexible operations\n |            like searching / selecting subsets of the data.\n |      append : bool, default False\n |          For Table formats, append the input data to the existing.\n |      data_columns :  list of columns or True, optional\n |          List of columns to create as indexed data columns for on-disk\n |          queries, or True to use all columns. By default only the axes\n |          of the object are indexed. See :ref:`io.hdf5-query-data-columns`.\n |          Applicable only to format='table'.\n |      complevel : {0-9}, optional\n |          Specifies a compression level for data.\n |          A value of 0 disables compression.\n |      complib : {'zlib', 'lzo', 'bzip2', 'blosc'}, default 'zlib'\n |          Specifies the compression library to be used.\n |          As of v0.20.2 these additional compressors for Blosc are supported\n |          (default if no compressor specified: 'blosc:blosclz'):\n |          {'blosc:blosclz', 'blosc:lz4', 'blosc:lz4hc', 'blosc:snappy',\n |          'blosc:zlib', 'blosc:zstd'}.\n |          Specifying a compression library which is not available issues\n |          a ValueError.\n |      fletcher32 : bool, default False\n |          If applying compression use the fletcher32 checksum.\n |      dropna : bool, default False\n |          If true, ALL nan rows will not be written to store.\n |      errors : str, default 'strict'\n |          Specifies how encoding and decoding errors are to be handled.\n |          See the errors argument for :func:`open` for a full list\n |          of options.\n |      \n |      See Also\n |      --------\n |      DataFrame.read_hdf : Read from HDF file.\n |      DataFrame.to_parquet : Write a DataFrame to the binary parquet format.\n |      DataFrame.to_sql : Write to a sql table.\n |      DataFrame.to_feather : Write out feather-format for DataFrames.\n |      DataFrame.to_csv : Write out to a csv file.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]},\n |      ...                   index=['a', 'b', 'c'])\n |      >>> df.to_hdf('data.h5', key='df', mode='w')\n |      \n |      We can add another object to the same file:\n |      \n |      >>> s = pd.Series([1, 2, 3, 4])\n |      >>> s.to_hdf('data.h5', key='s')\n |      \n |      Reading from HDF file:\n |      \n |      >>> pd.read_hdf('data.h5', 'df')\n |      A  B\n |      a  1  4\n |      b  2  5\n |      c  3  6\n |      >>> pd.read_hdf('data.h5', 's')\n |      0    1\n |      1    2\n |      2    3\n |      3    4\n |      dtype: int64\n |      \n |      Deleting file with data:\n |      \n |      >>> import os\n |      >>> os.remove('data.h5')\n |  \n |  to_json(self, path_or_buf=None, orient=None, date_format=None, double_precision=10, force_ascii=True, date_unit='ms', default_handler=None, lines=False, compression='infer', index=True)\n |      Convert the object to a JSON string.\n |      \n |      Note NaN's and None will be converted to null and datetime objects\n |      will be converted to UNIX timestamps.\n |      \n |      Parameters\n |      ----------\n |      path_or_buf : string or file handle, optional\n |          File path or object. If not specified, the result is returned as\n |          a string.\n |      orient : string\n |          Indication of expected JSON string format.\n |      \n |          * Series\n |      \n |            - default is 'index'\n |            - allowed values are: {'split','records','index','table'}\n |      \n |          * DataFrame\n |      \n |            - default is 'columns'\n |            - allowed values are:\n |              {'split','records','index','columns','values','table'}\n |      \n |          * The format of the JSON string\n |      \n |            - 'split' : dict like {'index' -> [index],\n |              'columns' -> [columns], 'data' -> [values]}\n |            - 'records' : list like\n |              [{column -> value}, ... , {column -> value}]\n |            - 'index' : dict like {index -> {column -> value}}\n |            - 'columns' : dict like {column -> {index -> value}}\n |            - 'values' : just the values array\n |            - 'table' : dict like {'schema': {schema}, 'data': {data}}\n |              describing the data, and the data component is\n |              like ``orient='records'``.\n |      \n |              .. versionchanged:: 0.20.0\n |      \n |      date_format : {None, 'epoch', 'iso'}\n |          Type of date conversion. 'epoch' = epoch milliseconds,\n |          'iso' = ISO8601. The default depends on the `orient`. For\n |          ``orient='table'``, the default is 'iso'. For all other orients,\n |          the default is 'epoch'.\n |      double_precision : int, default 10\n |          The number of decimal places to use when encoding\n |          floating point values.\n |      force_ascii : bool, default True\n |          Force encoded string to be ASCII.\n |      date_unit : string, default 'ms' (milliseconds)\n |          The time unit to encode to, governs timestamp and ISO8601\n |          precision.  One of 's', 'ms', 'us', 'ns' for second, millisecond,\n |          microsecond, and nanosecond respectively.\n |      default_handler : callable, default None\n |          Handler to call if object cannot otherwise be converted to a\n |          suitable format for JSON. Should receive a single argument which is\n |          the object to convert and return a serialisable object.\n |      lines : bool, default False\n |          If 'orient' is 'records' write out line delimited json format. Will\n |          throw ValueError if incorrect 'orient' since others are not list\n |          like.\n |      \n |          .. versionadded:: 0.19.0\n |      \n |      compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}\n |      \n |          A string representing the compression to use in the output file,\n |          only used when the first argument is a filename. By default, the\n |          compression is inferred from the filename.\n |      \n |          .. versionadded:: 0.21.0\n |          .. versionchanged:: 0.24.0\n |             'infer' option added and set to default\n |      index : bool, default True\n |          Whether to include the index values in the JSON string. Not\n |          including the index (``index=False``) is only supported when\n |          orient is 'split' or 'table'.\n |      \n |          .. versionadded:: 0.23.0\n |      \n |      Returns\n |      -------\n |      None or str\n |          If path_or_buf is None, returns the resulting json format as a\n |          string. Otherwise returns None.\n |      \n |      See Also\n |      --------\n |      read_json\n |      \n |      Examples\n |      --------\n |      \n |      >>> df = pd.DataFrame([['a', 'b'], ['c', 'd']],\n |      ...                   index=['row 1', 'row 2'],\n |      ...                   columns=['col 1', 'col 2'])\n |      >>> df.to_json(orient='split')\n |      '{\"columns\":[\"col 1\",\"col 2\"],\n |        \"index\":[\"row 1\",\"row 2\"],\n |        \"data\":[[\"a\",\"b\"],[\"c\",\"d\"]]}'\n |      \n |      Encoding/decoding a Dataframe using ``'records'`` formatted JSON.\n |      Note that index labels are not preserved with this encoding.\n |      \n |      >>> df.to_json(orient='records')\n |      '[{\"col 1\":\"a\",\"col 2\":\"b\"},{\"col 1\":\"c\",\"col 2\":\"d\"}]'\n |      \n |      Encoding/decoding a Dataframe using ``'index'`` formatted JSON:\n |      \n |      >>> df.to_json(orient='index')\n |      '{\"row 1\":{\"col 1\":\"a\",\"col 2\":\"b\"},\"row 2\":{\"col 1\":\"c\",\"col 2\":\"d\"}}'\n |      \n |      Encoding/decoding a Dataframe using ``'columns'`` formatted JSON:\n |      \n |      >>> df.to_json(orient='columns')\n |      '{\"col 1\":{\"row 1\":\"a\",\"row 2\":\"c\"},\"col 2\":{\"row 1\":\"b\",\"row 2\":\"d\"}}'\n |      \n |      Encoding/decoding a Dataframe using ``'values'`` formatted JSON:\n |      \n |      >>> df.to_json(orient='values')\n |      '[[\"a\",\"b\"],[\"c\",\"d\"]]'\n |      \n |      Encoding with Table Schema\n |      \n |      >>> df.to_json(orient='table')\n |      '{\"schema\": {\"fields\": [{\"name\": \"index\", \"type\": \"string\"},\n |                              {\"name\": \"col 1\", \"type\": \"string\"},\n |                              {\"name\": \"col 2\", \"type\": \"string\"}],\n |                   \"primaryKey\": \"index\",\n |                   \"pandas_version\": \"0.20.0\"},\n |        \"data\": [{\"index\": \"row 1\", \"col 1\": \"a\", \"col 2\": \"b\"},\n |                 {\"index\": \"row 2\", \"col 1\": \"c\", \"col 2\": \"d\"}]}'\n |  \n |  to_latex(self, buf=None, columns=None, col_space=None, header=True, index=True, na_rep='NaN', formatters=None, float_format=None, sparsify=None, index_names=True, bold_rows=False, column_format=None, longtable=None, escape=None, encoding=None, decimal='.', multicolumn=None, multicolumn_format=None, multirow=None)\n |      Render an object to a LaTeX tabular environment table.\n |      \n |      Render an object to a tabular environment table. You can splice\n |      this into a LaTeX document. Requires \\usepackage{booktabs}.\n |      \n |      .. versionchanged:: 0.20.2\n |         Added to Series\n |      \n |      Parameters\n |      ----------\n |      buf : file descriptor or None\n |          Buffer to write to. If None, the output is returned as a string.\n |      columns : list of label, optional\n |          The subset of columns to write. Writes all columns by default.\n |      col_space : int, optional\n |          The minimum width of each column.\n |      header : bool or list of str, default True\n |          Write out the column names. If a list of strings is given,\n |          it is assumed to be aliases for the column names.\n |      index : bool, default True\n |          Write row names (index).\n |      na_rep : str, default 'NaN'\n |          Missing data representation.\n |      formatters : list of functions or dict of {str: function}, optional\n |          Formatter functions to apply to columns' elements by position or\n |          name. The result of each function must be a unicode string.\n |          List must be of length equal to the number of columns.\n |      float_format : one-parameter function or str, optional, default None\n |          Formatter for floating point numbers. For example\n |          ``float_format=\"%%.2f\"`` and ``float_format=\"{:0.2f}\".format`` will\n |          both result in 0.1234 being formatted as 0.12.\n |      sparsify : bool, optional\n |          Set to False for a DataFrame with a hierarchical index to print\n |          every multiindex key at each row. By default, the value will be\n |          read from the config module.\n |      index_names : bool, default True\n |          Prints the names of the indexes.\n |      bold_rows : bool, default False\n |          Make the row labels bold in the output.\n |      column_format : str, optional\n |          The columns format as specified in `LaTeX table format\n |          <https://en.wikibooks.org/wiki/LaTeX/Tables>`__ e.g. 'rcl' for 3\n |          columns. By default, 'l' will be used for all columns except\n |          columns of numbers, which default to 'r'.\n |      longtable : bool, optional\n |          By default, the value will be read from the pandas config\n |          module. Use a longtable environment instead of tabular. Requires\n |          adding a \\usepackage{longtable} to your LaTeX preamble.\n |      escape : bool, optional\n |          By default, the value will be read from the pandas config\n |          module. When set to False prevents from escaping latex special\n |          characters in column names.\n |      encoding : str, optional\n |          A string representing the encoding to use in the output file,\n |          defaults to 'utf-8'.\n |      decimal : str, default '.'\n |          Character recognized as decimal separator, e.g. ',' in Europe.\n |      \n |          .. versionadded:: 0.18.0\n |      multicolumn : bool, default True\n |          Use \\multicolumn to enhance MultiIndex columns.\n |          The default will be read from the config module.\n |      \n |          .. versionadded:: 0.20.0\n |      multicolumn_format : str, default 'l'\n |          The alignment for multicolumns, similar to `column_format`\n |          The default will be read from the config module.\n |      \n |          .. versionadded:: 0.20.0\n |      multirow : bool, default False\n |          Use \\multirow to enhance MultiIndex rows. Requires adding a\n |          \\usepackage{multirow} to your LaTeX preamble. Will print\n |          centered labels (instead of top-aligned) across the contained\n |          rows, separating groups via clines. The default will be read\n |          from the pandas config module.\n |      \n |          .. versionadded:: 0.20.0\n |      \n |      Returns\n |      -------\n |      str or None\n |          If buf is None, returns the resulting LateX format as a\n |          string. Otherwise returns None.\n |      \n |      See Also\n |      --------\n |      DataFrame.to_string : Render a DataFrame to a console-friendly\n |          tabular output.\n |      DataFrame.to_html : Render a DataFrame as an HTML table.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'name': ['Raphael', 'Donatello'],\n |      ...                    'mask': ['red', 'purple'],\n |      ...                    'weapon': ['sai', 'bo staff']})\n |      >>> df.to_latex(index=False) # doctest: +NORMALIZE_WHITESPACE\n |      '\\\\begin{tabular}{lll}\\n\\\\toprule\\n      name &    mask &    weapon\n |      \\\\\\\\\\n\\\\midrule\\n   Raphael &     red &       sai \\\\\\\\\\n Donatello &\n |       purple &  bo staff \\\\\\\\\\n\\\\bottomrule\\n\\\\end{tabular}\\n'\n |  \n |  to_msgpack(self, path_or_buf=None, encoding='utf-8', **kwargs)\n |      Serialize object to input file path using msgpack format.\n |      \n |      .. deprecated:: 0.25.0\n |      \n |      to_msgpack is deprecated and will be removed in a future version.\n |      It is recommended to use pyarrow for on-the-wire transmission of\n |      pandas objects.\n |      \n |      Parameters\n |      ----------\n |      path : string File path, buffer-like, or None\n |          if None, return generated bytes\n |      append : bool whether to append to an existing msgpack\n |          (default is False)\n |      compress : type of compressor (zlib or blosc), default to None (no\n |          compression)\n |      \n |      Returns\n |      -------\n |      None or bytes\n |          If path_or_buf is None, returns the resulting msgpack format as a\n |          byte string. Otherwise returns None.\n |  \n |  to_pickle(self, path, compression='infer', protocol=4)\n |      Pickle (serialize) object to file.\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          File path where the pickled object will be stored.\n |      compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None},         default 'infer'\n |          A string representing the compression to use in the output file. By\n |          default, infers from the file extension in specified path.\n |      \n |          .. versionadded:: 0.20.0\n |      protocol : int\n |          Int which indicates which protocol should be used by the pickler,\n |          default HIGHEST_PROTOCOL (see [1]_ paragraph 12.1.2). The possible\n |          values are 0, 1, 2, 3, 4. A negative value for the protocol\n |          parameter is equivalent to setting its value to HIGHEST_PROTOCOL.\n |      \n |          .. [1] https://docs.python.org/3/library/pickle.html\n |          .. versionadded:: 0.21.0\n |      \n |      See Also\n |      --------\n |      read_pickle : Load pickled pandas object (or any object) from file.\n |      DataFrame.to_hdf : Write DataFrame to an HDF5 file.\n |      DataFrame.to_sql : Write DataFrame to a SQL database.\n |      DataFrame.to_parquet : Write a DataFrame to the binary parquet format.\n |      \n |      Examples\n |      --------\n |      >>> original_df = pd.DataFrame({\"foo\": range(5), \"bar\": range(5, 10)})\n |      >>> original_df\n |         foo  bar\n |      0    0    5\n |      1    1    6\n |      2    2    7\n |      3    3    8\n |      4    4    9\n |      >>> original_df.to_pickle(\"./dummy.pkl\")\n |      \n |      >>> unpickled_df = pd.read_pickle(\"./dummy.pkl\")\n |      >>> unpickled_df\n |         foo  bar\n |      0    0    5\n |      1    1    6\n |      2    2    7\n |      3    3    8\n |      4    4    9\n |      \n |      >>> import os\n |      >>> os.remove(\"./dummy.pkl\")\n |  \n |  to_sql(self, name, con, schema=None, if_exists='fail', index=True, index_label=None, chunksize=None, dtype=None, method=None)\n |      Write records stored in a DataFrame to a SQL database.\n |      \n |      Databases supported by SQLAlchemy [1]_ are supported. Tables can be\n |      newly created, appended to, or overwritten.\n |      \n |      Parameters\n |      ----------\n |      name : string\n |          Name of SQL table.\n |      con : sqlalchemy.engine.Engine or sqlite3.Connection\n |          Using SQLAlchemy makes it possible to use any DB supported by that\n |          library. Legacy support is provided for sqlite3.Connection objects.\n |      schema : string, optional\n |          Specify the schema (if database flavor supports this). If None, use\n |          default schema.\n |      if_exists : {'fail', 'replace', 'append'}, default 'fail'\n |          How to behave if the table already exists.\n |      \n |          * fail: Raise a ValueError.\n |          * replace: Drop the table before inserting new values.\n |          * append: Insert new values to the existing table.\n |      \n |      index : bool, default True\n |          Write DataFrame index as a column. Uses `index_label` as the column\n |          name in the table.\n |      index_label : string or sequence, default None\n |          Column label for index column(s). If None is given (default) and\n |          `index` is True, then the index names are used.\n |          A sequence should be given if the DataFrame uses MultiIndex.\n |      chunksize : int, optional\n |          Rows will be written in batches of this size at a time. By default,\n |          all rows will be written at once.\n |      dtype : dict, optional\n |          Specifying the datatype for columns. The keys should be the column\n |          names and the values should be the SQLAlchemy types or strings for\n |          the sqlite3 legacy mode.\n |      method : {None, 'multi', callable}, default None\n |          Controls the SQL insertion clause used:\n |      \n |          * None : Uses standard SQL ``INSERT`` clause (one per row).\n |          * 'multi': Pass multiple values in a single ``INSERT`` clause.\n |          * callable with signature ``(pd_table, conn, keys, data_iter)``.\n |      \n |          Details and a sample callable implementation can be found in the\n |          section :ref:`insert method <io.sql.method>`.\n |      \n |          .. versionadded:: 0.24.0\n |      \n |      Raises\n |      ------\n |      ValueError\n |          When the table already exists and `if_exists` is 'fail' (the\n |          default).\n |      \n |      See Also\n |      --------\n |      read_sql : Read a DataFrame from a table.\n |      \n |      Notes\n |      -----\n |      Timezone aware datetime columns will be written as\n |      ``Timestamp with timezone`` type with SQLAlchemy if supported by the\n |      database. Otherwise, the datetimes will be stored as timezone unaware\n |      timestamps local to the original timezone.\n |      \n |      .. versionadded:: 0.24.0\n |      \n |      References\n |      ----------\n |      .. [1] http://docs.sqlalchemy.org\n |      .. [2] https://www.python.org/dev/peps/pep-0249/\n |      \n |      Examples\n |      --------\n |      \n |      Create an in-memory SQLite database.\n |      \n |      >>> from sqlalchemy import create_engine\n |      >>> engine = create_engine('sqlite://', echo=False)\n |      \n |      Create a table from scratch with 3 rows.\n |      \n |      >>> df = pd.DataFrame({'name' : ['User 1', 'User 2', 'User 3']})\n |      >>> df\n |           name\n |      0  User 1\n |      1  User 2\n |      2  User 3\n |      \n |      >>> df.to_sql('users', con=engine)\n |      >>> engine.execute(\"SELECT * FROM users\").fetchall()\n |      [(0, 'User 1'), (1, 'User 2'), (2, 'User 3')]\n |      \n |      >>> df1 = pd.DataFrame({'name' : ['User 4', 'User 5']})\n |      >>> df1.to_sql('users', con=engine, if_exists='append')\n |      >>> engine.execute(\"SELECT * FROM users\").fetchall()\n |      [(0, 'User 1'), (1, 'User 2'), (2, 'User 3'),\n |       (0, 'User 4'), (1, 'User 5')]\n |      \n |      Overwrite the table with just ``df1``.\n |      \n |      >>> df1.to_sql('users', con=engine, if_exists='replace',\n |      ...            index_label='id')\n |      >>> engine.execute(\"SELECT * FROM users\").fetchall()\n |      [(0, 'User 4'), (1, 'User 5')]\n |      \n |      Specify the dtype (especially useful for integers with missing values).\n |      Notice that while pandas is forced to store the data as floating point,\n |      the database supports nullable integers. When fetching the data with\n |      Python, we get back integer scalars.\n |      \n |      >>> df = pd.DataFrame({\"A\": [1, None, 2]})\n |      >>> df\n |           A\n |      0  1.0\n |      1  NaN\n |      2  2.0\n |      \n |      >>> from sqlalchemy.types import Integer\n |      >>> df.to_sql('integers', con=engine, index=False,\n |      ...           dtype={\"A\": Integer()})\n |      \n |      >>> engine.execute(\"SELECT * FROM integers\").fetchall()\n |      [(1,), (None,), (2,)]\n |  \n |  to_xarray(self)\n |      Return an xarray object from the pandas object.\n |      \n |      Returns\n |      -------\n |      xarray.DataArray or xarray.Dataset\n |          Data in the pandas structure converted to Dataset if the object is\n |          a DataFrame, or a DataArray if the object is a Series.\n |      \n |      See Also\n |      --------\n |      DataFrame.to_hdf : Write DataFrame to an HDF5 file.\n |      DataFrame.to_parquet : Write a DataFrame to the binary parquet format.\n |      \n |      Notes\n |      -----\n |      See the `xarray docs <http://xarray.pydata.org/en/stable/>`__\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame([('falcon', 'bird',  389.0, 2),\n |      ...                    ('parrot', 'bird', 24.0, 2),\n |      ...                    ('lion',   'mammal', 80.5, 4),\n |      ...                    ('monkey', 'mammal', np.nan, 4)],\n |      ...                    columns=['name', 'class', 'max_speed',\n |      ...                             'num_legs'])\n |      >>> df\n |           name   class  max_speed  num_legs\n |      0  falcon    bird      389.0         2\n |      1  parrot    bird       24.0         2\n |      2    lion  mammal       80.5         4\n |      3  monkey  mammal        NaN         4\n |      \n |      >>> df.to_xarray()\n |      <xarray.Dataset>\n |      Dimensions:    (index: 4)\n |      Coordinates:\n |        * index      (index) int64 0 1 2 3\n |      Data variables:\n |          name       (index) object 'falcon' 'parrot' 'lion' 'monkey'\n |          class      (index) object 'bird' 'bird' 'mammal' 'mammal'\n |          max_speed  (index) float64 389.0 24.0 80.5 nan\n |          num_legs   (index) int64 2 2 4 4\n |      \n |      >>> df['max_speed'].to_xarray()\n |      <xarray.DataArray 'max_speed' (index: 4)>\n |      array([389. ,  24. ,  80.5,   nan])\n |      Coordinates:\n |        * index    (index) int64 0 1 2 3\n |      \n |      >>> dates = pd.to_datetime(['2018-01-01', '2018-01-01',\n |      ...                         '2018-01-02', '2018-01-02'])\n |      >>> df_multiindex = pd.DataFrame({'date': dates,\n |      ...                    'animal': ['falcon', 'parrot', 'falcon',\n |      ...                               'parrot'],\n |      ...                    'speed': [350, 18, 361, 15]}).set_index(['date',\n |      ...                                                    'animal'])\n |      >>> df_multiindex\n |                         speed\n |      date       animal\n |      2018-01-01 falcon    350\n |                 parrot     18\n |      2018-01-02 falcon    361\n |                 parrot     15\n |      \n |      >>> df_multiindex.to_xarray()\n |      <xarray.Dataset>\n |      Dimensions:  (animal: 2, date: 2)\n |      Coordinates:\n |        * date     (date) datetime64[ns] 2018-01-01 2018-01-02\n |        * animal   (animal) object 'falcon' 'parrot'\n |      Data variables:\n |          speed    (date, animal) int64 350 18 361 15\n |  \n |  truncate(self, before=None, after=None, axis=None, copy=True)\n |      Truncate a Series or DataFrame before and after some index value.\n |      \n |      This is a useful shorthand for boolean indexing based on index\n |      values above or below certain thresholds.\n |      \n |      Parameters\n |      ----------\n |      before : date, string, int\n |          Truncate all rows before this index value.\n |      after : date, string, int\n |          Truncate all rows after this index value.\n |      axis : {0 or 'index', 1 or 'columns'}, optional\n |          Axis to truncate. Truncates the index (rows) by default.\n |      copy : boolean, default is True,\n |          Return a copy of the truncated section.\n |      \n |      Returns\n |      -------\n |      type of caller\n |          The truncated Series or DataFrame.\n |      \n |      See Also\n |      --------\n |      DataFrame.loc : Select a subset of a DataFrame by label.\n |      DataFrame.iloc : Select a subset of a DataFrame by position.\n |      \n |      Notes\n |      -----\n |      If the index being truncated contains only datetime values,\n |      `before` and `after` may be specified as strings instead of\n |      Timestamps.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'A': ['a', 'b', 'c', 'd', 'e'],\n |      ...                    'B': ['f', 'g', 'h', 'i', 'j'],\n |      ...                    'C': ['k', 'l', 'm', 'n', 'o']},\n |      ...                    index=[1, 2, 3, 4, 5])\n |      >>> df\n |         A  B  C\n |      1  a  f  k\n |      2  b  g  l\n |      3  c  h  m\n |      4  d  i  n\n |      5  e  j  o\n |      \n |      >>> df.truncate(before=2, after=4)\n |         A  B  C\n |      2  b  g  l\n |      3  c  h  m\n |      4  d  i  n\n |      \n |      The columns of a DataFrame can be truncated.\n |      \n |      >>> df.truncate(before=\"A\", after=\"B\", axis=\"columns\")\n |         A  B\n |      1  a  f\n |      2  b  g\n |      3  c  h\n |      4  d  i\n |      5  e  j\n |      \n |      For Series, only rows can be truncated.\n |      \n |      >>> df['A'].truncate(before=2, after=4)\n |      2    b\n |      3    c\n |      4    d\n |      Name: A, dtype: object\n |      \n |      The index values in ``truncate`` can be datetimes or string\n |      dates.\n |      \n |      >>> dates = pd.date_range('2016-01-01', '2016-02-01', freq='s')\n |      >>> df = pd.DataFrame(index=dates, data={'A': 1})\n |      >>> df.tail()\n |                           A\n |      2016-01-31 23:59:56  1\n |      2016-01-31 23:59:57  1\n |      2016-01-31 23:59:58  1\n |      2016-01-31 23:59:59  1\n |      2016-02-01 00:00:00  1\n |      \n |      >>> df.truncate(before=pd.Timestamp('2016-01-05'),\n |      ...             after=pd.Timestamp('2016-01-10')).tail()\n |                           A\n |      2016-01-09 23:59:56  1\n |      2016-01-09 23:59:57  1\n |      2016-01-09 23:59:58  1\n |      2016-01-09 23:59:59  1\n |      2016-01-10 00:00:00  1\n |      \n |      Because the index is a DatetimeIndex containing only dates, we can\n |      specify `before` and `after` as strings. They will be coerced to\n |      Timestamps before truncation.\n |      \n |      >>> df.truncate('2016-01-05', '2016-01-10').tail()\n |                           A\n |      2016-01-09 23:59:56  1\n |      2016-01-09 23:59:57  1\n |      2016-01-09 23:59:58  1\n |      2016-01-09 23:59:59  1\n |      2016-01-10 00:00:00  1\n |      \n |      Note that ``truncate`` assumes a 0 value for any unspecified time\n |      component (midnight). This differs from partial string slicing, which\n |      returns any partially matching dates.\n |      \n |      >>> df.loc['2016-01-05':'2016-01-10', :].tail()\n |                           A\n |      2016-01-10 23:59:55  1\n |      2016-01-10 23:59:56  1\n |      2016-01-10 23:59:57  1\n |      2016-01-10 23:59:58  1\n |      2016-01-10 23:59:59  1\n |  \n |  tshift(self, periods=1, freq=None, axis=0)\n |      Shift the time index, using the index's frequency if available.\n |      \n |      Parameters\n |      ----------\n |      periods : int\n |          Number of periods to move, can be positive or negative\n |      freq : DateOffset, timedelta, or time rule string, default None\n |          Increment to use from the tseries module or time rule (e.g. 'EOM')\n |      axis : int or basestring\n |          Corresponds to the axis that contains the Index\n |      \n |      Returns\n |      -------\n |      shifted : Series/DataFrame\n |      \n |      Notes\n |      -----\n |      If freq is not specified then tries to use the freq or inferred_freq\n |      attributes of the index. If neither of those attributes exist, a\n |      ValueError is thrown\n |  \n |  tz_convert(self, tz, axis=0, level=None, copy=True)\n |      Convert tz-aware axis to target time zone.\n |      \n |      Parameters\n |      ----------\n |      tz : string or pytz.timezone object\n |      axis : the axis to convert\n |      level : int, str, default None\n |          If axis ia a MultiIndex, convert a specific level. Otherwise\n |          must be None\n |      copy : boolean, default True\n |          Also make a copy of the underlying data\n |      \n |      Returns\n |      -------\n |      %(klass)s\n |          Object with time zone converted axis.\n |      \n |      Raises\n |      ------\n |      TypeError\n |          If the axis is tz-naive.\n |  \n |  tz_localize(self, tz, axis=0, level=None, copy=True, ambiguous='raise', nonexistent='raise')\n |      Localize tz-naive index of a Series or DataFrame to target time zone.\n |      \n |      This operation localizes the Index. To localize the values in a\n |      timezone-naive Series, use :meth:`Series.dt.tz_localize`.\n |      \n |      Parameters\n |      ----------\n |      tz : string or pytz.timezone object\n |      axis : the axis to localize\n |      level : int, str, default None\n |          If axis ia a MultiIndex, localize a specific level. Otherwise\n |          must be None\n |      copy : boolean, default True\n |          Also make a copy of the underlying data\n |      ambiguous : 'infer', bool-ndarray, 'NaT', default 'raise'\n |          When clocks moved backward due to DST, ambiguous times may arise.\n |          For example in Central European Time (UTC+01), when going from\n |          03:00 DST to 02:00 non-DST, 02:30:00 local time occurs both at\n |          00:30:00 UTC and at 01:30:00 UTC. In such a situation, the\n |          `ambiguous` parameter dictates how ambiguous times should be\n |          handled.\n |      \n |          - 'infer' will attempt to infer fall dst-transition hours based on\n |            order\n |          - bool-ndarray where True signifies a DST time, False designates\n |            a non-DST time (note that this flag is only applicable for\n |            ambiguous times)\n |          - 'NaT' will return NaT where there are ambiguous times\n |          - 'raise' will raise an AmbiguousTimeError if there are ambiguous\n |            times\n |      nonexistent : str, default 'raise'\n |          A nonexistent time does not exist in a particular timezone\n |          where clocks moved forward due to DST. Valid values are:\n |      \n |          - 'shift_forward' will shift the nonexistent time forward to the\n |            closest existing time\n |          - 'shift_backward' will shift the nonexistent time backward to the\n |            closest existing time\n |          - 'NaT' will return NaT where there are nonexistent times\n |          - timedelta objects will shift nonexistent times by the timedelta\n |          - 'raise' will raise an NonExistentTimeError if there are\n |            nonexistent times\n |      \n |          .. versionadded:: 0.24.0\n |      \n |      Returns\n |      -------\n |      Series or DataFrame\n |          Same type as the input.\n |      \n |      Raises\n |      ------\n |      TypeError\n |          If the TimeSeries is tz-aware and tz is not None.\n |      \n |      Examples\n |      --------\n |      \n |      Localize local times:\n |      \n |      >>> s = pd.Series([1],\n |      ... index=pd.DatetimeIndex(['2018-09-15 01:30:00']))\n |      >>> s.tz_localize('CET')\n |      2018-09-15 01:30:00+02:00    1\n |      dtype: int64\n |      \n |      Be careful with DST changes. When there is sequential data, pandas\n |      can infer the DST time:\n |      \n |      >>> s = pd.Series(range(7), index=pd.DatetimeIndex([\n |      ... '2018-10-28 01:30:00',\n |      ... '2018-10-28 02:00:00',\n |      ... '2018-10-28 02:30:00',\n |      ... '2018-10-28 02:00:00',\n |      ... '2018-10-28 02:30:00',\n |      ... '2018-10-28 03:00:00',\n |      ... '2018-10-28 03:30:00']))\n |      >>> s.tz_localize('CET', ambiguous='infer')\n |      2018-10-28 01:30:00+02:00    0\n |      2018-10-28 02:00:00+02:00    1\n |      2018-10-28 02:30:00+02:00    2\n |      2018-10-28 02:00:00+01:00    3\n |      2018-10-28 02:30:00+01:00    4\n |      2018-10-28 03:00:00+01:00    5\n |      2018-10-28 03:30:00+01:00    6\n |      dtype: int64\n |      \n |      In some cases, inferring the DST is impossible. In such cases, you can\n |      pass an ndarray to the ambiguous parameter to set the DST explicitly\n |      \n |      >>> s = pd.Series(range(3), index=pd.DatetimeIndex([\n |      ... '2018-10-28 01:20:00',\n |      ... '2018-10-28 02:36:00',\n |      ... '2018-10-28 03:46:00']))\n |      >>> s.tz_localize('CET', ambiguous=np.array([True, True, False]))\n |      2018-10-28 01:20:00+02:00    0\n |      2018-10-28 02:36:00+02:00    1\n |      2018-10-28 03:46:00+01:00    2\n |      dtype: int64\n |      \n |      If the DST transition causes nonexistent times, you can shift these\n |      dates forward or backwards with a timedelta object or `'shift_forward'`\n |      or `'shift_backwards'`.\n |      >>> s = pd.Series(range(2), index=pd.DatetimeIndex([\n |      ... '2015-03-29 02:30:00',\n |      ... '2015-03-29 03:30:00']))\n |      >>> s.tz_localize('Europe/Warsaw', nonexistent='shift_forward')\n |      2015-03-29 03:00:00+02:00    0\n |      2015-03-29 03:30:00+02:00    1\n |      dtype: int64\n |      >>> s.tz_localize('Europe/Warsaw', nonexistent='shift_backward')\n |      2015-03-29 01:59:59.999999999+01:00    0\n |      2015-03-29 03:30:00+02:00              1\n |      dtype: int64\n |      >>> s.tz_localize('Europe/Warsaw', nonexistent=pd.Timedelta('1H'))\n |      2015-03-29 03:30:00+02:00    0\n |      2015-03-29 03:30:00+02:00    1\n |      dtype: int64\n |  \n |  where(self, cond, other=nan, inplace=False, axis=None, level=None, errors='raise', try_cast=False)\n |      Replace values where the condition is False.\n |      \n |      Parameters\n |      ----------\n |      cond : boolean Series/DataFrame, array-like, or callable\n |          Where `cond` is True, keep the original value. Where\n |          False, replace with corresponding value from `other`.\n |          If `cond` is callable, it is computed on the Series/DataFrame and\n |          should return boolean Series/DataFrame or array. The callable must\n |          not change input Series/DataFrame (though pandas doesn't check it).\n |      \n |          .. versionadded:: 0.18.1\n |              A callable can be used as cond.\n |      \n |      other : scalar, Series/DataFrame, or callable\n |          Entries where `cond` is False are replaced with\n |          corresponding value from `other`.\n |          If other is callable, it is computed on the Series/DataFrame and\n |          should return scalar or Series/DataFrame. The callable must not\n |          change input Series/DataFrame (though pandas doesn't check it).\n |      \n |          .. versionadded:: 0.18.1\n |              A callable can be used as other.\n |      \n |      inplace : bool, default False\n |          Whether to perform the operation in place on the data.\n |      axis : int, default None\n |          Alignment axis if needed.\n |      level : int, default None\n |          Alignment level if needed.\n |      errors : str, {'raise', 'ignore'}, default 'raise'\n |          Note that currently this parameter won't affect\n |          the results and will always coerce to a suitable dtype.\n |      \n |          - 'raise' : allow exceptions to be raised.\n |          - 'ignore' : suppress exceptions. On error return original object.\n |      \n |      try_cast : bool, default False\n |          Try to cast the result back to the input type (if possible).\n |      \n |      Returns\n |      -------\n |      Same type as caller\n |      \n |      See Also\n |      --------\n |      :func:`DataFrame.mask` : Return an object of same shape as\n |          self.\n |      \n |      Notes\n |      -----\n |      The where method is an application of the if-then idiom. For each\n |      element in the calling DataFrame, if ``cond`` is ``True`` the\n |      element is used; otherwise the corresponding element from the DataFrame\n |      ``other`` is used.\n |      \n |      The signature for :func:`DataFrame.where` differs from\n |      :func:`numpy.where`. Roughly ``df1.where(m, df2)`` is equivalent to\n |      ``np.where(m, df1, df2)``.\n |      \n |      For further details and examples see the ``where`` documentation in\n |      :ref:`indexing <indexing.where_mask>`.\n |      \n |      Examples\n |      --------\n |      >>> s = pd.Series(range(5))\n |      >>> s.where(s > 0)\n |      0    NaN\n |      1    1.0\n |      2    2.0\n |      3    3.0\n |      4    4.0\n |      dtype: float64\n |      \n |      >>> s.mask(s > 0)\n |      0    0.0\n |      1    NaN\n |      2    NaN\n |      3    NaN\n |      4    NaN\n |      dtype: float64\n |      \n |      >>> s.where(s > 1, 10)\n |      0    10\n |      1    10\n |      2    2\n |      3    3\n |      4    4\n |      dtype: int64\n |      \n |      >>> df = pd.DataFrame(np.arange(10).reshape(-1, 2), columns=['A', 'B'])\n |      >>> df\n |         A  B\n |      0  0  1\n |      1  2  3\n |      2  4  5\n |      3  6  7\n |      4  8  9\n |      >>> m = df % 3 == 0\n |      >>> df.where(m, -df)\n |         A  B\n |      0  0 -1\n |      1 -2  3\n |      2 -4 -5\n |      3  6 -7\n |      4 -8  9\n |      >>> df.where(m, -df) == np.where(m, df, -df)\n |            A     B\n |      0  True  True\n |      1  True  True\n |      2  True  True\n |      3  True  True\n |      4  True  True\n |      >>> df.where(m, -df) == df.mask(~m, -df)\n |            A     B\n |      0  True  True\n |      1  True  True\n |      2  True  True\n |      3  True  True\n |      4  True  True\n |  \n |  xs(self, key, axis=0, level=None, drop_level=True)\n |      Return cross-section from the Series/DataFrame.\n |      \n |      This method takes a `key` argument to select data at a particular\n |      level of a MultiIndex.\n |      \n |      Parameters\n |      ----------\n |      key : label or tuple of label\n |          Label contained in the index, or partially in a MultiIndex.\n |      axis : {0 or 'index', 1 or 'columns'}, default 0\n |          Axis to retrieve cross-section on.\n |      level : object, defaults to first n levels (n=1 or len(key))\n |          In case of a key partially contained in a MultiIndex, indicate\n |          which levels are used. Levels can be referred by label or position.\n |      drop_level : bool, default True\n |          If False, returns object with same levels as self.\n |      \n |      Returns\n |      -------\n |      Series or DataFrame\n |          Cross-section from the original Series or DataFrame\n |          corresponding to the selected index levels.\n |      \n |      See Also\n |      --------\n |      DataFrame.loc : Access a group of rows and columns\n |          by label(s) or a boolean array.\n |      DataFrame.iloc : Purely integer-location based indexing\n |          for selection by position.\n |      \n |      Notes\n |      -----\n |      `xs` can not be used to set values.\n |      \n |      MultiIndex Slicers is a generic way to get/set values on\n |      any level or levels.\n |      It is a superset of `xs` functionality, see\n |      :ref:`MultiIndex Slicers <advanced.mi_slicers>`.\n |      \n |      Examples\n |      --------\n |      >>> d = {'num_legs': [4, 4, 2, 2],\n |      ...      'num_wings': [0, 0, 2, 2],\n |      ...      'class': ['mammal', 'mammal', 'mammal', 'bird'],\n |      ...      'animal': ['cat', 'dog', 'bat', 'penguin'],\n |      ...      'locomotion': ['walks', 'walks', 'flies', 'walks']}\n |      >>> df = pd.DataFrame(data=d)\n |      >>> df = df.set_index(['class', 'animal', 'locomotion'])\n |      >>> df\n |                                 num_legs  num_wings\n |      class  animal  locomotion\n |      mammal cat     walks              4          0\n |             dog     walks              4          0\n |             bat     flies              2          2\n |      bird   penguin walks              2          2\n |      \n |      Get values at specified index\n |      \n |      >>> df.xs('mammal')\n |                         num_legs  num_wings\n |      animal locomotion\n |      cat    walks              4          0\n |      dog    walks              4          0\n |      bat    flies              2          2\n |      \n |      Get values at several indexes\n |      \n |      >>> df.xs(('mammal', 'dog'))\n |                  num_legs  num_wings\n |      locomotion\n |      walks              4          0\n |      \n |      Get values at specified index and level\n |      \n |      >>> df.xs('cat', level=1)\n |                         num_legs  num_wings\n |      class  locomotion\n |      mammal walks              4          0\n |      \n |      Get values at several indexes and levels\n |      \n |      >>> df.xs(('bird', 'walks'),\n |      ...       level=[0, 'locomotion'])\n |               num_legs  num_wings\n |      animal\n |      penguin         2          2\n |      \n |      Get values at specified column and axis\n |      \n |      >>> df.xs('num_wings', axis=1)\n |      class   animal   locomotion\n |      mammal  cat      walks         0\n |              dog      walks         0\n |              bat      flies         2\n |      bird    penguin  walks         2\n |      Name: num_wings, dtype: int64\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from pandas.core.generic.NDFrame:\n |  \n |  at\n |      Access a single value for a row/column label pair.\n |      \n |      Similar to ``loc``, in that both provide label-based lookups. Use\n |      ``at`` if you only need to get or set a single value in a DataFrame\n |      or Series.\n |      \n |      Raises\n |      ------\n |      KeyError\n |          When label does not exist in DataFrame\n |      \n |      See Also\n |      --------\n |      DataFrame.iat : Access a single value for a row/column pair by integer\n |          position.\n |      DataFrame.loc : Access a group of rows and columns by label(s).\n |      Series.at : Access a single value using a label.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]],\n |      ...                   index=[4, 5, 6], columns=['A', 'B', 'C'])\n |      >>> df\n |          A   B   C\n |      4   0   2   3\n |      5   0   4   1\n |      6  10  20  30\n |      \n |      Get value at specified row/column pair\n |      \n |      >>> df.at[4, 'B']\n |      2\n |      \n |      Set value at specified row/column pair\n |      \n |      >>> df.at[4, 'B'] = 10\n |      >>> df.at[4, 'B']\n |      10\n |      \n |      Get value within a Series\n |      \n |      >>> df.loc[5].at['B']\n |      4\n |  \n |  blocks\n |      Internal property, property synonym for as_blocks().\n |      \n |      .. deprecated:: 0.21.0\n |  \n |  dtypes\n |      Return the dtypes in the DataFrame.\n |      \n |      This returns a Series with the data type of each column.\n |      The result's index is the original DataFrame's columns. Columns\n |      with mixed types are stored with the ``object`` dtype. See\n |      :ref:`the User Guide <basics.dtypes>` for more.\n |      \n |      Returns\n |      -------\n |      pandas.Series\n |          The data type of each column.\n |      \n |      See Also\n |      --------\n |      DataFrame.ftypes : Dtype and sparsity information.\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame({'float': [1.0],\n |      ...                    'int': [1],\n |      ...                    'datetime': [pd.Timestamp('20180310')],\n |      ...                    'string': ['foo']})\n |      >>> df.dtypes\n |      float              float64\n |      int                  int64\n |      datetime    datetime64[ns]\n |      string              object\n |      dtype: object\n |  \n |  empty\n |      Indicator whether DataFrame is empty.\n |      \n |      True if DataFrame is entirely empty (no items), meaning any of the\n |      axes are of length 0.\n |      \n |      Returns\n |      -------\n |      bool\n |          If DataFrame is empty, return True, if not return False.\n |      \n |      See Also\n |      --------\n |      Series.dropna\n |      DataFrame.dropna\n |      \n |      Notes\n |      -----\n |      If DataFrame contains only NaNs, it is still not considered empty. See\n |      the example below.\n |      \n |      Examples\n |      --------\n |      An example of an actual empty DataFrame. Notice the index is empty:\n |      \n |      >>> df_empty = pd.DataFrame({'A' : []})\n |      >>> df_empty\n |      Empty DataFrame\n |      Columns: [A]\n |      Index: []\n |      >>> df_empty.empty\n |      True\n |      \n |      If we only have NaNs in our DataFrame, it is not considered empty! We\n |      will need to drop the NaNs to make the DataFrame empty:\n |      \n |      >>> df = pd.DataFrame({'A' : [np.nan]})\n |      >>> df\n |          A\n |      0 NaN\n |      >>> df.empty\n |      False\n |      >>> df.dropna().empty\n |      True\n |  \n |  ftypes\n |      Return the ftypes (indication of sparse/dense and dtype) in DataFrame.\n |      \n |      .. deprecated:: 0.25.0\n |         Use :func:`dtypes` instead.\n |      \n |      This returns a Series with the data type of each column.\n |      The result's index is the original DataFrame's columns. Columns\n |      with mixed types are stored with the ``object`` dtype.  See\n |      :ref:`the User Guide <basics.dtypes>` for more.\n |      \n |      Returns\n |      -------\n |      pandas.Series\n |          The data type and indication of sparse/dense of each column.\n |      \n |      See Also\n |      --------\n |      DataFrame.dtypes: Series with just dtype information.\n |      SparseDataFrame : Container for sparse tabular data.\n |      \n |      Notes\n |      -----\n |      Sparse data should have the same dtypes as its dense representation.\n |      \n |      Examples\n |      --------\n |      >>> arr = np.random.RandomState(0).randn(100, 4)\n |      >>> arr[arr < .8] = np.nan\n |      >>> pd.DataFrame(arr).ftypes\n |      0    float64:dense\n |      1    float64:dense\n |      2    float64:dense\n |      3    float64:dense\n |      dtype: object\n |      \n |      >>> pd.SparseDataFrame(arr).ftypes  # doctest: +SKIP\n |      0    float64:sparse\n |      1    float64:sparse\n |      2    float64:sparse\n |      3    float64:sparse\n |      dtype: object\n |  \n |  iat\n |      Access a single value for a row/column pair by integer position.\n |      \n |      Similar to ``iloc``, in that both provide integer-based lookups. Use\n |      ``iat`` if you only need to get or set a single value in a DataFrame\n |      or Series.\n |      \n |      Raises\n |      ------\n |      IndexError\n |          When integer position is out of bounds\n |      \n |      See Also\n |      --------\n |      DataFrame.at : Access a single value for a row/column label pair.\n |      DataFrame.loc : Access a group of rows and columns by label(s).\n |      DataFrame.iloc : Access a group of rows and columns by integer position(s).\n |      \n |      Examples\n |      --------\n |      >>> df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]],\n |      ...                   columns=['A', 'B', 'C'])\n |      >>> df\n |          A   B   C\n |      0   0   2   3\n |      1   0   4   1\n |      2  10  20  30\n |      \n |      Get value at specified row/column pair\n |      \n |      >>> df.iat[1, 2]\n |      1\n |      \n |      Set value at specified row/column pair\n |      \n |      >>> df.iat[1, 2] = 10\n |      >>> df.iat[1, 2]\n |      10\n |      \n |      Get value within a series\n |      \n |      >>> df.loc[0].iat[1]\n |      2\n |  \n |  iloc\n |      Purely integer-location based indexing for selection by position.\n |      \n |      ``.iloc[]`` is primarily integer position based (from ``0`` to\n |      ``length-1`` of the axis), but may also be used with a boolean\n |      array.\n |      \n |      Allowed inputs are:\n |      \n |      - An integer, e.g. ``5``.\n |      - A list or array of integers, e.g. ``[4, 3, 0]``.\n |      - A slice object with ints, e.g. ``1:7``.\n |      - A boolean array.\n |      - A ``callable`` function with one argument (the calling Series or\n |        DataFrame) and that returns valid output for indexing (one of the above).\n |        This is useful in method chains, when you don't have a reference to the\n |        calling object, but would like to base your selection on some value.\n |      \n |      ``.iloc`` will raise ``IndexError`` if a requested indexer is\n |      out-of-bounds, except *slice* indexers which allow out-of-bounds\n |      indexing (this conforms with python/numpy *slice* semantics).\n |      \n |      See more at :ref:`Selection by Position <indexing.integer>`.\n |      \n |      See Also\n |      --------\n |      DataFrame.iat : Fast integer location scalar accessor.\n |      DataFrame.loc : Purely label-location based indexer for selection by label.\n |      Series.iloc : Purely integer-location based indexing for\n |                     selection by position.\n |      \n |      Examples\n |      --------\n |      \n |      >>> mydict = [{'a': 1, 'b': 2, 'c': 3, 'd': 4},\n |      ...           {'a': 100, 'b': 200, 'c': 300, 'd': 400},\n |      ...           {'a': 1000, 'b': 2000, 'c': 3000, 'd': 4000 }]\n |      >>> df = pd.DataFrame(mydict)\n |      >>> df\n |            a     b     c     d\n |      0     1     2     3     4\n |      1   100   200   300   400\n |      2  1000  2000  3000  4000\n |      \n |      **Indexing just the rows**\n |      \n |      With a scalar integer.\n |      \n |      >>> type(df.iloc[0])\n |      <class 'pandas.core.series.Series'>\n |      >>> df.iloc[0]\n |      a    1\n |      b    2\n |      c    3\n |      d    4\n |      Name: 0, dtype: int64\n |      \n |      With a list of integers.\n |      \n |      >>> df.iloc[[0]]\n |         a  b  c  d\n |      0  1  2  3  4\n |      >>> type(df.iloc[[0]])\n |      <class 'pandas.core.frame.DataFrame'>\n |      \n |      >>> df.iloc[[0, 1]]\n |           a    b    c    d\n |      0    1    2    3    4\n |      1  100  200  300  400\n |      \n |      With a `slice` object.\n |      \n |      >>> df.iloc[:3]\n |            a     b     c     d\n |      0     1     2     3     4\n |      1   100   200   300   400\n |      2  1000  2000  3000  4000\n |      \n |      With a boolean mask the same length as the index.\n |      \n |      >>> df.iloc[[True, False, True]]\n |            a     b     c     d\n |      0     1     2     3     4\n |      2  1000  2000  3000  4000\n |      \n |      With a callable, useful in method chains. The `x` passed\n |      to the ``lambda`` is the DataFrame being sliced. This selects\n |      the rows whose index label even.\n |      \n |      >>> df.iloc[lambda x: x.index % 2 == 0]\n |            a     b     c     d\n |      0     1     2     3     4\n |      2  1000  2000  3000  4000\n |      \n |      **Indexing both axes**\n |      \n |      You can mix the indexer types for the index and columns. Use ``:`` to\n |      select the entire axis.\n |      \n |      With scalar integers.\n |      \n |      >>> df.iloc[0, 1]\n |      2\n |      \n |      With lists of integers.\n |      \n |      >>> df.iloc[[0, 2], [1, 3]]\n |            b     d\n |      0     2     4\n |      2  2000  4000\n |      \n |      With `slice` objects.\n |      \n |      >>> df.iloc[1:3, 0:3]\n |            a     b     c\n |      1   100   200   300\n |      2  1000  2000  3000\n |      \n |      With a boolean array whose length matches the columns.\n |      \n |      >>> df.iloc[:, [True, False, True, False]]\n |            a     c\n |      0     1     3\n |      1   100   300\n |      2  1000  3000\n |      \n |      With a callable function that expects the Series or DataFrame.\n |      \n |      >>> df.iloc[:, lambda df: [0, 2]]\n |            a     c\n |      0     1     3\n |      1   100   300\n |      2  1000  3000\n |  \n |  is_copy\n |      Return the copy.\n |  \n |  ix\n |      A primarily label-location based indexer, with integer position\n |      fallback.\n |      \n |      Warning: Starting in 0.20.0, the .ix indexer is deprecated, in\n |      favor of the more strict .iloc and .loc indexers.\n |      \n |      ``.ix[]`` supports mixed integer and label based access. It is\n |      primarily label based, but will fall back to integer positional\n |      access unless the corresponding axis is of integer type.\n |      \n |      ``.ix`` is the most general indexer and will support any of the\n |      inputs in ``.loc`` and ``.iloc``. ``.ix`` also supports floating\n |      point label schemes. ``.ix`` is exceptionally useful when dealing\n |      with mixed positional and label based hierarchical indexes.\n |      \n |      However, when an axis is integer based, ONLY label based access\n |      and not positional access is supported. Thus, in such cases, it's\n |      usually better to be explicit and use ``.iloc`` or ``.loc``.\n |      \n |      See more at :ref:`Advanced Indexing <advanced>`.\n |  \n |  loc\n |      Access a group of rows and columns by label(s) or a boolean array.\n |      \n |      ``.loc[]`` is primarily label based, but may also be used with a\n |      boolean array.\n |      \n |      Allowed inputs are:\n |      \n |      - A single label, e.g. ``5`` or ``'a'``, (note that ``5`` is\n |        interpreted as a *label* of the index, and **never** as an\n |        integer position along the index).\n |      - A list or array of labels, e.g. ``['a', 'b', 'c']``.\n |      - A slice object with labels, e.g. ``'a':'f'``.\n |      \n |        .. warning:: Note that contrary to usual python slices, **both** the\n |            start and the stop are included\n |      \n |      - A boolean array of the same length as the axis being sliced,\n |        e.g. ``[True, False, True]``.\n |      - A ``callable`` function with one argument (the calling Series or\n |        DataFrame) and that returns valid output for indexing (one of the above)\n |      \n |      See more at :ref:`Selection by Label <indexing.label>`\n |      \n |      Raises\n |      ------\n |      KeyError:\n |          when any items are not found\n |      \n |      See Also\n |      --------\n |      DataFrame.at : Access a single value for a row/column label pair.\n |      DataFrame.iloc : Access group of rows and columns by integer position(s).\n |      DataFrame.xs : Returns a cross-section (row(s) or column(s)) from the\n |          Series/DataFrame.\n |      Series.loc : Access group of values using labels.\n |      \n |      Examples\n |      --------\n |      **Getting values**\n |      \n |      >>> df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],\n |      ...      index=['cobra', 'viper', 'sidewinder'],\n |      ...      columns=['max_speed', 'shield'])\n |      >>> df\n |                  max_speed  shield\n |      cobra               1       2\n |      viper               4       5\n |      sidewinder          7       8\n |      \n |      Single label. Note this returns the row as a Series.\n |      \n |      >>> df.loc['viper']\n |      max_speed    4\n |      shield       5\n |      Name: viper, dtype: int64\n |      \n |      List of labels. Note using ``[[]]`` returns a DataFrame.\n |      \n |      >>> df.loc[['viper', 'sidewinder']]\n |                  max_speed  shield\n |      viper               4       5\n |      sidewinder          7       8\n |      \n |      Single label for row and column\n |      \n |      >>> df.loc['cobra', 'shield']\n |      2\n |      \n |      Slice with labels for row and single label for column. As mentioned\n |      above, note that both the start and stop of the slice are included.\n |      \n |      >>> df.loc['cobra':'viper', 'max_speed']\n |      cobra    1\n |      viper    4\n |      Name: max_speed, dtype: int64\n |      \n |      Boolean list with the same length as the row axis\n |      \n |      >>> df.loc[[False, False, True]]\n |                  max_speed  shield\n |      sidewinder          7       8\n |      \n |      Conditional that returns a boolean Series\n |      \n |      >>> df.loc[df['shield'] > 6]\n |                  max_speed  shield\n |      sidewinder          7       8\n |      \n |      Conditional that returns a boolean Series with column labels specified\n |      \n |      >>> df.loc[df['shield'] > 6, ['max_speed']]\n |                  max_speed\n |      sidewinder          7\n |      \n |      Callable that returns a boolean Series\n |      \n |      >>> df.loc[lambda df: df['shield'] == 8]\n |                  max_speed  shield\n |      sidewinder          7       8\n |      \n |      **Setting values**\n |      \n |      Set value for all items matching the list of labels\n |      \n |      >>> df.loc[['viper', 'sidewinder'], ['shield']] = 50\n |      >>> df\n |                  max_speed  shield\n |      cobra               1       2\n |      viper               4      50\n |      sidewinder          7      50\n |      \n |      Set value for an entire row\n |      \n |      >>> df.loc['cobra'] = 10\n |      >>> df\n |                  max_speed  shield\n |      cobra              10      10\n |      viper               4      50\n |      sidewinder          7      50\n |      \n |      Set value for an entire column\n |      \n |      >>> df.loc[:, 'max_speed'] = 30\n |      >>> df\n |                  max_speed  shield\n |      cobra              30      10\n |      viper              30      50\n |      sidewinder         30      50\n |      \n |      Set value for rows matching callable condition\n |      \n |      >>> df.loc[df['shield'] > 35] = 0\n |      >>> df\n |                  max_speed  shield\n |      cobra              30      10\n |      viper               0       0\n |      sidewinder          0       0\n |      \n |      **Getting values on a DataFrame with an index that has integer labels**\n |      \n |      Another example using integers for the index\n |      \n |      >>> df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],\n |      ...      index=[7, 8, 9], columns=['max_speed', 'shield'])\n |      >>> df\n |         max_speed  shield\n |      7          1       2\n |      8          4       5\n |      9          7       8\n |      \n |      Slice with integer labels for rows. As mentioned above, note that both\n |      the start and stop of the slice are included.\n |      \n |      >>> df.loc[7:9]\n |         max_speed  shield\n |      7          1       2\n |      8          4       5\n |      9          7       8\n |      \n |      **Getting values with a MultiIndex**\n |      \n |      A number of examples using a DataFrame with a MultiIndex\n |      \n |      >>> tuples = [\n |      ...    ('cobra', 'mark i'), ('cobra', 'mark ii'),\n |      ...    ('sidewinder', 'mark i'), ('sidewinder', 'mark ii'),\n |      ...    ('viper', 'mark ii'), ('viper', 'mark iii')\n |      ... ]\n |      >>> index = pd.MultiIndex.from_tuples(tuples)\n |      >>> values = [[12, 2], [0, 4], [10, 20],\n |      ...         [1, 4], [7, 1], [16, 36]]\n |      >>> df = pd.DataFrame(values, columns=['max_speed', 'shield'], index=index)\n |      >>> df\n |                           max_speed  shield\n |      cobra      mark i           12       2\n |                 mark ii           0       4\n |      sidewinder mark i           10      20\n |                 mark ii           1       4\n |      viper      mark ii           7       1\n |                 mark iii         16      36\n |      \n |      Single label. Note this returns a DataFrame with a single index.\n |      \n |      >>> df.loc['cobra']\n |               max_speed  shield\n |      mark i          12       2\n |      mark ii          0       4\n |      \n |      Single index tuple. Note this returns a Series.\n |      \n |      >>> df.loc[('cobra', 'mark ii')]\n |      max_speed    0\n |      shield       4\n |      Name: (cobra, mark ii), dtype: int64\n |      \n |      Single label for row and column. Similar to passing in a tuple, this\n |      returns a Series.\n |      \n |      >>> df.loc['cobra', 'mark i']\n |      max_speed    12\n |      shield        2\n |      Name: (cobra, mark i), dtype: int64\n |      \n |      Single tuple. Note using ``[[]]`` returns a DataFrame.\n |      \n |      >>> df.loc[[('cobra', 'mark ii')]]\n |                     max_speed  shield\n |      cobra mark ii          0       4\n |      \n |      Single tuple for the index with a single label for the column\n |      \n |      >>> df.loc[('cobra', 'mark i'), 'shield']\n |      2\n |      \n |      Slice from index tuple to single label\n |      \n |      >>> df.loc[('cobra', 'mark i'):'viper']\n |                           max_speed  shield\n |      cobra      mark i           12       2\n |                 mark ii           0       4\n |      sidewinder mark i           10      20\n |                 mark ii           1       4\n |      viper      mark ii           7       1\n |                 mark iii         16      36\n |      \n |      Slice from index tuple to index tuple\n |      \n |      >>> df.loc[('cobra', 'mark i'):('viper', 'mark ii')]\n |                          max_speed  shield\n |      cobra      mark i          12       2\n |                 mark ii          0       4\n |      sidewinder mark i          10      20\n |                 mark ii          1       4\n |      viper      mark ii          7       1\n |  \n |  ndim\n |      Return an int representing the number of axes / array dimensions.\n |      \n |      Return 1 if Series. Otherwise return 2 if DataFrame.\n |      \n |      See Also\n |      --------\n |      ndarray.ndim : Number of array dimensions.\n |      \n |      Examples\n |      --------\n |      >>> s = pd.Series({'a': 1, 'b': 2, 'c': 3})\n |      >>> s.ndim\n |      1\n |      \n |      >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n |      >>> df.ndim\n |      2\n |  \n |  size\n |      Return an int representing the number of elements in this object.\n |      \n |      Return the number of rows if Series. Otherwise return the number of\n |      rows times number of columns if DataFrame.\n |      \n |      See Also\n |      --------\n |      ndarray.size : Number of elements in the array.\n |      \n |      Examples\n |      --------\n |      >>> s = pd.Series({'a': 1, 'b': 2, 'c': 3})\n |      >>> s.size\n |      3\n |      \n |      >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n |      >>> df.size\n |      4\n |  \n |  values\n |      Return a Numpy representation of the DataFrame.\n |      \n |      .. warning::\n |      \n |         We recommend using :meth:`DataFrame.to_numpy` instead.\n |      \n |      Only the values in the DataFrame will be returned, the axes labels\n |      will be removed.\n |      \n |      Returns\n |      -------\n |      numpy.ndarray\n |          The values of the DataFrame.\n |      \n |      See Also\n |      --------\n |      DataFrame.to_numpy : Recommended alternative to this method.\n |      DataFrame.index : Retrieve the index labels.\n |      DataFrame.columns : Retrieving the column names.\n |      \n |      Notes\n |      -----\n |      The dtype will be a lower-common-denominator dtype (implicit\n |      upcasting); that is to say if the dtypes (even of numeric types)\n |      are mixed, the one that accommodates all will be chosen. Use this\n |      with care if you are not dealing with the blocks.\n |      \n |      e.g. If the dtypes are float16 and float32, dtype will be upcast to\n |      float32.  If dtypes are int32 and uint8, dtype will be upcast to\n |      int32. By :func:`numpy.find_common_type` convention, mixing int64\n |      and uint64 will result in a float64 dtype.\n |      \n |      Examples\n |      --------\n |      A DataFrame where all columns are the same type (e.g., int64) results\n |      in an array of the same type.\n |      \n |      >>> df = pd.DataFrame({'age':    [ 3,  29],\n |      ...                    'height': [94, 170],\n |      ...                    'weight': [31, 115]})\n |      >>> df\n |         age  height  weight\n |      0    3      94      31\n |      1   29     170     115\n |      >>> df.dtypes\n |      age       int64\n |      height    int64\n |      weight    int64\n |      dtype: object\n |      >>> df.values\n |      array([[  3,  94,  31],\n |             [ 29, 170, 115]], dtype=int64)\n |      \n |      A DataFrame with mixed type columns(e.g., str/object, int64, float32)\n |      results in an ndarray of the broadest type that accommodates these\n |      mixed types (e.g., object).\n |      \n |      >>> df2 = pd.DataFrame([('parrot',   24.0, 'second'),\n |      ...                     ('lion',     80.5, 1),\n |      ...                     ('monkey', np.nan, None)],\n |      ...                   columns=('name', 'max_speed', 'rank'))\n |      >>> df2.dtypes\n |      name          object\n |      max_speed    float64\n |      rank          object\n |      dtype: object\n |      >>> df2.values\n |      array([['parrot', 24.0, 'second'],\n |             ['lion', 80.5, 1],\n |             ['monkey', nan, None]], dtype=object)\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pandas.core.generic.NDFrame:\n |  \n |  __array_priority__ = 1000\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pandas.core.base.PandasObject:\n |  \n |  __sizeof__(self)\n |      Generates the total memory usage for an object that returns\n |      either a value or Series of values\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pandas.core.accessor.DirNamesMixin:\n |  \n |  __dir__(self)\n |      Provide method name lookup and completion\n |      Only provide 'public' methods.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from pandas.core.accessor.DirNamesMixin:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n\n","name":"stdout"},{"output_type":"execute_result","execution_count":119,"data":{"text/plain":"     city         ds       yhat\n1191   sj 2013-03-17 -16.490303\n1192   sj 2013-03-24 -20.784494\n1193   sj 2013-03-31 -22.755732\n1194   sj 2013-04-07 -22.844209\n1195   sj 2013-04-14 -22.974892","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>city</th>\n      <th>ds</th>\n      <th>yhat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1191</th>\n      <td>sj</td>\n      <td>2013-03-17</td>\n      <td>-16.490303</td>\n    </tr>\n    <tr>\n      <th>1192</th>\n      <td>sj</td>\n      <td>2013-03-24</td>\n      <td>-20.784494</td>\n    </tr>\n    <tr>\n      <th>1193</th>\n      <td>sj</td>\n      <td>2013-03-31</td>\n      <td>-22.755732</td>\n    </tr>\n    <tr>\n      <th>1194</th>\n      <td>sj</td>\n      <td>2013-04-07</td>\n      <td>-22.844209</td>\n    </tr>\n    <tr>\n      <th>1195</th>\n      <td>sj</td>\n      <td>2013-04-14</td>\n      <td>-22.974892</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"execute_result","execution_count":119,"data":{"text/plain":"(260, 3)"},"metadata":{}},{"output_type":"execute_result","execution_count":119,"data":{"text/plain":"     city         ds  total_cases\n1191   sj 2013-03-17   -16.490303\n1192   sj 2013-03-24   -20.784494\n1193   sj 2013-03-31   -22.755732\n1194   sj 2013-04-07   -22.844209\n1195   sj 2013-04-14   -22.974892","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>city</th>\n      <th>ds</th>\n      <th>total_cases</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1191</th>\n      <td>sj</td>\n      <td>2013-03-17</td>\n      <td>-16.490303</td>\n    </tr>\n    <tr>\n      <th>1192</th>\n      <td>sj</td>\n      <td>2013-03-24</td>\n      <td>-20.784494</td>\n    </tr>\n    <tr>\n      <th>1193</th>\n      <td>sj</td>\n      <td>2013-03-31</td>\n      <td>-22.755732</td>\n    </tr>\n    <tr>\n      <th>1194</th>\n      <td>sj</td>\n      <td>2013-04-07</td>\n      <td>-22.844209</td>\n    </tr>\n    <tr>\n      <th>1195</th>\n      <td>sj</td>\n      <td>2013-04-14</td>\n      <td>-22.974892</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#IQ's analysis\nis_iq = training_data.city == 'iq'\niq_training_data=training_data[is_iq]\niq_training_data.head()\niq_training_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iq_model_data= iq_training_data.iloc[:,[3,25]]\niq_model_data.columns=[\"ds\",\"y\"]\niq_model_data.tail()\niq_prophet=Prophet(yearly_seasonality=True, weekly_seasonality=True, seasonality_mode='multiplicative')\niq_prophet.fit(iq_model_data)\n#help(Prophet.make_future_dataframe)\niq_prop_future= iq_prophet.make_future_dataframe(freq='W',periods=156)\niq_prop_future.tail()\niq_prop_forecast= iq_prophet.predict(iq_prop_future)\niq_prop_forecast.tail()\nplot1 = iq_prophet.plot(iq_prop_forecast)\nplot2 = iq_prophet.plot_components(iq_prop_forecast)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iq_test_forecast=pd.DataFrame(dict(city=\"iq\",ds=iq_prop_forecast['ds'], yhat=iq_prop_forecast['yhat']))\niq_test_forecast.tail()\niq_predicted=iq_test_forecast.iloc[520:676,]\niq_predicted.shape\niq_predicted.columns=[\"city\",\"ds\",\"total_cases\"]\niq_predicted.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result=sj_predicted.append(iq_predicted)\nresult.head()\nresult.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.to_csv('result-v1.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}